"""
Additional questions for Computer Architecture & IT Security subtopics.
This module extends the QUESTIONS_DB with more subtopic questions.
"""

# 3. Instruction Set Architecture
INSTRUCTION_SET_QUESTIONS = {
    "easy": [
        {"question": "What is the architecture design philosophy that uses a small set of simple, fast instructions?", "answer": "RISC (Reduced Instruction Set Computer)", "alternatives": ["Reduced Instruction Set Computer", "RISC Architecture"], "explanation": "RISC (Reduced Instruction Set Computer) philosophy emphasizes a small set of simple instructions that execute in a single clock cycle. This approach, used in ARM and MIPS processors, prioritizes simplicity and speed over instruction complexity, relying on compilers to generate efficient code sequences."},
        {"question": "What is the part of an instruction that specifies the operation to be performed?", "answer": "Opcode", "alternatives": ["Operation Code"], "explanation": "The opcode (operation code) is the portion of a machine instruction that specifies the operation to perform, such as ADD, LOAD, or JUMP. It tells the CPU what action to execute, while operands specify what data to use."},
        {"question": "What is the name of the interface that defines all instructions a CPU can execute?", "answer": "ISA (Instruction Set Architecture)", "alternatives": ["Instruction Set Architecture"], "explanation": "The Instruction Set Architecture (ISA) is the abstract interface between hardware and software, defining all instructions, registers, memory addressing modes, and data types a processor supports. It's crucial for software compatibility - programs compiled for one ISA won't run on a processor with a different ISA without emulation."},
        {"question": "What popular instruction set architecture is used in most smartphones and tablets?", "answer": "ARM", "alternatives": ["ARM Architecture", "Advanced RISC Machine"], "explanation": "ARM (Advanced RISC Machine, originally Acorn RISC Machine) is a RISC-based ISA dominant in mobile devices due to its power efficiency. ARM processors power most smartphones, tablets, and increasingly laptops, with Apple's M-series chips and Qualcomm's Snapdragon being notable examples."},
        {"question": "What is the low-level programming language that uses mnemonics for machine instructions?", "answer": "Assembly Language", "alternatives": ["Assembly", "Assembler Language"], "explanation": "Assembly language uses human-readable mnemonics (like MOV, ADD, JMP) instead of binary machine code, making it easier to write low-level programs. Each assembly instruction typically corresponds to one machine instruction, and an assembler translates assembly code into machine code."},
        {"question": "What instruction set architecture is used by Intel and AMD processors?", "answer": "x86", "alternatives": ["x86 Architecture", "IA-32"], "explanation": "x86 is a CISC instruction set architecture originally developed by Intel, used in most desktop and laptop computers. It includes 32-bit (IA-32) and 64-bit (x86-64/AMD64) versions. Despite being complex, x86's backward compatibility has made it the dominant desktop/server architecture."},
        {"question": "What is the binary representation of instructions that the CPU directly executes?", "answer": "Machine Code", "alternatives": ["Machine Language", "Binary Code"], "explanation": "Machine code consists of binary instructions (1s and 0s) that the CPU can directly execute without translation. Every program must ultimately be converted to machine code, whether from high-level languages (via compilation) or assembly language (via assembly)."},
        {"question": "What addressing mode uses a fixed memory address in the instruction?", "answer": "Direct Addressing", "alternatives": ["Absolute Addressing"], "explanation": "In direct addressing mode, the instruction contains the actual memory address of the operand. This is simple and fast but limits flexibility since addresses are hardcoded. For example, 'LOAD 1000' would load the value from memory address 1000."},
        {"question": "What is the term for the method used to specify where operands are located?", "answer": "Addressing Mode", "alternatives": ["Addressing Modes"], "explanation": "Addressing modes determine how the CPU interprets operand specifications in instructions. Common modes include immediate (value in instruction), direct (address in instruction), indirect (address of address), and register (operand in register). Different modes offer tradeoffs between simplicity, flexibility, and efficiency."},
        {"question": "What type of instruction set has variable-length instructions?", "answer": "CISC", "alternatives": ["Complex Instruction Set Computer"], "explanation": "CISC (Complex Instruction Set Computer) architectures like x86 use variable-length instructions, ranging from 1 to 15+ bytes. This allows compact code representation but complicates instruction decoding. In contrast, RISC architectures typically use fixed-length instructions (e.g., 32 bits) for simpler decoding."}
    ],
    "average": [
        {"question": "What RISC-based instruction set architecture is gaining popularity for its open-source, royalty-free nature?", "answer": "RISC-V", "alternatives": ["RISCV"], "explanation": "RISC-V is an open-source RISC instruction set architecture that anyone can use without royalties. Unlike ARM or x86, RISC-V specifications are freely available, allowing custom processor designs. It's gaining traction in embedded systems, research, and increasingly in commercial products, offering an alternative to proprietary ISAs."},
        {"question": "What addressing mode calculates the operand address by adding a constant to a register value?", "answer": "Base-Displacement addressing", "alternatives": ["Indexed addressing", "Register-Offset addressing"], "explanation": "Base-displacement (or indexed) addressing adds a constant offset to a base register value to calculate the operand address. This is extremely useful for accessing array elements, structure fields, and stack variables. For example, accessing array[5] might use base register + (5 × element_size)."},
        {"question": "What is the term for instructions that operate on operands located in CPU registers only?", "answer": "Register-to-Register instructions", "alternatives": ["Register operations", "R-type instructions"], "explanation": "Register-to-register (or R-type) instructions operate entirely on register operands without accessing memory. They're the fastest instructions since registers are immediately accessible. RISC architectures emphasize register operations, using separate load/store instructions for memory access (load-store architecture)."},
        {"question": "What technique allows a single instruction to operate on multiple data elements simultaneously?", "answer": "SIMD (Single Instruction Multiple Data)", "alternatives": ["Vector processing", "SIMD"], "explanation": "SIMD allows a single instruction to perform the same operation on multiple data elements in parallel. Modern processors include SIMD extensions (SSE, AVX for x86; NEON for ARM) that can process 4, 8, or 16 values simultaneously. This is crucial for multimedia processing, scientific computing, and machine learning."},
        {"question": "What is the name for instructions that transfer control to a different part of the program?", "answer": "Branch instructions", "alternatives": ["Jump instructions", "Control flow instructions"], "explanation": "Branch (or jump) instructions alter program control flow by changing the program counter to a different address. They include conditional branches (branch if condition true), unconditional jumps, function calls, and returns. Efficient branch handling is critical for performance, hence sophisticated branch prediction in modern CPUs."},
        {"question": "What addressing mode uses the contents of a register as the memory address?", "answer": "Register Indirect addressing", "alternatives": ["Indirect addressing", "Register deferred"], "explanation": "In register indirect addressing, a register contains the memory address of the operand, not the operand itself. This enables dynamic addressing, crucial for pointer operations, dynamic data structures, and implementing arrays. For example, if R1=1000, then 'LOAD (R1)' loads from address 1000."},
        {"question": "What is the term for ISA features that maintain compatibility with older software?", "answer": "Backward compatibility", "alternatives": ["Legacy support"], "explanation": "Backward compatibility ensures new processors can run software designed for older processors in the same family. x86 maintains remarkable backward compatibility - modern 64-bit processors can run 16-bit DOS programs. This is crucial for commercial success but constrains architectural innovation."},
        {"question": "What instruction format dedicates separate fields for opcode and operands with fixed sizes?", "answer": "Fixed-format instructions", "alternatives": ["RISC format", "Regular format"], "explanation": "Fixed-format instructions use predetermined field positions and sizes for opcode and operands. This simplifies decoding logic and enables faster instruction processing, typical of RISC architectures. All instructions are the same length (e.g., 32 bits), though this may waste encoding space compared to variable-length formats."},
        {"question": "What is the technique where the compiler, not hardware, schedules instructions to avoid hazards?", "answer": "Static scheduling", "alternatives": ["Compile-time scheduling"], "explanation": "Static scheduling relies on the compiler to arrange instructions to minimize pipeline stalls and hazards. This is common in RISC processors where the ISA exposes pipeline behavior to software. The compiler inserts NOPs or reorders independent instructions, reducing hardware complexity at the cost of larger code size."},
        {"question": "What addressing mode automatically increments or decrements a register after using it?", "answer": "Auto-increment/Auto-decrement addressing", "alternatives": ["Post-increment addressing", "Auto-indexing"], "explanation": "Auto-increment/decrement addressing automatically modifies the register after (post) or before (pre) using it as an address. This is extremely efficient for traversing arrays or stacks, eliminating separate increment instructions. For example, 'LOAD (R1)+' loads from R1's address then increments R1."}
    ],
    "difficult": [
        {"question": "What ISA feature allows conditional execution of instructions without branching?", "answer": "Predicated execution", "alternatives": ["Conditional execution", "Predication"], "explanation": "Predicated (or conditional) execution allows instructions to be conditionally executed based on a predicate condition without branching. ARM and IA-64 support this extensively. Instead of 'if (cond) x=a; else x=b;' requiring branches, both assignments execute with one's result discarded based on the condition, eliminating branch misprediction penalties."},
        {"question": "What ISA design principle advocates that frequently used operations should execute faster?", "answer": "Common case optimization", "alternatives": ["Make the common case fast"], "explanation": "The 'make the common case fast' principle guides ISA and microarchitecture design. Since simple operations like loads, stores, and arithmetic dominate program execution, they should be fast even if it makes rare operations slower. This justifies RISC's simple instructions and hardware optimization for common patterns."},
        {"question": "What technique allows processors to expose parallelism explicitly through very long instruction words?", "answer": "VLIW (Very Long Instruction Word)", "alternatives": ["Very Long Instruction Word", "Explicitly Parallel Instruction Computing"], "explanation": "VLIW architectures package multiple operations into a single long instruction word, with the compiler scheduling operations for parallel execution. Unlike superscalar processors that discover parallelism dynamically, VLIW relies entirely on compiler analysis. Intel's Itanium used VLIW (called EPIC), but compiler limitations hindered commercial success."},
        {"question": "What ISA feature allows instructions to specify multiple operations bundled together?", "answer": "Instruction fusion", "alternatives": ["Macro-op fusion", "Op fusion"], "explanation": "Instruction fusion (or macro-op fusion) combines multiple simple instructions into a single complex operation at decode time. For example, compare-and-branch might fuse into one micro-op. This reduces pressure on execution resources and improves performance while maintaining ISA simplicity. Modern x86 processors extensively use fusion."},
        {"question": "What design philosophy motivated RISC development in opposition to increasingly complex CISC processors?", "answer": "ISA minimalism", "alternatives": ["Simplicity principle", "RISC philosophy"], "explanation": "RISC emerged from research showing that CISC complexity provided diminishing returns - most programs used simple instructions primarily. RISC philosophy advocates for simple, uniform instructions that execute quickly, moving complexity to the compiler. This enables higher clock speeds, simpler pipelines, and more efficient hardware implementation."},
        {"question": "What addressing mode allows position-independent code by using the program counter as a base?", "answer": "PC-relative addressing", "alternatives": ["Program counter relative", "IP-relative"], "explanation": "PC-relative addressing calculates addresses relative to the current program counter, enabling position-independent code that works regardless of where it's loaded in memory. This is crucial for shared libraries, relocatable code, and modern security features like ASLR. Branch instructions typically use PC-relative addressing."},
        {"question": "What ISA extension adds instructions specifically for accelerating cryptographic operations?", "answer": "AES-NI", "alternatives": ["AES New Instructions", "Crypto extensions"], "explanation": "AES-NI (Advanced Encryption Standard New Instructions) provides hardware acceleration for AES encryption/decryption, offering 3-10× performance improvement over software implementation. Similar extensions exist for other crypto operations (SHA, etc.). These specialized instructions demonstrate the trend of adding domain-specific accelerators to general-purpose ISAs."},
        {"question": "What technique allows ISAs to add new instruction without breaking compatibility using instruction prefixes or modes?", "answer": "ISA extensions", "alternatives": ["Instruction set extensions", "ISA evolution"], "explanation": "ISA extensions add new capabilities while maintaining backward compatibility through various mechanisms: using previously undefined opcodes, adding instruction prefixes (x86's 0x0F prefix), or mode bits. x86 evolved from 16-bit to 64-bit through extensions (MMX, SSE, AVX), maintaining compatibility across decades while gradually increasing complexity."},
        {"question": "What addressing mode chains multiple indirections to reach the final operand location?", "answer": "Multi-level indirect addressing", "alternatives": ["Cascaded indirect", "Pointer chain"], "explanation": "Multi-level indirect addressing follows a chain of pointers through memory to reach the operand. While conceptually simple (like following linked structures), hardware support is rare due to multiple memory accesses. It's typically implemented in software using multiple instructions, though some historical architectures provided hardware support."},
        {"question": "What ISA characteristic measures the degree to which instructions are orthogonal and composable?", "answer": "ISA regularity", "alternatives": ["Orthogonality", "Instruction uniformity"], "explanation": "ISA regularity (or orthogonality) means instructions work consistently across addressing modes, data types, and operands. Highly regular ISAs like RISC are easier to learn, compile for, and implement in hardware. In contrast, x86's irregularity (special cases, exceptions, asymmetric registers) complicates both hardware and software but evolved from historical constraints."}
    ]
}

# 4. Pipelining 
PIPELINING_QUESTIONS = {
    "easy": [
        {"question": "What is the technique of breaking instruction execution into stages like an assembly line?", "answer": "Pipelining", "alternatives": ["Instruction pipelining", "Pipeline"], "explanation": "Pipelining divides instruction execution into stages (fetch, decode, execute, memory, write-back), allowing multiple instructions to be processed simultaneously at different stages, like an assembly line. This increases throughput without reducing individual instruction latency."},
        {"question": "What is the term for a situation where the pipeline must wait because the next instruction depends on a previous one?", "answer": "Pipeline stall", "alternatives": ["Bubble", "Stall", "Pipeline delay"], "explanation": "A pipeline stall (or bubble) occurs when the pipeline must pause because an instruction cannot proceed to the next stage. This wastes clock cycles and reduces throughput. Common causes include data hazards, control hazards, and structural hazards."},
        {"question": "What is the first stage of a typical instruction pipeline that retrieves instructions from memory?", "answer": "Fetch", "alternatives": ["Instruction Fetch", "IF stage"], "explanation": "The Fetch stage retrieves the next instruction from memory using the program counter address. It's the first stage in a typical 5-stage pipeline (Fetch, Decode, Execute, Memory, Write-back). Efficient fetching is crucial since it feeds the entire pipeline."},
        {"question": "What type of hazard occurs when an instruction needs data that hasn't been computed yet?", "answer": "Data hazard", "alternatives": ["Data dependency"], "explanation": "Data hazards occur when an instruction depends on the result of a previous instruction still in the pipeline. For example, if instruction 2 needs the result of instruction 1, but instruction 1 hasn't reached the write-back stage yet, a data hazard exists."},
        {"question": "What is the term for the wasted time in a pipeline due to hazards or dependencies?", "answer": "Pipeline overhead", "alternatives": ["Stall cycles", "Bubble cycles"], "explanation": "Pipeline overhead refers to wasted clock cycles where pipeline stages sit idle due to hazards, preventing the pipeline from achieving its theoretical maximum throughput. Minimizing overhead through hazard detection and resolution is crucial for pipeline efficiency."},
        {"question": "In a 5-stage pipeline, what is the stage that interprets the instruction and reads operands?", "answer": "Decode", "alternatives": ["Instruction Decode", "ID stage"], "explanation": "The Decode stage interprets the instruction opcode, identifies required operands, and reads values from registers. It's the second stage in a classic 5-stage RISC pipeline and prepares everything needed for the Execute stage."},
        {"question": "What is the maximum number of instructions that can be simultaneously in different stages of an N-stage pipeline?", "answer": "N", "alternatives": ["N instructions"], "explanation": "An N-stage pipeline can have at most N instructions in flight simultaneously, one per stage. For example, a 5-stage pipeline can process 5 instructions at once. This is the key to pipelining's performance improvement - overlapping execution of multiple instructions."},
        {"question": "What technique inserts wasted cycles (NOPs) to resolve pipeline hazards?", "answer": "Pipeline stalling", "alternatives": ["Inserting bubbles", "Adding NOPs"], "explanation": "Pipeline stalling intentionally inserts empty cycles (bubbles or NOPs) to delay dependent instructions until their operands are ready. While simple to implement, stalling wastes cycles and reduces pipeline efficiency. More sophisticated techniques like forwarding can often avoid stalls."},
        {"question": "What is the term for when two instructions try to use the same hardware resource simultaneously?", "answer": "Structural hazard", "alternatives": ["Resource conflict"], "explanation": "Structural hazards occur when hardware resources can't support all concurrent operations in a pipeline. For example, if there's only one memory port but both fetch and memory stages need to access memory simultaneously, a structural hazard exists. Solutions include adding more resources or scheduling to avoid conflicts."},
        {"question": "What pipeline stage performs arithmetic, logical, and comparison operations?", "answer": "Execute", "alternatives": ["EX stage", "Execution stage"], "explanation": "The Execute (EX) stage performs the actual operation specified by the instruction: arithmetic (add, subtract), logical (AND, OR), shifts, or comparisons. It's the third stage in a classic 5-stage pipeline and is where the ALU does its work."}
    ],
    "average": [
        {"question": "What technique bypasses the write-back stage to directly forward results to dependent instructions?", "answer": "Data forwarding", "alternatives": ["Bypassing", "Short-circuiting"], "explanation": "Data forwarding (or bypassing) provides results directly from one pipeline stage to another without waiting for write-back to registers. For example, the ALU output can be forwarded immediately to a dependent instruction's ALU input, avoiding stalls. This is a key technique for maintaining pipeline efficiency."},
        {"question": "What type of hazard occurs when pipeline decisions depend on branch outcomes not yet determined?", "answer": "Control hazard", "alternatives": ["Branch hazard"], "explanation": "Control hazards (or branch hazards) occur when the pipeline must make decisions before knowing whether a branch will be taken. The pipeline doesn't know which instruction to fetch next until the branch condition is evaluated, potentially several stages later, causing pipeline stalls or requiring speculative execution."},
        {"question": "What is the technique where the compiler rearranges instructions to minimize pipeline stalls?", "answer": "Instruction scheduling", "alternatives": ["Code scheduling", "Compiler scheduling"], "explanation": "Instruction scheduling is a compiler optimization that reorders independent instructions to minimize pipeline stalls. The compiler analyzes data dependencies and arranges instructions to maximize pipeline utilization, filling potential stall cycles with useful work. This is especially important for statically scheduled RISC processors."},
        {"question": "What is the RAW (Read After Write) hazard where an instruction needs data before it's written?", "answer": "True dependency", "alternatives": ["Data dependency", "RAW hazard"], "explanation": "A RAW (Read After Write) hazard is a true data dependency where an instruction must read a value that a previous instruction will write. For example: 'ADD R1, R2, R3' followed by 'SUB R4, R1, R5' has a RAW hazard on R1. The second instruction must wait for R1 to be written. Forwarding can often resolve RAW hazards."},
        {"question": "What technique speculatively executes instructions from both branch paths simultaneously?", "answer": "Multipath execution", "alternatives": ["Eager execution"], "explanation": "Multipath (or eager) execution speculatively executes both possible branch paths simultaneously, keeping both results until the branch resolves. While resource-intensive, this eliminates branch misprediction penalties entirely. It's practical only for short instruction sequences due to resource constraints."},
        {"question": "What is the maximum theoretical speedup achievable by an N-stage pipeline compared to unpipelined execution?", "answer": "N", "alternatives": ["N times", "Linear speedup"], "explanation": "The theoretical maximum speedup of an N-stage pipeline is N times the unpipelined performance, assuming perfect conditions (no hazards, balanced stages). In practice, hazards, unbalanced stages, and overhead reduce actual speedup. For example, a 5-stage pipeline theoretically provides 5× speedup."},
        {"question": "What WAW (Write After Write) hazard occurs when instructions write to the same register out of order?", "answer": "Output dependency", "alternatives": ["WAW hazard"], "explanation": "A WAW (Write After Write) hazard is an output dependency where two instructions write to the same register, and they might complete out of order. For example, if instruction 2 writes R1 before instruction 1 (which also writes R1), the final value would be incorrect. Register renaming typically resolves WAW hazards."},
        {"question": "What is the term for the time between injecting consecutive instructions into the pipeline?", "answer": "Pipeline initiation interval", "alternatives": ["Throughput rate"], "explanation": "The pipeline initiation interval (or issue rate) is the time between starting consecutive instructions. In an ideal pipeline, it's one clock cycle - a new instruction enters every cycle. The reciprocal gives the throughput. Hazards increase the initiation interval, reducing throughput."},
        {"question": "What technique delays branch resolution while continuing to execute subsequent instructions?", "answer": "Delayed branching", "alternatives": ["Branch delay slot"], "explanation": "Delayed branching exposes the branch delay to the ISA - the instruction(s) immediately following a branch execute regardless of the branch outcome. The compiler fills these 'delay slots' with useful instructions from before the branch or makes them NOPs. This was common in early RISC processors (MIPS, SPARC)."},
        {"question": "What is the WAR (Write After Read) hazard that occurs only with out-of-order execution?", "answer": "Anti-dependency", "alternatives": ["WAR hazard"], "explanation": "A WAR (Write After Read) anti-dependency occurs when an instruction writes a value before a previous instruction reads the old value. This only causes problems with out-of-order execution since in-order pipelines naturally avoid it. Register renaming eliminates WAR hazards by giving each write a unique destination."}
    ],
    "difficult": [
        {"question": "What advanced technique allows a pipeline to continue execution past conditional branches before the condition is known?", "answer": "Speculative execution", "alternatives": ["Branch speculation"], "explanation": "Speculative execution allows the pipeline to continue beyond branches by guessing the outcome and executing along the predicted path. If the prediction is correct, performance improves; if wrong, the speculatively executed work is discarded and the pipeline restarts from the correct path. This trades occasional penalties for usually avoiding stalls."},
        {"question": "What hardware structure tracks which pipeline stages have produced results available for forwarding?", "answer": "Scoreboard", "alternatives": ["Hazard detection unit"], "explanation": "A scoreboard is a hardware table that tracks register availability and pending operations, enabling hazard detection and resolution. It monitors which instructions are using which registers and controls forwarding and stalling. Scoreboards are central to Tomasulo's algorithm and other dynamic scheduling schemes."},
        {"question": "What is the phenomenon where deeper pipelines provide diminishing returns and eventually hurt performance?", "answer": "Pipeline depth limitation", "alternatives": ["Deep pipeline penalty"], "explanation": "Beyond a certain depth, adding pipeline stages hurts performance due to increased branch misprediction penalties, forwarding complexity, and overhead. Each added stage reduces per-stage work but increases pipeline latency, making branch mispredictions more costly. Modern designs balance depth (typically 10-20 stages) against these factors."},
        {"question": "What technique uses multiple parallel pipelines to issue multiple instructions per clock cycle?", "answer": "Superscalar execution", "alternatives": ["Multiple issue", "Superscalar"], "explanation": "Superscalar processors have multiple parallel pipelines, allowing them to fetch, decode, and execute multiple instructions per cycle. A 4-wide superscalar can theoretically complete 4 instructions per clock. This requires significant hardware for dependency checking, multiple execution units, and sophisticated scheduling."},
        {"question": "What is the latency penalty for a branch misprediction in a pipeline with N stages between fetch and execute?", "answer": "N cycles", "alternatives": ["N clock cycles"], "explanation": "When a branch mispredicts in an N-stage pipeline, all speculatively fetched instructions (up to N-1) must be flushed, wasting N cycles before correct-path instructions enter execution. This is why deep pipelines are vulnerable to control hazards and why accurate branch prediction is crucial for performance."},
        {"question": "What technique saves pipeline state at branch points to enable rapid recovery from mispredictions?", "answer": "Checkpoint recovery", "alternatives": ["State checkpointing"], "explanation": "Checkpoint recovery saves the processor state (registers, flags) at branch points. On misprediction, instead of flushing the entire pipeline and re-fetching instructions, the checkpoint is restored, reducing recovery time. This is especially valuable in deep pipelines where misprediction penalties are severe."},
        {"question": "What hardware structure allows out-of-order execution while maintaining in-order commit?", "answer": "Reorder buffer", "alternatives": ["ROB", "Completion buffer"], "explanation": "The reorder buffer (ROB) is a circular buffer that holds instructions in program order from dispatch until retirement. Instructions execute out-of-order but commit in-order from the ROB, maintaining precise exceptions and architectural state. The ROB enables aggressive out-of-order execution while preserving sequential semantics."},
        {"question": "What advanced scheduling technique issues instructions to reservation stations that execute when operands arrive?", "answer": "Tomasulo's algorithm", "alternatives": ["Tomasulo scheduling"], "explanation": "Tomasulo's algorithm uses reservation stations to track instructions waiting for operands. When results are produced, they're broadcast to all stations; ready instructions execute immediately. This enables out-of-order execution, register renaming, and dynamic scheduling without compiler support. Tomasulo's algorithm was pioneering in enabling high-performance out-of-order processors."},
        {"question": "What is the technique of executing multiple iterations of a loop in parallel across pipeline stages?", "answer": "Software pipelining", "alternatives": ["Loop pipelining", "Modulo scheduling"], "explanation": "Software pipelining (or modulo scheduling) rearranges loop iterations so different iterations execute in parallel pipeline stages. Instead of completing iteration N before starting N+1, operations from multiple iterations overlap. This is like loop unrolling but maintains compact code while exposing parallelism. It requires sophisticated compiler analysis of loop-carried dependencies."},
        {"question": "What phenomenon causes performance degradation when pipeline stages have significantly different execution times?", "answer": "Pipeline imbalance", "alternatives": ["Stage imbalance"], "explanation": "Pipeline imbalance occurs when stages have unequal delays, causing the slowest stage to limit overall throughput (the pipeline is only as fast as its slowest stage). Ideally, all stages should take equal time. Imbalance wastes potential performance since faster stages sit idle waiting for slower ones. Careful pipeline design aims to balance stage complexity."}
    ]
}

# Continue with remaining subtopics...
# This file can be imported by question_generator.py to extend the database

ALL_ADDITIONAL_QUESTIONS = {
    'instruction_set': INSTRUCTION_SET_QUESTIONS,
    'pipelining': PIPELINING_QUESTIONS
}

# 5. Parallel Processing
PARALLEL_PROCESSING_QUESTIONS = {
    "easy": [
        {"question": "What term describes performing multiple operations or tasks simultaneously?", "answer": "Parallelism", "alternatives": ["Parallel processing", "Concurrent processing"], "explanation": "Parallelism involves executing multiple operations simultaneously to improve performance. This can occur at different levels: instruction-level (executing multiple instructions at once), thread-level (running multiple threads), or task-level (distributing work across multiple processors)."},
        {"question": "What type of parallel processing has multiple processing units with shared memory?", "answer": "Multiprocessor", "alternatives": ["Shared memory multiprocessor", "SMP"], "explanation": "Multiprocessor systems have multiple CPUs sharing a common memory space. This allows different processors to access the same data directly, simplifying programming but requiring synchronization to prevent conflicts. Symmetric multiprocessors (SMP) are common in modern servers and workstations."},
        {"question": "What is the term for running multiple instructions from a single instruction stream in parallel?", "answer": "SIMD (Single Instruction Multiple Data)", "alternatives": ["Vector processing"], "explanation": "SIMD executes the same operation on multiple data elements simultaneously. One instruction operates on vectors of data rather than single values. This is efficient for data-parallel workloads like image processing, where the same operation applies to many pixels."},
        {"question": "What type of parallelism exists when multiple processors execute different programs on different data?", "answer": "MIMD (Multiple Instruction Multiple Data)", "alternatives": ["Multiple Instruction Multiple Data"], "explanation": "MIMD systems have multiple processors independently executing different instructions on different data. This is the most flexible parallel architecture, supporting both data and task parallelism. Modern multicore processors and computer clusters are MIMD systems."},
        {"question": "What is the term for dividing a large problem into smaller sub-problems that can be solved in parallel?", "answer": "Decomposition", "alternatives": ["Problem decomposition", "Parallelization"], "explanation": "Decomposition breaks a large problem into independent sub-problems that can execute in parallel. Effective decomposition is key to parallel programming success, requiring identification of independent computations and managing dependencies between sub-problems."},
        {"question": "What hardware feature allows a CPU to have multiple independent processing cores on a single chip?", "answer": "Multicore", "alternatives": ["Multi-core processor", "Multicore architecture"], "explanation": "Multicore processors integrate multiple complete CPU cores on a single chip, each capable of independent execution. This provides genuine parallelism within a single package, improving performance for multi-threaded applications while managing power consumption better than increasing clock speeds."},
        {"question": "What is the term for the overhead involved in distributing work among parallel processors?", "answer": "Parallelization overhead", "alternatives": ["Parallel overhead", "Coordination cost"], "explanation": "Parallelization overhead includes time spent distributing work, synchronizing processors, communicating between them, and combining results. This overhead can limit speedup - if it's too high, parallel execution might be slower than sequential. Efficient parallel algorithms minimize this overhead."},
        {"question": "What law states that speedup is limited by the sequential portion of a program?", "answer": "Amdahl's Law", "alternatives": ["Amdahl"], "explanation": "Amdahl's Law states that the maximum speedup of a program is limited by its sequential fraction. Even with infinite processors, if 10% of the program must execute sequentially, maximum speedup is 10×. This highlights the importance of maximizing the parallel portion of programs."},
        {"question": "What term describes multiple processors working on a single shared task?", "answer": "Data parallelism", "alternatives": ["Data-parallel processing"], "explanation": "Data parallelism divides data among processors, with each processor performing the same operation on different data subsets. This is effective for problems with large data sets where the same operation applies to all elements, like matrix operations or image filtering."},
        {"question": "What is the maximum theoretical speedup when using N processors?", "answer": "N", "alternatives": ["Linear speedup", "N times"], "explanation": "The theoretical maximum speedup with N processors is N× (linear speedup), meaning the parallel version runs N times faster than serial execution. However, real speedup is usually less due to parallelization overhead, sequential portions (Amdahl's Law), and communication costs."}
    ],
    "average": [
        {"question": "What type of memory architecture has each processor with its own local memory?", "answer": "Distributed memory", "alternatives": ["Distributed shared memory"], "explanation": "In distributed memory systems, each processor has its own local memory, and processors communicate by passing messages. This scales better than shared memory for large systems but requires explicit communication programming. Computer clusters and supercomputers typically use distributed memory."},
        {"question": "What synchronization primitive ensures mutual exclusion for critical sections?", "answer": "Lock", "alternatives": ["Mutex", "Mutual exclusion lock"], "explanation": "Locks (or mutexes) provide mutual exclusion, ensuring only one thread accesses a critical section at a time. Before entering protected code, a thread acquires the lock; afterward, it releases it. This prevents race conditions but requires careful programming to avoid deadlocks."},
        {"question": "What is the phenomenon where adding more processors provides diminishing returns?", "answer": "Scalability limitation", "alternatives": ["Parallel efficiency degradation"], "explanation": "As more processors are added, efficiency typically decreases due to increased communication overhead, synchronization costs, and load imbalance. This is why real speedup is sublinear - doubling processors doesn't double performance. Strong scaling (fixed problem size) hits limits faster than weak scaling (problem size grows with processors)."},
        {"question": "What parallel programming model passes messages between independent processes?", "answer": "Message passing", "alternatives": ["MPI (Message Passing Interface)"], "explanation": "Message passing is a parallel programming model where processes communicate by explicitly sending and receiving messages. Each process has its own memory space, and data sharing requires explicit communication. MPI (Message Passing Interface) is the standard API for message passing in HPC."},
        {"question": "What is the term for uneven distribution of work among processors?", "answer": "Load imbalance", "alternatives": ["Work imbalance"], "explanation": "Load imbalance occurs when some processors have more work than others, causing them to finish at different times. This wastes processor cycles as some sit idle waiting for others. Effective load balancing, either static or dynamic, distributes work evenly to maximize utilization and minimize total execution time."},
        {"question": "What technique allows threads to share memory while executing independently?", "answer": "Shared memory multithreading", "alternatives": ["Thread-level parallelism"], "explanation": "Shared memory multithreading allows multiple threads within a process to share memory while executing independently on different cores. This simplifies data sharing compared to message passing but requires synchronization (locks, barriers) to prevent race conditions. OpenMP is a popular shared-memory parallel programming API."},
        {"question": "What barrier synchronization point ensures all threads reach a certain point before any can proceed?", "answer": "Barrier", "alternatives": ["Synchronization barrier"], "explanation": "A barrier is a synchronization point where threads must wait until all threads reach it before any can proceed past it. This ensures all threads complete one phase before starting the next. Barriers are useful for algorithms with distinct phases that depend on all processors completing previous work."},
        {"question": "What metric measures the fraction of time processors spend doing useful work versus waiting?", "answer": "Parallel efficiency", "alternatives": ["Utilization"], "explanation": "Parallel efficiency is the speedup divided by the number of processors, indicating how effectively processors are utilized. Perfect efficiency (100%) means linear speedup; lower efficiency indicates overhead, load imbalance, or insufficient parallelism. Maintaining high efficiency as processor count increases is a major challenge."},
        {"question": "What technique duplicates data across processors to reduce communication?", "answer": "Data replication", "alternatives": ["Redundant storage"], "explanation": "Data replication stores copies of data on multiple processors, reducing the need for remote access and communication. The tradeoff is increased memory usage and the need to maintain consistency if data is modified. Replication is effective for read-mostly data shared across many processors."},
        {"question": "What architecture connects processors in a grid where each can communicate with neighbors?", "answer": "Mesh topology", "alternatives": ["2D mesh", "Grid network"], "explanation": "Mesh topology arranges processors in a 2D (or 3D) grid where each processor connects to its neighbors. This provides good locality for many algorithms and scales well, though communication between distant processors requires multiple hops. Many-core processors and NoC (Network-on-Chip) designs use mesh topologies."}
    ],
    "difficult": [
        {"question": "What consistency model guarantees that operations appear to execute atomically in some sequential order?", "answer": "Sequential consistency", "alternatives": ["Sequential memory consistency"], "explanation": "Sequential consistency guarantees that the result of parallel execution is equivalent to some sequential interleaving of operations from all processors, with operations from each processor in program order. This is intuitive but expensive to implement, requiring synchronization that can hurt performance. Many systems use weaker models."},
        {"question": "What phenomenon causes performance degradation when multiple cores access the same cache line?", "answer": "False sharing", "alternatives": ["Cache line contention"], "explanation": "False sharing occurs when threads on different cores modify different variables that happen to reside in the same cache line. Even though there's no true data sharing, the cache coherence protocol causes invalidations and performance degradation. Padding variables to different cache lines avoids this."},
        {"question": "What lock-free synchronization technique allows threads to retry operations if conflicts occur?", "answer": "Optimistic concurrency", "alternatives": ["Transactional memory", "Compare-and-swap"], "explanation": "Optimistic concurrency assumes conflicts are rare and allows threads to proceed without locking. If a conflict is detected (via compare-and-swap or similar atomics), the operation retries. This can outperform locks when contention is low but may waste work under high contention. Software transactional memory is an advanced form."},
        {"question": "What technique overlaps computation with communication to hide latency?", "answer": "Latency hiding", "alternatives": ["Communication-computation overlap"], "explanation": "Latency hiding initiates communication asynchronously and performs other computation while waiting for data to arrive. This overlaps communication latency with useful work, improving efficiency. Pre-fetching and non-blocking communication primitives enable latency hiding, crucial for distributed memory systems where communication is expensive."},
        {"question": "What synchronization mechanism allows producers and consumers to communicate through a fixed-size buffer?", "answer": "Bounded buffer", "alternatives": ["Producer-consumer queue", "Ring buffer"], "explanation": "A bounded buffer (or circular buffer) is a fixed-size queue for producer-consumer communication. Producers add items; consumers remove them. Synchronization ensures producers wait when full and consumers wait when empty. This decouples producers and consumers, allowing them to run at different rates, improving parallelism."},
        {"question": "What advanced processor feature allows cores to temporarily use another core's resources?", "answer": "Resource sharing", "alternatives": ["Dynamic resource allocation"], "explanation": "Modern processors allow resource sharing where idle execution units from one core can be used by another core. This is beyond simple SMT - it's dynamic reallocation of functional units, cache capacity, or bandwidth based on workload needs. This improves utilization but adds complexity to resource management."},
        {"question": "What technique partitions data so that communication only occurs at partition boundaries?", "answer": "Domain decomposition", "alternatives": ["Spatial decomposition"], "explanation": "Domain decomposition partitions the problem domain (e.g., physical space in simulations) across processors. Each processor works on its partition, communicating only with neighbors at boundaries. This minimizes communication volume and is fundamental to parallel scientific computing, enabling massive-scale simulations."},
        {"question": "What memory consistency model only guarantees ordering for synchronization operations?", "answer": "Weak consistency", "alternatives": ["Weak ordering"], "explanation": "Weak consistency models relax ordering guarantees, requiring synchronization only at explicit synchronization points. Ordinary loads and stores can be reordered freely; only synchronization operations (like barriers or atomic operations) enforce ordering. This allows aggressive optimizations but requires careful programming. Most modern systems use weak models."},
        {"question": "What technique adjusts the number of threads dynamically based on available resources?", "answer": "Dynamic parallelism", "alternatives": ["Adaptive parallelism"], "explanation": "Dynamic parallelism adjusts thread count at runtime based on workload characteristics and available resources. For example, reducing threads when other applications compete for cores, or increasing threads for larger problem sizes. This adapts to varying conditions but adds runtime overhead for thread management."},
        {"question": "What protocol coordinates cache coherence in multiprocessor systems?", "answer": "Cache coherence protocol", "alternatives": ["MESI", "Snooping protocol", "Directory protocol"], "explanation": "Cache coherence protocols ensure all processors see a consistent view of memory when data is cached in multiple places. Snooping protocols (like MESI) broadcast cache operations; directory protocols maintain a central directory of cache contents. Coherence is essential for shared-memory multiprocessors but adds overhead that limits scalability."}
    ]
}

# Update the dictionary at the end
ALL_ADDITIONAL_QUESTIONS['parallel_processing'] = PARALLEL_PROCESSING_QUESTIONS

# 6. Cryptography
CRYPTOGRAPHY_QUESTIONS = {
    "easy": [
        {"question": "What is the practice of securing information by transforming it into an unreadable format?", "answer": "Encryption", "alternatives": ["Cryptographic encryption"], "explanation": "Encryption transforms plaintext (readable data) into ciphertext (unreadable data) using an algorithm and a key. Only those with the correct key can decrypt the ciphertext back to plaintext. Encryption protects data confidentiality during storage and transmission."},
        {"question": "What type of encryption uses the same key for both encryption and decryption?", "answer": "Symmetric encryption", "alternatives": ["Secret key encryption", "Symmetric key cryptography"], "explanation": "Symmetric encryption uses a single shared key for both encryption and decryption. It's fast and efficient for large data volumes but requires secure key distribution. Examples include AES, DES, and ChaCha20. The main challenge is securely sharing the key between parties."},
        {"question": "What encryption method uses two different but mathematically related keys?", "answer": "Asymmetric encryption", "alternatives": ["Public key encryption", "Public key cryptography"], "explanation": "Asymmetric encryption uses a key pair: a public key (shared openly) for encryption and a private key (kept secret) for decryption. Anyone can encrypt with the public key, but only the private key holder can decrypt. RSA and ECC are common asymmetric algorithms."},
        {"question": "What fixed-size value represents data that has been processed through a hash function?", "answer": "Hash", "alternatives": ["Hash value", "Digest", "Checksum"], "explanation": "A hash (or digest) is a fixed-size output from a hash function that uniquely represents input data. Good hash functions are one-way (can't reverse to find input) and collision-resistant (different inputs produce different hashes). Common uses include password storage and data integrity verification."},
        {"question": "What key in public key cryptography can be freely shared with anyone?", "answer": "Public key", "alternatives": ["Public encryption key"], "explanation": "The public key in asymmetric cryptography can be freely distributed to anyone. Data encrypted with a public key can only be decrypted with the corresponding private key. This solves the key distribution problem of symmetric encryption and enables applications like secure email and HTTPS."},
        {"question": "What is the process of converting ciphertext back into readable plaintext?", "answer": "Decryption", "alternatives": ["Deciphering"], "explanation": "Decryption is the reverse of encryption - converting ciphertext back to its original plaintext form using the appropriate key and algorithm. In symmetric encryption, the same key encrypts and decrypts. In asymmetric encryption, the private key decrypts data encrypted with the public key."},
        {"question": "What technique protects passwords by storing only their hash values?", "answer": "Password hashing", "alternatives": ["Cryptographic hashing"], "explanation": "Password hashing stores only the hash of passwords, not the passwords themselves. When users log in, their entered password is hashed and compared to the stored hash. Even if the database is compromised, attackers only get hashes, not actual passwords. Salt (random data added before hashing) prevents rainbow table attacks."},
        {"question": "What cryptographic technique combines symmetric and asymmetric encryption for efficiency?", "answer": "Hybrid encryption", "alternatives": ["Hybrid cryptography"], "explanation": "Hybrid encryption uses asymmetric encryption to exchange a symmetric key, then uses that symmetric key for bulk data encryption. This combines the security of public key cryptography with the speed of symmetric encryption. HTTPS/TLS uses hybrid encryption."},
        {"question": "What algorithm is the current standard for symmetric encryption approved by NIST?", "answer": "AES", "alternatives": ["Advanced Encryption Standard", "AES encryption"], "explanation": "AES (Advanced Encryption Standard) is the current standard for symmetric encryption, adopted by NIST in 2001. It replaced DES and offers 128, 192, or 256-bit key sizes. AES is fast, secure, and widely implemented in hardware and software, used in everything from file encryption to VPNs."},
        {"question": "What is the secret piece of information used to encrypt or decrypt data?", "answer": "Key", "alternatives": ["Encryption key", "Cryptographic key"], "explanation": "A key is secret information (typically a string of bits) used by cryptographic algorithms to encrypt and decrypt data. Key strength (length and randomness) determines encryption security. Longer keys provide more security but may require more computation. Key management (generation, distribution, storage) is critical for system security."}
    ],
    "average": [
        {"question": "What attack tries all possible keys until finding the correct one?", "answer": "Brute force attack", "alternatives": ["Exhaustive key search"], "explanation": "A brute force attack systematically tries every possible key until finding one that decrypts the data correctly. Modern encryption uses key sizes large enough to make brute force computationally infeasible - AES-256 would require astronomical time even with all world's computing power. This is why key length is crucial for security."},
        {"question": "What cryptographic primitive ensures data has not been modified?", "answer": "Message Authentication Code", "alternatives": ["MAC", "HMAC"], "explanation": "A Message Authentication Code (MAC) is a tag computed from a message and a secret key, ensuring both integrity (data hasn't been modified) and authenticity (message came from someone with the key). HMAC (Hash-based MAC) is a common construction using hash functions. MACs detect tampering and forgery."},
        {"question": "What method adds random data to passwords before hashing to prevent rainbow table attacks?", "answer": "Salting", "alternatives": ["Salt"], "explanation": "Salting adds random data (salt) to each password before hashing. Even identical passwords have different hashes due to unique salts. This defeats precomputed rainbow tables since attackers must recompute hashes for each salt. Salt is stored with the hash and doesn't need to be secret, just unique and random."},
        {"question": "What public key cryptography algorithm is based on the difficulty of factoring large numbers?", "answer": "RSA", "alternatives": ["RSA encryption"], "explanation": "RSA (Rivest-Shamir-Adleman) is based on the difficulty of factoring the product of two large prime numbers. It's widely used for secure data transmission and digital signatures. While secure with sufficient key size (2048+ bits), RSA is computationally expensive compared to symmetric encryption, hence its use in hybrid schemes."},
        {"question": "What cryptographic technique allows verification of a message's sender without revealing their identity?", "answer": "Digital signature", "alternatives": ["Digital signing"], "explanation": "Digital signatures use asymmetric cryptography to verify message authenticity and integrity. The sender signs with their private key; anyone with the public key can verify. Unlike MACs, signatures provide non-repudiation - the signer can't deny creating the signature. Digital signatures are fundamental to PKI and secure communications."},
        {"question": "What attack exploits weaknesses in cryptographic implementation rather than the algorithm itself?", "answer": "Side-channel attack", "alternatives": ["Timing attack"], "explanation": "Side-channel attacks exploit information leaked during cryptographic operations - timing variations, power consumption, electromagnetic emissions, or acoustic signals. For example, timing attacks measure how long decryption takes to deduce information about the key. Constant-time implementations and other countermeasures defend against side-channels."},
        {"question": "What protocol establishes secure communication by negotiating encryption parameters?", "answer": "Key exchange protocol", "alternatives": ["Key agreement"], "explanation": "Key exchange protocols allow two parties to establish a shared secret key over an insecure channel. Diffie-Hellman is the classic example. Even if an attacker observes all communication, they can't determine the shared key. Key exchange is fundamental to protocols like TLS/SSL."},
        {"question": "What cryptographic concept ensures a sender cannot deny sending a message?", "answer": "Non-repudiation", "alternatives": ["Non-denial"], "explanation": "Non-repudiation prevents a sender from denying they sent a message. Digital signatures provide non-repudiation - the signature can only be created with the private key, proving the signer had that key. This is crucial for legal and financial applications where proof of origin is required."},
        {"question": "What type of cipher encrypts data one bit or byte at a time in a continuous stream?", "answer": "Stream cipher", "alternatives": ["Streaming encryption"], "explanation": "Stream ciphers encrypt data as a continuous stream, typically one byte or bit at a time using a keystream. They're fast and have no padding requirements, ideal for real-time communication. ChaCha20 and RC4 (now deprecated) are stream ciphers. They contrast with block ciphers that process fixed-size blocks."},
        {"question": "What infrastructure manages public keys and digital certificates?", "answer": "PKI (Public Key Infrastructure)", "alternatives": ["Public Key Infrastructure"], "explanation": "PKI is a framework for managing public key cryptography, including certificate authorities (CAs), registration authorities, and certificate repositories. PKI enables secure communication between parties who've never met by having trusted CAs vouch for key ownership through digital certificates. The web's HTTPS relies on PKI."}
    ],
    "difficult": [
        {"question": "What advanced cryptographic technique allows computations on encrypted data without decrypting it?", "answer": "Homomorphic encryption", "alternatives": ["FHE", "Fully homomorphic encryption"], "explanation": "Homomorphic encryption allows operations on encrypted data that produce encrypted results matching operations on plaintext. Fully homomorphic encryption (FHE) supports arbitrary computations, enabling cloud computing on sensitive data without exposing it. While theoretically revolutionary, FHE remains computationally expensive despite recent advances."},
        {"question": "What attack recovers plaintext by finding two inputs that produce the same hash?", "answer": "Collision attack", "alternatives": ["Hash collision"], "explanation": "Collision attacks find two different inputs producing the same hash output. If successful against a cryptographic hash, this breaks integrity guarantees. MD5 and SHA-1 have known collision attacks and are deprecated. Modern hashes like SHA-256 and SHA-3 resist collision attacks with sufficient output size."},
        {"question": "What cryptographic mode combines encryption with authentication in a single operation?", "answer": "Authenticated encryption", "alternatives": ["AEAD", "Authenticated Encryption with Associated Data", "GCM"], "explanation": "Authenticated Encryption with Associated Data (AEAD) provides both confidentiality and authenticity in one operation. GCM (Galois/Counter Mode) and ChaCha20-Poly1305 are popular AEAD modes. They're more efficient and secure than combining separate encryption and MAC operations, preventing subtle implementation vulnerabilities like padding oracle attacks."},
        {"question": "What mathematical problem underlies elliptic curve cryptography's security?", "answer": "Discrete logarithm problem", "alternatives": ["Elliptic curve discrete logarithm", "ECDLP"], "explanation": "Elliptic Curve Cryptography (ECC) security relies on the difficulty of the elliptic curve discrete logarithm problem. ECC achieves equivalent security to RSA with much shorter keys - 256-bit ECC equals 3072-bit RSA. This makes ECC efficient for resource-constrained environments like mobile devices and IoT, explaining its growing adoption."},
        {"question": "What cryptographic technique allows proving knowledge of a secret without revealing the secret itself?", "answer": "Zero-knowledge proof", "alternatives": ["ZKP", "Zero-knowledge protocol"], "explanation": "Zero-knowledge proofs allow one party (prover) to convince another (verifier) they know a secret without revealing the secret itself. This seems paradoxical but is mathematically rigorous. Applications include privacy-preserving authentication and cryptocurrencies. zk-SNARKs and zk-STARKs are advanced zero-knowledge constructions."},
        {"question": "What attack exploits mathematical relationships between plaintext, ciphertext, and keys?", "answer": "Cryptanalysis", "alternatives": ["Cryptanalytic attack"], "explanation": "Cryptanalysis is the science of breaking cryptographic systems through mathematical analysis rather than brute force. Techniques include differential cryptanalysis (analyzing how differences in input affect output), linear cryptanalysis (finding linear approximations), and algebraic attacks. Modern ciphers are designed to resist known cryptanalytic attacks."},
        {"question": "What protocol allows two parties to agree on a shared secret without prior communication?", "answer": "Diffie-Hellman key exchange", "alternatives": ["Diffie-Hellman", "DH key exchange"], "explanation": "Diffie-Hellman enables two parties to establish a shared secret over an insecure channel with no prior shared secrets. Its security relies on the discrete logarithm problem. While not providing authentication (vulnerable to man-in-the-middle without additional measures), DH is fundamental to TLS and VPNs."},
        {"question": "What quantum computing algorithm threatens current public key cryptography?", "answer": "Shor's algorithm", "alternatives": ["Shor"], "explanation": "Shor's algorithm allows quantum computers to factor large numbers and solve discrete logarithms efficiently, breaking RSA and ECC. While large-scale quantum computers don't yet exist, their potential has driven development of post-quantum cryptography - algorithms resistant to both classical and quantum attacks, like lattice-based and hash-based schemes."},
        {"question": "What technique derives multiple keys from a single master secret?", "answer": "Key derivation function", "alternatives": ["KDF"], "explanation": "Key Derivation Functions (KDFs) generate one or more cryptographic keys from a secret value like a password or master key. PBKDF2, bcrypt, scrypt, and Argon2 are KDFs designed for password hashing, using iteration and memory-hardness to slow brute force attacks. Other KDFs like HKDF expand keys for protocol use."},
        {"question": "What mode of operation turns a block cipher into a stream cipher using XOR with keystream?", "answer": "Counter mode", "alternatives": ["CTR mode", "CTR"], "explanation": "Counter (CTR) mode encrypts a counter value with the block cipher to generate a keystream, which is XORed with plaintext. This turns block ciphers like AES into stream ciphers, enabling parallel encryption/decryption and random access. CTR is faster and more flexible than CBC mode and is used in modern protocols like GCM."}
    ]
}

# Continue with additional_questions.py updates
ALL_ADDITIONAL_QUESTIONS['cryptography'] = CRYPTOGRAPHY_QUESTIONS

# 7. Authentication
AUTHENTICATION_QUESTIONS = {
    "easy": [
        {"question": "What is the process of verifying the identity of a user or system?", "answer": "Authentication", "alternatives": ["User authentication", "Identity verification"], "explanation": "Authentication is the process of confirming that someone or something is who or what they claim to be. This typically involves checking credentials like passwords, fingerprints, or security tokens against stored information. Authentication is the first step in access control, followed by authorization."},
        {"question": "What is the most common type of authentication that uses a secret string known only to the user?", "answer": "Password", "alternatives": ["Password authentication"], "explanation": "Passwords are the most widely used authentication method. Users create a secret string that only they know, which is verified during login. While convenient, passwords have vulnerabilities including weak choices, reuse, and susceptibility to various attacks, leading to the adoption of stronger authentication methods."},
        {"question": "What authentication method uses physical characteristics unique to an individual?", "answer": "Biometric authentication", "alternatives": ["Biometrics"], "explanation": "Biometric authentication uses unique physical characteristics like fingerprints, facial recognition, iris scans, or voice patterns to verify identity. These characteristics are difficult to forge or steal, making biometrics more secure than passwords. However, they raise privacy concerns and can't be changed if compromised."},
        {"question": "What is the term for requiring two different forms of identification to verify a user?", "answer": "Two-factor authentication", "alternatives": ["2FA", "Multi-factor authentication", "MFA"], "explanation": "Two-factor authentication (2FA) requires two different types of credentials to verify identity, typically combining something you know (password), something you have (phone/token), or something you are (biometric). This significantly improves security because compromising one factor isn't enough to gain access."},
        {"question": "What type of authentication uses a physical device that generates time-based codes?", "answer": "Hardware token", "alternatives": ["Security token", "Authentication token"], "explanation": "Hardware tokens are physical devices that generate one-time passwords (OTP) or codes for authentication. They can be key fobs displaying changing numbers or USB devices. Hardware tokens are more secure than SMS-based 2FA because they're not vulnerable to SIM swapping or network attacks."},
        {"question": "What authentication factor involves something the user possesses?", "answer": "Possession factor", "alternatives": ["Something you have"], "explanation": "The possession factor ('something you have') includes physical objects like security tokens, smart cards, mobile phones, or key fobs. This is commonly used in multi-factor authentication combined with knowledge factors (passwords) to strengthen security by requiring both knowledge and physical possession."},
        {"question": "What is the authentication method where users log in once and gain access to multiple systems?", "answer": "Single Sign-On", "alternatives": ["SSO"], "explanation": "Single Sign-On (SSO) allows users to authenticate once and access multiple applications or systems without re-entering credentials. This improves user experience and can enhance security by reducing password fatigue and allowing centralized access control. Examples include logging into multiple Google services with one account."},
        {"question": "What temporary, time-sensitive code is used for one authentication session?", "answer": "One-time password", "alternatives": ["OTP", "One-time code"], "explanation": "One-time passwords (OTP) are temporary codes valid for a single login session or transaction. They're typically generated by authenticator apps or hardware tokens and expire quickly (often 30-60 seconds). OTPs prevent replay attacks since each code can only be used once."},
        {"question": "What authentication factor involves something the user knows?", "answer": "Knowledge factor", "alternatives": ["Something you know"], "explanation": "The knowledge factor ('something you know') includes information only the user should know, such as passwords, PINs, security questions, or passphrases. This is the most common authentication factor but is vulnerable to guessing, phishing, and social engineering if not properly protected."},
        {"question": "What is the process of granting or denying specific access rights after successful authentication?", "answer": "Authorization", "alternatives": ["Access control"], "explanation": "Authorization determines what an authenticated user is allowed to do. While authentication verifies identity, authorization specifies permissions and access levels. For example, after logging in (authentication), a user may be authorized to read files but not delete them. These are distinct but related security concepts."}
    ],
    "average": [
        {"question": "What protocol allows authentication across different domains using security tokens?", "answer": "SAML", "alternatives": ["Security Assertion Markup Language"], "explanation": "SAML (Security Assertion Markup Language) is an XML-based protocol for exchanging authentication and authorization data between identity providers and service providers. It enables SSO across different domains and is widely used in enterprise environments for federated authentication."},
        {"question": "What attack involves trying many passwords rapidly until finding the correct one?", "answer": "Brute force attack", "alternatives": ["Password brute force"], "explanation": "Brute force attacks systematically try all possible password combinations until finding the correct one. Defense mechanisms include account lockouts after failed attempts, rate limiting, CAPTCHAs, and requiring strong passwords. The attack's effectiveness decreases exponentially with password length and complexity."},
        {"question": "What authentication method uses cryptographic keys instead of passwords?", "answer": "Public key authentication", "alternatives": ["PKI authentication", "Certificate-based authentication"], "explanation": "Public key authentication uses asymmetric cryptography where users possess a private key and share a public key. The system verifies identity by checking if the user possesses the private key corresponding to the registered public key. This is more secure than passwords and is used in SSH, SSL/TLS certificates, and digital signatures."},
        {"question": "What attack tricks users into revealing their credentials by impersonating legitimate services?", "answer": "Phishing", "alternatives": ["Phishing attack"], "explanation": "Phishing is a social engineering attack where attackers impersonate trusted entities (banks, companies, colleagues) to trick victims into revealing credentials or sensitive information. Common vectors include emails, fake websites, and text messages. Education, email filtering, and multi-factor authentication help defend against phishing."},
        {"question": "What standard describes time-based one-time passwords using HMAC?", "answer": "TOTP", "alternatives": ["Time-based One-Time Password"], "explanation": "TOTP (Time-based One-Time Password) is an algorithm that generates one-time passwords using the current time and a shared secret key. The code changes every 30 seconds, making it secure against replay attacks. Google Authenticator and similar apps implement TOTP for 2FA."},
        {"question": "What OAuth flow is used for server-side web applications to obtain access tokens?", "answer": "Authorization Code flow", "alternatives": ["Authorization Code grant"], "explanation": "The Authorization Code flow is OAuth 2.0's most secure flow for server-side applications. Users authenticate with the authorization server, which redirects back with a code. The application exchanges this code for an access token server-to-server, keeping tokens secure on the backend. This prevents token exposure to browsers."},
        {"question": "What attack exploits weakly hashed passwords using precomputed tables?", "answer": "Rainbow table attack", "alternatives": ["Rainbow table"], "explanation": "Rainbow table attacks use precomputed tables of password hashes to quickly crack hashed passwords. Attackers compare stolen hashes against the rainbow table to find matching passwords. Salting (adding random data before hashing) defeats rainbow tables by making each hash unique, requiring attackers to recompute tables for each salt."},
        {"question": "What authentication mechanism uses a centralized server to validate credentials in large networks?", "answer": "RADIUS", "alternatives": ["Remote Authentication Dial-In User Service"], "explanation": "RADIUS (Remote Authentication Dial-In User Service) is a networking protocol providing centralized authentication, authorization, and accounting (AAA) for network access. It's commonly used for WiFi networks, VPNs, and ISP access, allowing centralized user management and policy enforcement across distributed systems."},
        {"question": "What newer authentication protocol provides better security than RADIUS with modern encryption?", "answer": "DIAMETER", "alternatives": ["DIAMETER protocol"], "explanation": "DIAMETER is the successor to RADIUS, designed for modern networks with improved security (mandatory encryption), reliability (TCP/SCTP instead of UDP), and capability negotiation. Despite the name, DIAMETER isn't exactly twice as good as RADIUS - the name is a play on words. It's used in mobile networks and IMS."},
        {"question": "What attack captures authentication tokens to replay them and gain unauthorized access?", "answer": "Session hijacking", "alternatives": ["Session replay attack", "Cookie hijacking"], "explanation": "Session hijacking occurs when an attacker captures or predicts a valid session token to impersonate an authenticated user. Methods include network sniffing, XSS attacks, or session fixation. Defenses include HTTPS encryption, secure cookie flags, short session timeouts, and binding sessions to IP addresses or user agents."}
    ],
    "difficult": [
        {"question": "What challenge-response protocol uses a cryptographic hash to authenticate without sending passwords?", "answer": "CHAP", "alternatives": ["Challenge Handshake Authentication Protocol"], "explanation": "CHAP (Challenge Handshake Authentication Protocol) authenticates users without transmitting passwords over the network. The server sends a random challenge, the client hashes it with their password, and sends the hash back. The server verifies by computing the same hash. This prevents password interception and replay attacks."},
        {"question": "What authentication framework enables passwordless login using biometrics and public key cryptography?", "answer": "FIDO2", "alternatives": ["WebAuthn", "Fast Identity Online 2"], "explanation": "FIDO2 is a modern authentication standard comprising WebAuthn (browser API) and CTAP (communication protocol). It enables passwordless authentication using biometrics, PINs, or hardware tokens with public key cryptography. Each site gets unique credentials, preventing phishing and credential stuffing. Major platforms now support FIDO2."},
        {"question": "What Kerberos component issues tickets for authentication in Windows domains?", "answer": "Key Distribution Center", "alternatives": ["KDC"], "explanation": "The Key Distribution Center (KDC) is the central component of Kerberos authentication, consisting of the Authentication Server (AS) and Ticket Granting Server (TGS). It issues encrypted tickets that prove user identity to services without repeatedly transmitting passwords. Kerberos is the foundation of Windows Active Directory authentication."},
        {"question": "What OAuth 2.0 extension adds authentication on top of the authorization framework?", "answer": "OpenID Connect", "alternatives": ["OIDC"], "explanation": "OpenID Connect (OIDC) is an identity layer built on OAuth 2.0, adding authentication to OAuth's authorization capabilities. It introduces ID tokens (JWT format) containing user identity claims, standardized user info endpoints, and discovery mechanisms. OIDC enables SSO and is widely used for social login and enterprise authentication."},
        {"question": "What attack bypasses authentication by manipulating the order or timing of authentication steps?", "answer": "Authentication bypass", "alternatives": ["Auth bypass", "Logic flaw"], "explanation": "Authentication bypass exploits logic flaws in authentication implementations, allowing attackers to access resources without proper credentials. Examples include skipping authentication checks, manipulating session states, exploiting race conditions, or leveraging default credentials. Preventing these requires secure coding practices, thorough testing, and security audits."},
        {"question": "What cryptographic protocol provides mutual authentication and key exchange for TLS?", "answer": "TLS handshake", "alternatives": ["Transport Layer Security handshake"], "explanation": "The TLS handshake establishes secure communication by performing mutual authentication (optionally), negotiating cipher suites, and exchanging keys. It uses asymmetric cryptography to securely exchange a symmetric session key. Modern TLS 1.3 improves performance with fewer round trips while maintaining security through authenticated encryption."},
        {"question": "What zero-knowledge proof protocol allows authentication without revealing passwords?", "answer": "Zero-knowledge password proof", "alternatives": ["ZKPP", "SRP", "Secure Remote Password"], "explanation": "Zero-knowledge password proofs (like SRP - Secure Remote Password protocol) allow users to prove they know a password without transmitting it or any information that could be used to derive it. Even the server doesn't store the password, only a verifier. This protects against server compromise and network eavesdropping."},
        {"question": "What attack exploits weak entropy in token generation to predict valid session tokens?", "answer": "Session prediction", "alternatives": ["Token prediction attack"], "explanation": "Session prediction attacks exploit weak randomness in session token generation. If tokens are predictable (sequential, weak PRNG, insufficient entropy), attackers can guess valid tokens and hijack sessions. Defense requires cryptographically secure random number generators (CSRNG) and sufficient token length to make brute force infeasible."},
        {"question": "What authentication mechanism allows services to authenticate users on behalf of other services without sharing credentials?", "answer": "Delegated authentication", "alternatives": ["OAuth delegation", "Trusted third-party authentication"], "explanation": "Delegated authentication allows users to grant third-party applications limited access to their resources without sharing credentials. OAuth 2.0 implements this through access tokens with defined scopes. Users authenticate with the identity provider, which issues tokens to third-party apps. This is fundamental to modern API security and social login."},
        {"question": "What technique binds authentication tokens to specific devices using hardware identifiers?", "answer": "Device fingerprinting", "alternatives": ["Hardware binding", "Device binding"], "explanation": "Device fingerprinting creates unique identifiers from device characteristics (OS, browser, screen resolution, hardware IDs, installed fonts) to bind authentication sessions to specific devices. This detects token theft and account takeover. However, advanced fingerprinting raises privacy concerns and can sometimes be circumvented by sophisticated attackers spoofing device characteristics."}
    ]
}

ALL_ADDITIONAL_QUESTIONS['authentication'] = AUTHENTICATION_QUESTIONS

# 8. Security Threats
SECURITY_THREATS_QUESTIONS = {
    "easy": [
        {"question": "What malicious software is designed to damage, disrupt, or gain unauthorized access to a computer system?", "answer": "Malware", "alternatives": ["Malicious software"], "explanation": "Malware is an umbrella term for any software intentionally designed to cause harm. Types include viruses, worms, trojans, ransomware, spyware, and adware. Malware can steal data, encrypt files for ransom, consume resources, or provide backdoor access to attackers. Antivirus software and safe computing practices help prevent infections."},
        {"question": "What type of malware disguises itself as legitimate software to trick users into installing it?", "answer": "Trojan", "alternatives": ["Trojan horse"], "explanation": "Trojans (named after the Greek wooden horse) appear to be useful programs but contain hidden malicious functionality. Unlike viruses, trojans don't replicate themselves. They might disguise as games, utilities, or email attachments. Once installed, they can steal data, download additional malware, or create backdoors for remote access."},
        {"question": "What attack involves sending fraudulent communications that appear to come from a reputable source?", "answer": "Phishing", "alternatives": ["Phishing attack"], "explanation": "Phishing is a social engineering attack where attackers impersonate trusted entities through email, text messages, or fake websites to trick victims into revealing credentials, financial information, or installing malware. Spear phishing targets specific individuals with personalized attacks. Education and email filtering help combat phishing."},
        {"question": "What malicious software replicates itself and spreads to other computers without user intervention?", "answer": "Worm", "alternatives": ["Computer worm"], "explanation": "Worms are self-replicating malware that spread automatically across networks without user action. Unlike viruses (which require a host file), worms are standalone programs. They exploit vulnerabilities to propagate, consuming network bandwidth and system resources. Famous examples include Morris Worm, Code Red, and WannaCry."},
        {"question": "What attack overwhelms a system with traffic to make it unavailable to legitimate users?", "answer": "DoS attack", "alternatives": ["Denial of Service", "DDoS", "Denial of Service attack"], "explanation": "Denial of Service (DoS) attacks flood systems with excessive requests, consuming resources and preventing legitimate access. Distributed DoS (DDoS) attacks use many compromised computers (botnets) to amplify the attack. Defenses include rate limiting, traffic filtering, and DDoS protection services that absorb attack traffic."},
        {"question": "What type of malware encrypts files and demands payment for decryption?", "answer": "Ransomware", "alternatives": ["Crypto-ransomware"], "explanation": "Ransomware encrypts victim files or locks systems, demanding ransom (typically cryptocurrency) for the decryption key. Notable examples include WannaCry and CryptoLocker. Prevention includes regular backups, security updates, and email filtering. Paying ransom doesn't guarantee file recovery and encourages attackers."},
        {"question": "What vulnerability exploitation occurs before the software vendor is aware of it?", "answer": "Zero-day exploit", "alternatives": ["Zero-day attack", "0-day"], "explanation": "Zero-day exploits target previously unknown vulnerabilities before vendors can create patches. These are particularly dangerous because no defense exists yet. Attackers may discover these vulnerabilities themselves or purchase them on dark markets. Organizations use defense-in-depth strategies to limit zero-day impact."},
        {"question": "What attack intercepts communication between two parties without their knowledge?", "answer": "Man-in-the-middle attack", "alternatives": ["MITM", "Interception attack"], "explanation": "Man-in-the-middle (MITM) attacks occur when attackers secretly intercept and possibly alter communication between two parties who believe they're directly communicating. Attackers can eavesdrop or inject malicious content. HTTPS, VPNs, and certificate validation help prevent MITM attacks by ensuring communication authenticity and encryption."},
        {"question": "What malicious software secretly monitors and records user activity?", "answer": "Spyware", "alternatives": ["Surveillance software"], "explanation": "Spyware covertly monitors user activities, collecting sensitive information like passwords, browsing habits, credit card numbers, or keystrokes. It transmits this data to attackers without user knowledge. Keyloggers are a specific type of spyware recording keystrokes. Anti-malware software can detect and remove spyware."},
        {"question": "What social engineering attack involves creating a fabricated scenario to obtain information?", "answer": "Pretexting", "alternatives": ["Pretext attack"], "explanation": "Pretexting involves creating a fabricated scenario (pretext) to manipulate victims into divulging information or performing actions. Attackers might impersonate IT support, executives, or authorities to gain trust. Unlike phishing which uses deception, pretexting involves more elaborate interaction and storytelling. Training helps employees recognize and resist pretexting."}
    ],
    "average": [
        {"question": "What attack injects malicious code into web pages viewed by other users?", "answer": "Cross-site scripting", "alternatives": ["XSS", "XSS attack"], "explanation": "Cross-Site Scripting (XSS) occurs when attackers inject malicious JavaScript into web pages viewed by other users. The script executes in victims' browsers, potentially stealing session cookies, redirecting users, or defacing pages. Types include stored XSS (persistent), reflected XSS (one-time), and DOM-based XSS. Input validation and output encoding prevent XSS."},
        {"question": "What attack manipulates SQL queries by injecting malicious SQL code through user inputs?", "answer": "SQL injection", "alternatives": ["SQLi", "SQL injection attack"], "explanation": "SQL injection exploits vulnerable database queries by injecting malicious SQL code through user inputs. Attackers can bypass authentication, extract sensitive data, modify databases, or execute administrative operations. Parameterized queries (prepared statements) and input validation prevent SQL injection, one of the most common web vulnerabilities."},
        {"question": "What attack tricks authenticated users into executing unwanted actions on web applications?", "answer": "Cross-Site Request Forgery", "alternatives": ["CSRF", "XSRF"], "explanation": "CSRF attacks trick authenticated users into submitting malicious requests they didn't intend. Attackers craft requests (in emails or websites) that execute with the victim's authentication. For example, transferring funds or changing settings. Anti-CSRF tokens that validate request origin prevent these attacks."},
        {"question": "What type of attack exploits buffer overflows to execute arbitrary code?", "answer": "Buffer overflow attack", "alternatives": ["Buffer overflow"], "explanation": "Buffer overflow attacks occur when programs write more data to a buffer than it can hold, overwriting adjacent memory. Attackers exploit this to inject and execute malicious code, often gaining system privileges. Modern defenses include Address Space Layout Randomization (ASLR), Data Execution Prevention (DEP), and safe programming practices."},
        {"question": "What attack uses compromised computers to perform coordinated malicious activities?", "answer": "Botnet", "alternatives": ["Bot network", "Zombie network"], "explanation": "Botnets are networks of compromised computers (bots/zombies) controlled by attackers. They're used for DDoS attacks, spam distribution, credential theft, and cryptocurrency mining. Devices become bots through malware infections exploiting weak passwords or unpatched vulnerabilities. Botnet takedowns require coordinated law enforcement and industry efforts."},
        {"question": "What vulnerability allows attackers to include malicious files from remote servers?", "answer": "Remote File Inclusion", "alternatives": ["RFI"], "explanation": "Remote File Inclusion (RFI) occurs when applications dynamically include external files based on user input without proper validation. Attackers can force the application to include malicious files from their servers, leading to code execution. Disabling remote file inclusion features and validating file paths prevents RFI attacks."},
        {"question": "What attack exploits race conditions between checking and using resources?", "answer": "Time-of-check to time-of-use", "alternatives": ["TOCTOU", "Race condition attack"], "explanation": "TOCTOU attacks exploit the time gap between checking a resource's state and using it. If the resource changes between check and use, attackers can manipulate the outcome. Examples include file system races where permissions change after checking but before access. Atomic operations and proper locking mechanisms prevent TOCTOU vulnerabilities."},
        {"question": "What social engineering attack involves searching through trash for valuable information?", "answer": "Dumpster diving", "alternatives": ["Trash diving"], "explanation": "Dumpster diving involves physically searching through an organization's trash for sensitive documents, printouts, or storage media containing passwords, proprietary information, or personal data. It's surprisingly effective when organizations don't properly dispose of sensitive materials. Secure document destruction (shredding) and data wiping prevent information leakage through dumpster diving."},
        {"question": "What attack follows authorized persons into restricted areas without proper authentication?", "answer": "Tailgating", "alternatives": ["Piggybacking"], "explanation": "Tailgating (or piggybacking) is a physical security breach where unauthorized individuals follow authorized persons through secure entry points. Attackers might pose as delivery personnel or pretend to have forgotten badges. Security awareness training, mantrap doors, and strict badge policies prevent tailgating."},
        {"question": "What attack involves eavesdropping on wireless network communications?", "answer": "Packet sniffing", "alternatives": ["Network sniffing", "Eavesdropping"], "explanation": "Packet sniffing captures network traffic to analyze or steal transmitted data. On unencrypted networks, attackers can intercept passwords, emails, and sensitive information. Tools like Wireshark perform legitimate sniffing for troubleshooting, but attackers misuse them for espionage. Encryption (HTTPS, VPNs) protects against sniffing."}
    ],
    "difficult": [
        {"question": "What advanced persistent threat involves long-term covert access for espionage or sabotage?", "answer": "APT", "alternatives": ["Advanced Persistent Threat"], "explanation": "Advanced Persistent Threats (APTs) are sophisticated, long-term attacks typically by nation-states or well-funded groups targeting specific organizations for espionage, intellectual property theft, or sabotage. APTs use multiple techniques, maintain persistence, and operate covertly for months or years. Detection requires behavioral analysis, threat intelligence, and comprehensive security monitoring."},
        {"question": "What attack exploits the trust relationship between websites to hijack user clicks?", "answer": "Clickjacking", "alternatives": ["UI redress attack"], "explanation": "Clickjacking tricks users into clicking hidden elements by overlaying invisible or disguised frames over legitimate content. Users think they're clicking one thing but actually interact with malicious content. This can lead to unintended actions like enabling webcam access or transferring funds. X-Frame-Options headers and Content Security Policy prevent clickjacking."},
        {"question": "What side-channel attack exploits CPU speculative execution to leak sensitive data?", "answer": "Spectre", "alternatives": ["Spectre attack"], "explanation": "Spectre exploits speculative execution in modern CPUs to leak sensitive data across security boundaries. It tricks the CPU into speculatively executing instructions that shouldn't be executed, then uses timing attacks to extract information left in cache. Spectre is difficult to mitigate completely, requiring software patches, microcode updates, and sometimes performance penalties."},
        {"question": "What attack manipulates DNS responses to redirect users to malicious sites?", "answer": "DNS poisoning", "alternatives": ["DNS cache poisoning", "DNS spoofing"], "explanation": "DNS poisoning corrupts DNS cache data, causing name servers to return incorrect IP addresses. Users attempting to access legitimate sites are redirected to malicious ones that may look identical but steal credentials or distribute malware. DNSSEC (DNS Security Extensions) cryptographically validates DNS responses to prevent poisoning."},
        {"question": "What attack exploits deserialization of untrusted data to execute arbitrary code?", "answer": "Insecure deserialization", "alternatives": ["Deserialization attack"], "explanation": "Insecure deserialization occurs when applications deserialize untrusted data without validation, potentially allowing remote code execution. Attackers craft malicious serialized objects that, when deserialized, execute arbitrary code or manipulate application logic. This was added to OWASP Top 10 due to its severity. Avoid deserializing untrusted data or use safe alternatives like JSON."},
        {"question": "What attack compromises legitimate websites to distribute malware to visitors?", "answer": "Watering hole attack", "alternatives": ["Watering hole"], "explanation": "Watering hole attacks compromise websites frequently visited by target victims (like industry forums or news sites), then use these sites to distribute malware. Rather than attacking targets directly, attackers infect sites targets trust and visit regularly. This is common in targeted espionage. Browser security, ad blockers, and endpoint protection provide defense."},
        {"question": "What technique allows attackers to extract cryptographic keys through power consumption analysis?", "answer": "Power analysis attack", "alternatives": ["Differential power analysis", "DPA"], "explanation": "Power analysis attacks extract cryptographic keys by analyzing a device's power consumption during cryptographic operations. Differential Power Analysis (DPA) uses statistical methods to reveal secret keys from power traces. This is a side-channel attack particularly effective against embedded devices and smart cards. Countermeasures include random delays and power consumption masking."},
        {"question": "What attack manipulates return addresses on the stack to execute attacker-controlled code?", "answer": "Return-oriented programming", "alternatives": ["ROP", "ROP attack"], "explanation": "Return-Oriented Programming (ROP) chains together existing code snippets (gadgets) ending in return instructions to perform arbitrary operations, bypassing Data Execution Prevention (DEP). Attackers don't inject new code but reuse existing executable code in creative sequences. Control-Flow Integrity (CFI) and stack cookies help defend against ROP attacks."},
        {"question": "What attack forces users to perform actions by hiding commands in trusted media?", "answer": "Confused deputy attack", "alternatives": ["Confused deputy"], "explanation": "Confused deputy attacks exploit programs with excessive privileges that can be tricked into misusing their authority. The program (deputy) is confused into performing actions on behalf of an attacker that it shouldn't authorize. This is a privilege escalation technique. Principle of least privilege and careful privilege design prevent confused deputy vulnerabilities."},
        {"question": "What attack exfiltrates data by encoding it in DNS queries to bypass firewalls?", "answer": "DNS tunneling", "alternatives": ["DNS exfiltration"], "explanation": "DNS tunneling encodes data within DNS queries and responses, creating a covert communication channel that bypasses firewalls and security controls since DNS is typically allowed. Attackers exfiltrate data or establish C2 (command and control) channels through DNS. Detection requires analyzing DNS query patterns, frequency, and payload sizes for anomalies."}
    ]
}

ALL_ADDITIONAL_QUESTIONS['security_threats'] = SECURITY_THREATS_QUESTIONS

# 9. Security Protocols
SECURITY_PROTOCOLS_QUESTIONS = {
    "easy": [
        {"question": "What protocol secures web communications by encrypting data between browsers and servers?", "answer": "HTTPS", "alternatives": ["HTTP Secure", "SSL/TLS"], "explanation": "HTTPS (HTTP Secure) encrypts web traffic using SSL/TLS protocols, protecting data from eavesdropping and tampering. The padlock icon in browsers indicates HTTPS is active. It's essential for protecting sensitive information like passwords and financial data. HTTPS has become standard for all websites, not just those handling sensitive data."},
        {"question": "What security protocol encrypts data transmitted over wireless networks?", "answer": "WPA2", "alternatives": ["WPA3", "WiFi Protected Access"], "explanation": "WPA2 (WiFi Protected Access 2) secures wireless networks using AES encryption. It replaced the vulnerable WEP protocol. WPA3, the latest version, offers enhanced security including protection against brute force attacks and forward secrecy. Using strong passwords and WPA2/WPA3 is essential for WiFi security."},
        {"question": "What protocol creates an encrypted tunnel for secure remote access over public networks?", "answer": "VPN", "alternatives": ["Virtual Private Network"], "explanation": "VPNs (Virtual Private Networks) create encrypted tunnels between devices and networks over the internet, protecting data from interception. They're used for secure remote access to corporate networks, privacy protection, and bypassing geographic restrictions. Common VPN protocols include OpenVPN, IPsec, and WireGuard."},
        {"question": "What protocol secures email transmission between mail servers?", "answer": "TLS", "alternatives": ["Transport Layer Security", "STARTTLS"], "explanation": "TLS (Transport Layer Security) secures various internet protocols including email (SMTP with STARTTLS). It provides encryption, authentication, and integrity for data in transit. TLS 1.2 and 1.3 are current standards, with older versions like SSL 3.0 and TLS 1.0 being deprecated due to vulnerabilities."},
        {"question": "What protocol provides secure remote command-line access to servers?", "answer": "SSH", "alternatives": ["Secure Shell"], "explanation": "SSH (Secure Shell) provides encrypted remote access to systems, replacing insecure Telnet. It supports secure file transfer (SCP, SFTP), port forwarding, and key-based authentication. SSH uses public key cryptography for authentication and symmetric encryption for data transfer. It's fundamental for system administration."},
        {"question": "What security layer was replaced by the more secure TLS protocol?", "answer": "SSL", "alternatives": ["Secure Sockets Layer"], "explanation": "SSL (Secure Sockets Layer) was the predecessor to TLS. SSL 2.0 and 3.0 have known vulnerabilities and are now deprecated. While people still say 'SSL certificates' or 'SSL/TLS', modern systems actually use TLS. The term persists due to historical naming, but TLS 1.2 and 1.3 are the secure standards."},
        {"question": "What protocol secures file transfers by encrypting data during transmission?", "answer": "SFTP", "alternatives": ["SSH File Transfer Protocol", "Secure FTP"], "explanation": "SFTP (SSH File Transfer Protocol) provides secure file transfer over SSH, encrypting both commands and data. It's different from FTPS (FTP over SSL/TLS). SFTP is preferred for secure file transfers because it uses a single port and integrates with SSH authentication. It replaces insecure FTP for sensitive data."},
        {"question": "What protocol authenticates and encrypts VPN connections using certificates?", "answer": "IPsec", "alternatives": ["IP Security"], "explanation": "IPsec (Internet Protocol Security) authenticates and encrypts IP packets, commonly used in VPNs for site-to-site and remote access connections. It operates at the network layer, providing security for all applications. IPsec can use AH (Authentication Header) for integrity or ESP (Encapsulating Security Payload) for encryption and integrity."},
        {"question": "What protocol ensures DNS responses are authentic and unmodified?", "answer": "DNSSEC", "alternatives": ["DNS Security Extensions"], "explanation": "DNSSEC (DNS Security Extensions) adds cryptographic signatures to DNS records, allowing verification that responses are authentic and haven't been tampered with. This prevents DNS poisoning and man-in-the-middle attacks. DNSSEC uses public key cryptography to create a chain of trust from root DNS servers down to individual domains."},
        {"question": "What protocol family secures electronic mail end-to-end?", "answer": "PGP", "alternatives": ["Pretty Good Privacy", "GPG", "GnuPG"], "explanation": "PGP (Pretty Good Privacy) and its open-source implementation GPG (GNU Privacy Guard) provide end-to-end email encryption using public key cryptography. Senders encrypt with recipients' public keys; only recipients can decrypt with their private keys. PGP also provides digital signatures for authentication and integrity."}
    ],
    "average": [
        {"question": "What TLS handshake phase negotiates cipher suites and exchanges keys?", "answer": "TLS handshake", "alternatives": ["SSL handshake"], "explanation": "The TLS handshake establishes secure connections by negotiating protocol version, cipher suites, and authentication. It exchanges keys for symmetric encryption and verifies server (and optionally client) certificates. TLS 1.3 streamlines this process with fewer round trips and removes weak cipher suites, improving both security and performance."},
        {"question": "What IPsec mode encrypts only the payload, leaving original IP headers intact?", "answer": "Transport mode", "alternatives": ["IPsec transport mode"], "explanation": "IPsec transport mode encrypts only the IP payload, keeping original IP headers visible. This is efficient for end-to-end communication between two hosts but exposes routing information. Tunnel mode encrypts the entire original packet and adds new headers, hiding source/destination. Transport mode is common for host-to-host VPNs."},
        {"question": "What protocol prevents replay attacks by using sequence numbers and time windows?", "answer": "Kerberos", "alternatives": ["Kerberos authentication"], "explanation": "Kerberos uses timestamps and ticket lifetimes to prevent replay attacks. Tickets are time-limited, and systems must have synchronized clocks. The authentication process involves the KDC (Key Distribution Center) issuing time-stamped tickets that expire. This prevents attackers from capturing and replaying valid authentication messages to gain unauthorized access."},
        {"question": "What VPN protocol offers the best balance of security and performance?", "answer": "OpenVPN", "alternatives": ["OpenVPN protocol"], "explanation": "OpenVPN is an open-source VPN protocol using SSL/TLS for encryption and key exchange. It supports various authentication methods, works through firewalls, and provides strong security with good performance. It runs in userspace rather than kernel space, making it portable across platforms. WireGuard is a newer alternative with simpler code and potentially better performance."},
        {"question": "What extension to SSH provides secure graphical remote desktop capabilities?", "answer": "X11 forwarding", "alternatives": ["SSH X forwarding"], "explanation": "SSH X11 forwarding tunnels X Window System connections through SSH, allowing secure remote graphical applications. Applications run on the remote server but display on the local machine. While secure, it can be slow over high-latency connections. Alternatives like VNC over SSH or RDP tunneling provide full desktop access."},
        {"question": "What certificate authority mechanism allows verification of certificate validity status?", "answer": "OCSP", "alternatives": ["Online Certificate Status Protocol"], "explanation": "OCSP (Online Certificate Status Protocol) provides real-time certificate revocation checking, allowing clients to verify whether certificates are still valid. It's more efficient than downloading entire Certificate Revocation Lists (CRLs). OCSP stapling improves performance and privacy by having servers query OCSP and include signed responses in TLS handshakes."},
        {"question": "What wireless security protocol introduced 192-bit encryption for enterprise networks?", "answer": "WPA3-Enterprise", "alternatives": ["WPA3 Enterprise mode"], "explanation": "WPA3-Enterprise offers 192-bit security with GCMP-256 encryption, HMAC-SHA-384 for key derivation, and ECDSA-384 for authentication. This provides government-grade security for enterprise networks with high security requirements. It maintains backward compatibility with WPA2 while significantly strengthening cryptographic security for sensitive environments."},
        {"question": "What protocol secures voice and video communications over IP networks?", "answer": "SRTP", "alternatives": ["Secure Real-time Transport Protocol"], "explanation": "SRTP (Secure Real-time Transport Protocol) extends RTP with encryption, message authentication, and replay protection for VoIP and video calls. It uses AES encryption for confidentiality and HMAC-SHA1 for authentication. SRTP is used in applications like WebRTC, Zoom, and enterprise telephony systems. Key exchange is typically handled by protocols like DTLS-SRTP."},
        {"question": "What protocol establishes authenticated, encrypted channels without requiring infrastructure?", "answer": "TLS-PSK", "alternatives": ["Pre-Shared Key TLS", "PSK"], "explanation": "TLS-PSK (Pre-Shared Key) mode allows TLS connections using pre-shared symmetric keys instead of certificates. This is useful for resource-constrained IoT devices or closed systems where distributing certificates is impractical. PSK eliminates certificate overhead but requires secure key distribution and management. It's common in industrial control and IoT applications."},
        {"question": "What mechanism allows websites to specify which other domains can embed their content?", "answer": "CORS", "alternatives": ["Cross-Origin Resource Sharing"], "explanation": "CORS (Cross-Origin Resource Sharing) is a security mechanism that allows servers to specify which origins can access their resources. Browsers enforce the same-origin policy, blocking cross-origin requests by default. CORS headers like Access-Control-Allow-Origin relax this restriction when appropriate, enabling secure cross-domain API access while preventing unauthorized data theft."}
    ],
    "difficult": [
        {"question": "What perfect forward secrecy mechanism ensures session keys aren't compromised if private keys are stolen?", "answer": "Ephemeral Diffie-Hellman", "alternatives": ["DHE", "ECDHE", "Ephemeral key exchange"], "explanation": "Ephemeral Diffie-Hellman (DHE/ECDHE) generates temporary session keys that aren't derived from long-term private keys. Even if an attacker later obtains the server's private key, they can't decrypt previous sessions. This provides perfect forward secrecy. TLS 1.3 requires ephemeral key exchange. ECDHE (Elliptic Curve) is more efficient than traditional DHE."},
        {"question": "What protocol framework enables mutual authentication without passwords using public key cryptography?", "answer": "FIDO2", "alternatives": ["WebAuthn", "Fast Identity Online"], "explanation": "FIDO2 enables passwordless authentication using public key cryptography and local authentication (biometrics, PINs, security keys). Each site receives unique credentials, preventing credential reuse and phishing. The private key never leaves the device. FIDO2 comprises WebAuthn (web standard) and CTAP (communication with authenticators). Major browsers and platforms support it."},
        {"question": "What TLS extension allows clients to send the target hostname during handshake?", "answer": "SNI", "alternatives": ["Server Name Indication"], "explanation": "SNI (Server Name Indication) allows multiple HTTPS sites to share a single IP address by sending the hostname in the TLS handshake before encryption begins. This enables servers to present the correct certificate. While essential for virtual hosting, SNI exposes the target hostname to network observers. Encrypted SNI (ESNI) in TLS 1.3 addresses this privacy concern."},
        {"question": "What quantum-resistant key exchange algorithm is being standardized by NIST?", "answer": "Kyber", "alternatives": ["CRYSTALS-Kyber", "ML-KEM"], "explanation": "Kyber (CRYSTALS-Kyber) is a lattice-based key exchange algorithm selected by NIST for post-quantum cryptography standardization. It resists attacks by quantum computers that would break current RSA and ECC-based systems. Kyber offers good performance and security levels. Organizations are beginning to implement post-quantum crypto in preparation for quantum computer threats."},
        {"question": "What mechanism allows servers to push certificate status information to clients during TLS handshakes?", "answer": "OCSP stapling", "alternatives": ["TLS certificate status request"], "explanation": "OCSP stapling allows servers to obtain signed, time-stamped OCSP responses and send them during TLS handshakes. This improves performance (clients don't query OCSP responders), privacy (clients don't reveal which sites they're visiting), and reliability (handshake doesn't depend on OCSP responder availability). It's now a TLS best practice."},
        {"question": "What protocol extension encrypts initial client data in TLS 1.3 to prevent passive surveillance?", "answer": "0-RTT", "alternatives": ["Zero Round Trip Time", "Early data"], "explanation": "TLS 1.3 0-RTT mode allows clients to send encrypted application data in the first flight of the handshake, eliminating the latency penalty of TLS for resumed sessions. However, 0-RTT data isn't protected against replay attacks, so it should only be used for idempotent operations. This significantly improves performance for frequent connections."},
        {"question": "What cryptographic protocol allows proving password knowledge without revealing it to the server?", "answer": "SRP", "alternatives": ["Secure Remote Password", "Zero-knowledge password proof"], "explanation": "SRP (Secure Remote Password) is a zero-knowledge password authentication protocol where clients prove they know the password without sending it or any information that could determine it. Even the server doesn't store the password, only a verifier. This protects against server compromise and network eavesdropping. SRP is used in some VPN and encrypted backup systems."},
        {"question": "What protocol provides authenticated encryption for wireless networks using GCMP?", "answer": "WPA3", "alternatives": ["WiFi Protected Access 3"], "explanation": "WPA3 uses GCMP (Galois/Counter Mode Protocol) providing authenticated encryption with stronger cryptography than WPA2's CCMP. It also implements SAE (Simultaneous Authentication of Equals) replacing WPA2's PSK, protecting against offline dictionary attacks. WPA3 provides forward secrecy and mandatory Protected Management Frames (PMF), significantly improving WiFi security."},
        {"question": "What mechanism prevents TLS downgrade attacks by having servers declare minimum supported versions?", "answer": "TLS version negotiation", "alternatives": ["TLS fallback protection", "SCSV"], "explanation": "TLS version negotiation allows clients and servers to agree on the highest mutually supported version. TLS_FALLBACK_SCSV (Signaling Cipher Suite Value) prevents downgrade attacks where attackers force use of older, vulnerable versions. TLS 1.3 improves this with encrypted version negotiation. Servers should disable obsolete versions (SSL 3.0, TLS 1.0/1.1) entirely."},
        {"question": "What protocol provides confidential, authenticated channels for DNS queries?", "answer": "DNS over HTTPS", "alternatives": ["DoH", "DNS-over-HTTPS"], "explanation": "DNS over HTTPS (DoH) encrypts DNS queries within HTTPS connections, providing privacy and preventing DNS surveillance and manipulation. Unlike DNS over TLS (DoT) which uses a dedicated port, DoH uses standard HTTPS port 443, making it harder to block. Major browsers support DoH. However, it's controversial as it bypasses network-level DNS filtering and monitoring."}
    ]
}

ALL_ADDITIONAL_QUESTIONS['security_protocols'] = SECURITY_PROTOCOLS_QUESTIONS

# 10. Cyber Defense
CYBER_DEFENSE_QUESTIONS = {
    "easy": [
        {"question": "What security system monitors network traffic and blocks suspicious activity?", "answer": "Firewall", "alternatives": ["Network firewall"], "explanation": "Firewalls are the first line of defense in network security, controlling incoming and outgoing traffic based on security rules. They can be hardware devices or software applications. Modern firewalls include stateful inspection, deep packet inspection, and application-aware filtering. Firewalls block unauthorized access while allowing legitimate communication."},
        {"question": "What software detects and removes malicious programs from computers?", "answer": "Antivirus", "alternatives": ["Antivirus software", "Anti-malware"], "explanation": "Antivirus software scans files and programs for known malware signatures, suspicious behaviors, or heuristic patterns indicating malicious intent. Modern solutions include real-time protection, automatic updates, and cloud-based threat intelligence. While essential, antivirus should be part of a layered defense strategy since new malware appears constantly."},
        {"question": "What practice involves creating copies of data to prevent loss from failures or attacks?", "answer": "Backup", "alternatives": ["Data backup"], "explanation": "Regular backups are critical for recovering from ransomware, hardware failures, or disasters. The 3-2-1 rule recommends three copies on two different media types with one offsite. Backups should be tested regularly and protected from tampering. Automated backup solutions and immutable storage help ensure recovery capability."},
        {"question": "What security measure requires users to prove their identity before accessing resources?", "answer": "Authentication", "alternatives": ["User authentication"], "explanation": "Authentication verifies user identity through credentials like passwords, biometrics, or security tokens. It's the first step in access control. Strong authentication uses multiple factors (something you know, have, or are) to increase security. Authentication prevents unauthorized access but must be followed by proper authorization to control what users can do."},
        {"question": "What security concept limits user permissions to only what's necessary for their job?", "answer": "Least privilege", "alternatives": ["Principle of least privilege"], "explanation": "The principle of least privilege grants users, processes, and systems only the minimum permissions needed to perform their functions. This limits damage from compromised accounts, reduces attack surface, and helps contain breaches. Regular permission reviews and just-in-time access help maintain least privilege."},
        {"question": "What process involves applying updates to fix security vulnerabilities in software?", "answer": "Patching", "alternatives": ["Security patching", "Software updates"], "explanation": "Patching closes security vulnerabilities by updating software with fixes from vendors. Unpatched systems are common attack vectors. Effective patch management includes testing patches, prioritizing critical updates, and maintaining inventory of all systems. Automated patch management helps ensure timely updates across large environments."},
        {"question": "What training helps employees recognize and resist social engineering attacks?", "answer": "Security awareness training", "alternatives": ["Cybersecurity training"], "explanation": "Security awareness training educates employees about threats like phishing, social engineering, and safe computing practices. Humans are often the weakest link in security. Regular training, simulated phishing exercises, and clear policies help create a security-conscious culture. Training should cover recognizing threats, reporting incidents, and following security procedures."},
        {"question": "What security approach uses multiple defensive layers to protect systems?", "answer": "Defense in depth", "alternatives": ["Layered security"], "explanation": "Defense in depth implements multiple security layers so if one fails, others provide protection. Layers include network security, host security, application security, and data security. This approach assumes breaches will occur and aims to detect, contain, and minimize damage. No single security measure is sufficient against determined attackers."},
        {"question": "What document outlines how an organization responds to security incidents?", "answer": "Incident response plan", "alternatives": ["IR plan", "Security incident response plan"], "explanation": "An incident response plan defines procedures for detecting, responding to, recovering from, and learning from security incidents. It specifies roles, communication protocols, containment strategies, and recovery steps. Regular testing through tabletop exercises ensures the plan works when needed. Good IR plans minimize damage and recovery time."},
        {"question": "What type of security testing simulates real attacks to find vulnerabilities?", "answer": "Penetration testing", "alternatives": ["Pen testing", "Ethical hacking"], "explanation": "Penetration testing involves authorized simulated attacks to identify vulnerabilities before malicious actors find them. Pen testers use the same tools and techniques as attackers but with permission and defined scope. Results help organizations prioritize security improvements. Regular pen testing is essential for maintaining strong security posture."}
    ],
    "average": [
        {"question": "What system detects potential security threats by analyzing network traffic patterns?", "answer": "Intrusion Detection System", "alternatives": ["IDS"], "explanation": "IDS monitors network traffic for suspicious activity and known attack signatures, alerting administrators to potential threats. Unlike firewalls that block traffic, IDS passively observes and alerts. NIDS monitors network segments; HIDS monitors hosts. IDS can use signature-based detection (known attacks) or anomaly-based detection (unusual behavior). False positives require careful tuning."},
        {"question": "What security system actively blocks detected threats in addition to alerting?", "answer": "Intrusion Prevention System", "alternatives": ["IPS"], "explanation": "IPS combines IDS detection capabilities with active blocking. When threats are detected, IPS can drop malicious packets, block IP addresses, or reset connections. IPS sits inline with traffic flow, unlike passive IDS. While more effective than IDS, IPS risks blocking legitimate traffic if misconfigured. Modern NGFW (Next-Generation Firewalls) integrate IPS functionality."},
        {"question": "What authentication method verifies identity using physical or behavioral characteristics?", "answer": "Biometric authentication", "alternatives": ["Biometrics"], "explanation": "Biometric authentication uses unique physical (fingerprints, iris, facial recognition) or behavioral (typing rhythm, gait) characteristics. Biometrics are difficult to forge or steal and don't require remembering passwords. However, they can't be changed if compromised and raise privacy concerns. Multi-factor systems often combine biometrics with other authentication methods."},
        {"question": "What security architecture isolates network segments to contain breaches?", "answer": "Network segmentation", "alternatives": ["Network isolation"], "explanation": "Network segmentation divides networks into isolated segments with controlled connections between them. This limits lateral movement if attackers breach one segment. VLANs, subnets, and firewalls implement segmentation. Critical systems should be in separate segments with strict access controls. Micro-segmentation extends this concept to individual workloads."},
        {"question": "What security zone contains decoy systems to detect and analyze attacks?", "answer": "Honeypot", "alternatives": ["Honeynet"], "explanation": "Honeypots are decoy systems that appear vulnerable to attract attackers. They have no legitimate use, so any interaction indicates malicious activity. Honeypots provide early warning, gather attack intelligence, and distract attackers from real systems. Honeynets are networks of honeypots. However, they require careful isolation to prevent attackers from pivoting to production systems."},
        {"question": "What centralized platform collects and analyzes security logs from multiple sources?", "answer": "SIEM", "alternatives": ["Security Information and Event Management"], "explanation": "SIEM systems aggregate logs from firewalls, servers, applications, and security tools, correlating events to detect threats. They provide real-time analysis, alerting, forensics, and compliance reporting. Modern SIEMs use machine learning to detect anomalies. However, SIEMs require significant tuning and skilled analysts to be effective. They're central to Security Operations Centers (SOCs)."},
        {"question": "What proactive security approach assumes breach and focuses on threat detection?", "answer": "Threat hunting", "alternatives": ["Proactive threat hunting"], "explanation": "Threat hunting proactively searches for threats that evaded automated defenses. Unlike reactive incident response, hunters use intelligence, behavioral analysis, and hypothesis testing to find hidden adversaries. This assumes breaches occur and focuses on early detection before major damage. Effective hunting requires skilled analysts, good tools, and comprehensive visibility."},
        {"question": "What security technique analyzes malware behavior in a controlled environment?", "answer": "Sandboxing", "alternatives": ["Sandbox analysis"], "explanation": "Sandboxing executes suspicious files in isolated virtual environments to observe behavior without risking production systems. This detects malware that evades signature-based detection. Sandboxes monitor file operations, network connections, registry changes, and system calls. Advanced malware may detect sandboxes and alter behavior, requiring evasion-resistant sandboxing techniques."},
        {"question": "What security framework verifies every access request regardless of network location?", "answer": "Zero Trust", "alternatives": ["Zero Trust Architecture", "Zero Trust Security"], "explanation": "Zero Trust assumes no implicit trust based on network location - every access request is fully authenticated, authorized, and encrypted regardless of origin. It eliminates the perimeter-based security model. Key principles include verify explicitly, use least privilege, and assume breach. Implementation requires strong identity management, micro-segmentation, and continuous monitoring."},
        {"question": "What team simulates adversary tactics to test an organization's security defenses?", "answer": "Red team", "alternatives": ["Red teaming"], "explanation": "Red teams simulate real-world attackers using advanced techniques to test security controls and incident response. Unlike pen testing with defined scope, red teaming is adversarial with minimal constraints. The goal is identifying gaps in people, processes, and technology. Blue teams defend; purple teams facilitate cooperation. Red teaming provides realistic assessment of security posture."}
    ],
    "difficult": [
        {"question": "What advanced malware analysis technique reverses compiled code to understand its functionality?", "answer": "Reverse engineering", "alternatives": ["Code reverse engineering", "Malware reverse engineering"], "explanation": "Reverse engineering examines malware binaries using disassemblers (IDA Pro, Ghidra) and debuggers to understand functionality, identify indicators of compromise, and develop signatures. This requires deep understanding of assembly language, file formats, and operating system internals. Static analysis examines code without execution; dynamic analysis observes runtime behavior."},
        {"question": "What threat intelligence approach attributes attacks to specific adversary groups?", "answer": "Threat attribution", "alternatives": ["Adversary attribution"], "explanation": "Threat attribution identifies which threat actor or nation-state is behind attacks by analyzing tactics, techniques, procedures (TTPs), infrastructure, and code similarities. This is challenging as attackers use false flags and share tools. Attribution informs defensive priorities and geopolitical responses. Frameworks like MITRE ATT&CK help categorize adversary behaviors."},
        {"question": "What security orchestration platform automates incident response workflows?", "answer": "SOAR", "alternatives": ["Security Orchestration, Automation and Response"], "explanation": "SOAR platforms automate repetitive security tasks and orchestrate complex workflows across multiple tools. They integrate with SIEM, firewalls, EDR, and threat intelligence to enable automated containment, investigation, and response. SOAR reduces response time, ensures consistent processes, and allows analysts to focus on complex threats. Playbooks define automated response procedures."},
        {"question": "What technique analyzes system memory to detect rootkits and advanced malware?", "answer": "Memory forensics", "alternatives": ["RAM analysis", "Volatile memory analysis"], "explanation": "Memory forensics examines system RAM to find malware, encryption keys, passwords, and attack evidence that doesn't persist on disk. Fileless malware and rootkits often hide only in memory. Tools like Volatility analyze memory dumps to recover processes, network connections, and injected code. Memory forensics is crucial for investigating advanced persistent threats."},
        {"question": "What advanced firewall capability inspects encrypted traffic for threats?", "answer": "SSL/TLS inspection", "alternatives": ["HTTPS inspection", "Deep packet inspection"], "explanation": "SSL/TLS inspection decrypts encrypted traffic, inspects content for threats, then re-encrypts it. This is necessary as most traffic is now encrypted, providing a hiding place for malware. Implementation requires careful certificate management to avoid breaking trust. Privacy concerns and performance impact require balanced policies about what traffic to inspect."},
        {"question": "What security approach detects threats by identifying deviations from baseline behavior?", "answer": "Behavioral analysis", "alternatives": ["Anomaly detection", "User and Entity Behavior Analytics", "UEBA"], "explanation": "Behavioral analysis establishes baselines of normal behavior for users, systems, and networks, then detects anomalies indicating potential threats. UEBA (User and Entity Behavior Analytics) uses machine learning to identify compromised accounts, insider threats, and advanced attacks that evade signature-based detection. This catches novel attacks but requires tuning to reduce false positives."},
        {"question": "What isolation technique runs applications in separate containers to limit malware impact?", "answer": "Application sandboxing", "alternatives": ["Containerization", "Process isolation"], "explanation": "Application sandboxing restricts what applications can access using OS-level isolation. Each app runs in a restricted environment with limited file system, network, and system call access. This contains malware even if an application is compromised. Technologies include Docker containers, browser sandboxes, and mobile app sandboxes. However, sandbox escapes remain a concern."},
        {"question": "What deception technology creates fake network assets to detect lateral movement?", "answer": "Deception technology", "alternatives": ["Active defense", "Cyber deception"], "explanation": "Deception technology deploys fake credentials, servers, databases, and files throughout networks. Any access to these decoys indicates reconnaissance or lateral movement. Unlike passive honeypots, deception tools integrate with production networks. They provide high-fidelity alerts since legitimate users shouldn't access decoys. This shifts defenders' advantage by creating a minefield for attackers."},
        {"question": "What technique isolates potentially compromised systems while investigating incidents?", "answer": "Network containment", "alternatives": ["Quarantine", "Incident containment"], "explanation": "Network containment isolates suspected compromised systems from the network without shutting them down, allowing investigation while preventing malware spread. Techniques include VLAN changes, firewall rules, or EDR-enforced isolation. Containment balances business continuity with security - systems remain operational but can't communicate maliciously. This is crucial for ransomware and APT incidents."},
        {"question": "What advanced persistent threat defense mechanism detects command and control communications?", "answer": "Network traffic analysis", "alternatives": ["NTA", "Flow analysis"], "explanation": "Network Traffic Analysis (NTA) monitors network flows to detect C2 (command and control) communications, data exfiltration, and lateral movement. Unlike signature-based detection, NTA uses machine learning and behavioral analysis to identify suspicious patterns like beaconing, unusual protocols, or abnormal data volumes. This is effective against APTs that evade endpoint detection."}
    ]
}

ALL_ADDITIONAL_QUESTIONS['cyber_defense'] = CYBER_DEFENSE_QUESTIONS

# ============================================================================
# DATA SCIENCE & ANALYTICS (30 files, 300 questions)
# ============================================================================

# 11. Data Fundamentals
DATA_FUNDAMENTALS_QUESTIONS = {
    "easy": [
        {"question": "What is the term for raw facts and figures without context?", "answer": "Data", "alternatives": ["Raw data"], "explanation": "Data consists of raw facts, figures, or observations without interpretation or context. Examples include numbers, text, images, or sensor readings. Data becomes information when processed and given context, enabling decision-making and insights."},
        {"question": "What type of data consists of distinct categories without numerical meaning?", "answer": "Categorical data", "alternatives": ["Qualitative data", "Nominal data"], "explanation": "Categorical (or qualitative) data represents characteristics or attributes that can be divided into categories. Examples include colors, gender, or country names. Unlike numerical data, categories have no inherent order or mathematical meaning, though they can be counted."},
        {"question": "What type of data represents measurable quantities with numerical values?", "answer": "Numerical data", "alternatives": ["Quantitative data"], "explanation": "Numerical (or quantitative) data consists of numbers representing measurable quantities. It can be discrete (countable, like number of students) or continuous (measurable, like height or temperature). Numerical data supports mathematical operations like addition and averaging."},
        {"question": "What is a collection of related data organized in rows and columns?", "answer": "Table", "alternatives": ["Data table", "Dataset"], "explanation": "A table organizes data into rows (records/observations) and columns (fields/attributes). Tables are fundamental to databases and spreadsheets. Each row represents a single entity or observation, while columns represent attributes or variables. This structured format facilitates data analysis and querying."},
        {"question": "What do we call a single piece of information about an entity?", "answer": "Attribute", "alternatives": ["Field", "Variable", "Feature"], "explanation": "An attribute (also called field, variable, or feature) is a single characteristic or property of an entity. In a table, attributes are columns. For example, in a customer table, attributes might include name, email, age, and purchase history."},
        {"question": "What is a complete set of information about a single entity in a table?", "answer": "Record", "alternatives": ["Row", "Instance", "Observation"], "explanation": "A record (or row) contains all attributes for a single entity or observation. In a customer database, one record includes all information about one customer. Records are the horizontal entries in tables, while attributes are vertical columns."},
        {"question": "What type of data has a meaningful order but unequal intervals?", "answer": "Ordinal data", "alternatives": ["Ranked data"], "explanation": "Ordinal data has a natural order but intervals between values aren't equal or meaningful. Examples include education levels (elementary, high school, college) or satisfaction ratings (poor, fair, good). You can rank ordinal data but can't perform arithmetic operations meaningfully."},
        {"question": "What is data with equal intervals and a meaningful zero point?", "answer": "Ratio data", "alternatives": ["Ratio scale"], "explanation": "Ratio data has equal intervals between values and a true zero point representing absence. Examples include height, weight, age, and income. Ratio data supports all mathematical operations, and ratios are meaningful (10kg is twice as heavy as 5kg)."},
        {"question": "What do we call missing or incomplete values in a dataset?", "answer": "Missing data", "alternatives": ["Null values", "NA values"], "explanation": "Missing data occurs when values are absent from a dataset, represented as NULL, NA, or NaN (Not a Number). Missing data can result from data collection errors, non-response, or system issues. Handling missing data is crucial as it can bias analysis if not addressed properly."},
        {"question": "What is the process of converting data from one format to another?", "answer": "Data transformation", "alternatives": ["Data conversion"], "explanation": "Data transformation changes data format, structure, or values to make it suitable for analysis. Examples include converting units, normalizing scales, encoding categories as numbers, or aggregating data. Transformation is essential for preparing data for machine learning and analytics."}
    ],
    "average": [
        {"question": "What type of data can take any value within a range?", "answer": "Continuous data", "alternatives": ["Continuous variable"], "explanation": "Continuous data can take any value within a range, limited only by measurement precision. Examples include height (175.5cm), temperature (98.6°F), or time (3.14 seconds). Continuous data is typically measured rather than counted and can be infinitely subdivided."},
        {"question": "What type of data can only take specific, countable values?", "answer": "Discrete data", "alternatives": ["Discrete variable"], "explanation": "Discrete data consists of distinct, separate values that can be counted. Examples include number of students, cars, or clicks. Discrete data often involves integers and cannot be meaningfully subdivided (you can't have 2.5 students). It's typically counted rather than measured."},
        {"question": "What is the dimensionality problem that occurs with too many features?", "answer": "Curse of dimensionality", "alternatives": ["High dimensionality"], "explanation": "The curse of dimensionality refers to problems arising when data has too many features/dimensions. As dimensions increase, data becomes sparse, distances become less meaningful, and more data is needed to maintain statistical significance. This affects machine learning performance, requiring dimensionality reduction techniques."},
        {"question": "What term describes data with both structured and unstructured elements?", "answer": "Semi-structured data", "alternatives": ["Hybrid data"], "explanation": "Semi-structured data has some organizational properties but doesn't fit neatly into tables. Examples include JSON, XML, and email (with structured headers and unstructured body). It's more flexible than structured data but more organized than purely unstructured data like images or videos."},
        {"question": "What is data that doesn't fit into predefined models or schemas?", "answer": "Unstructured data", "alternatives": ["Non-structured data"], "explanation": "Unstructured data lacks a predefined format or organization, making it difficult to process with traditional databases. Examples include text documents, images, videos, and social media posts. Unstructured data comprises about 80% of organizational data and requires specialized tools like NLP or computer vision for analysis."},
        {"question": "What statistical concept describes the center point of a data distribution?", "answer": "Central tendency", "alternatives": ["Average"], "explanation": "Central tendency measures identify the center or typical value of a dataset. The three main measures are mean (arithmetic average), median (middle value), and mode (most frequent value). Each provides different insights and is appropriate for different data types and distributions."},
        {"question": "What measures how spread out data values are from the center?", "answer": "Dispersion", "alternatives": ["Variability", "Spread"], "explanation": "Dispersion (or variability) quantifies how scattered data values are from the central value. Common measures include range (max - min), variance (average squared deviation), and standard deviation (square root of variance). High dispersion indicates data points are spread widely; low dispersion indicates clustering near the center."},
        {"question": "What is the format where each row represents a unique entity with attributes in columns?", "answer": "Tidy data", "alternatives": ["Long format", "Normalized data"], "explanation": "Tidy data follows principles where each variable is a column, each observation is a row, and each type of observational unit is a table. This format simplifies analysis and visualization in tools like R and Python. Converting data to tidy format is often the first step in data cleaning."},
        {"question": "What type of relationship exists when one variable increases as another increases?", "answer": "Positive correlation", "alternatives": ["Direct correlation"], "explanation": "Positive correlation occurs when two variables tend to move in the same direction - as one increases, the other tends to increase. Examples include height and weight, or study time and test scores. Correlation measures range from -1 to +1, with positive values indicating positive relationships."},
        {"question": "What principle states that 80% of effects come from 20% of causes?", "answer": "Pareto Principle", "alternatives": ["80-20 rule"], "explanation": "The Pareto Principle (80-20 rule) observes that roughly 80% of outcomes result from 20% of inputs. In data analysis, this might mean 80% of sales come from 20% of customers, or 80% of errors from 20% of bugs. This principle helps prioritize analysis and interventions on high-impact factors."}
    ],
    "difficult": [
        {"question": "What advanced data structure enables hierarchical relationships with parent-child nodes?", "answer": "Tree structure", "alternatives": ["Hierarchical data structure"], "explanation": "Tree structures organize data hierarchically with nodes connected by edges. Each node (except the root) has one parent and zero or more children. Trees are used for file systems, organizational charts, XML/JSON documents, and decision trees. Common types include binary trees, B-trees, and tries."},
        {"question": "What data storage format uses key-value pairs in nested structures?", "answer": "JSON", "alternatives": ["JavaScript Object Notation"], "explanation": "JSON (JavaScript Object Notation) is a lightweight, human-readable format for structured data using key-value pairs, arrays, and nested objects. It's language-independent and widely used for APIs, configuration files, and data interchange. JSON's simplicity and flexibility make it preferred over XML for many applications."},
        {"question": "What technique represents high-dimensional data in lower dimensions while preserving structure?", "answer": "Dimensionality reduction", "alternatives": ["Feature reduction"], "explanation": "Dimensionality reduction transforms high-dimensional data into fewer dimensions while retaining important information. Techniques include PCA (Principal Component Analysis), t-SNE, and autoencoders. This addresses the curse of dimensionality, enables visualization of complex data, and improves machine learning performance by removing redundant features."},
        {"question": "What data quality issue occurs when the same entity has multiple different representations?", "answer": "Data inconsistency", "alternatives": ["Conflicting data"], "explanation": "Data inconsistency arises when the same information is represented differently across records or systems. Examples include 'NY' vs 'New York', different date formats, or conflicting values for the same entity. Inconsistency hampers analysis and integration. Resolution requires standardization, deduplication, and data governance."},
        {"question": "What sampling technique divides population into groups and samples all members of selected groups?", "answer": "Cluster sampling", "alternatives": ["Area sampling"], "explanation": "Cluster sampling divides the population into clusters (groups) and randomly selects entire clusters for study. All members of selected clusters are included. This is cost-effective for geographically dispersed populations but may introduce bias if clusters aren't representative. It differs from stratified sampling, which samples within each stratum."},
        {"question": "What bias occurs when sample systematically differs from the population?", "answer": "Selection bias", "alternatives": ["Sampling bias"], "explanation": "Selection bias occurs when sample selection isn't random, causing systematic differences between the sample and population. This leads to non-representative samples and invalid inferences. Examples include volunteer bias, survivorship bias, and non-response bias. Random sampling and careful study design help minimize selection bias."},
        {"question": "What graph structure consists of nodes connected by edges without hierarchy?", "answer": "Network graph", "alternatives": ["Graph data structure"], "explanation": "Network graphs represent relationships as nodes (vertices) connected by edges (links). Unlike trees, graphs can have cycles and multiple connections. They model social networks, transportation systems, and web pages. Graph databases and algorithms analyze connectivity, shortest paths, centrality, and communities."},
        {"question": "What data quality dimension measures whether data values are within acceptable ranges?", "answer": "Data validity", "alternatives": ["Data conformity"], "explanation": "Data validity ensures values conform to defined formats, types, and business rules. Valid data falls within acceptable ranges, follows format constraints (e.g., email format), and satisfies domain rules. Validation rules prevent invalid data entry and can be enforced at collection, storage, or analysis stages."},
        {"question": "What technique assigns numerical values to categorical variables for machine learning?", "answer": "Encoding", "alternatives": ["Feature encoding", "Categorical encoding"], "explanation": "Encoding converts categorical variables to numerical format for machine learning algorithms. Techniques include label encoding (assigning integers), one-hot encoding (binary columns for each category), and target encoding (based on target variable statistics). The choice depends on whether categories have inherent order and the algorithm being used."},
        {"question": "What statistical measure indicates how much variance in one variable is explained by another?", "answer": "R-squared", "alternatives": ["Coefficient of determination"], "explanation": "R-squared (R²) measures the proportion of variance in the dependent variable predictable from independent variables. Values range from 0 to 1, where 1 indicates perfect prediction. In regression, R² of 0.80 means 80% of variance is explained by the model. However, high R² doesn't guarantee causation or model appropriateness."}
    ]
}

ALL_ADDITIONAL_QUESTIONS['data_fundamentals'] = DATA_FUNDAMENTALS_QUESTIONS

# 12. Statistics
STATISTICS_QUESTIONS = {
    "easy": [
        {"question": "What is the arithmetic average of a set of numbers?", "answer": "Mean", "alternatives": ["Average", "Arithmetic mean"], "explanation": "The mean is calculated by summing all values and dividing by the count. It's the most common measure of central tendency. For example, the mean of 2, 4, 6 is (2+4+6)/3 = 4. The mean is sensitive to outliers, which can pull it away from the typical value."},
        {"question": "What is the middle value when data is arranged in order?", "answer": "Median", "alternatives": ["Middle value"], "explanation": "The median is the value separating the higher half from the lower half of data. For odd-sized datasets, it's the middle number; for even-sized, it's the average of the two middle numbers. The median is resistant to outliers, making it better than mean for skewed distributions."},
        {"question": "What is the most frequently occurring value in a dataset?", "answer": "Mode", "alternatives": ["Modal value"], "explanation": "The mode is the value that appears most often in a dataset. A dataset can have one mode (unimodal), multiple modes (bimodal, multimodal), or no mode if all values are unique. Mode is useful for categorical data where mean and median don't apply."},
        {"question": "What measures the likelihood of an event occurring?", "answer": "Probability", "alternatives": ["Chance"], "explanation": "Probability quantifies the likelihood of events, ranging from 0 (impossible) to 1 (certain). Classical probability is favorable outcomes divided by total outcomes. For example, the probability of rolling a 6 on a die is 1/6. Probability is fundamental to statistics, machine learning, and decision-making."},
        {"question": "What is the difference between the largest and smallest values?", "answer": "Range", "alternatives": ["Data range"], "explanation": "Range is the simplest measure of dispersion, calculated as maximum minus minimum value. It gives a quick sense of data spread but is highly sensitive to outliers. For example, dataset [1, 3, 5, 7, 100] has range 99, dominated by the outlier 100."},
        {"question": "What graph uses bars to show frequency of different categories?", "answer": "Bar chart", "alternatives": ["Bar graph"], "explanation": "Bar charts display categorical data using rectangular bars whose lengths represent values or frequencies. Each bar represents a category, and bars can be vertical or horizontal. Bar charts are ideal for comparing quantities across categories, like sales by product or population by country."},
        {"question": "What graph shows the distribution of numerical data using bins?", "answer": "Histogram", "alternatives": ["Frequency histogram"], "explanation": "Histograms display continuous data distribution by dividing the range into intervals (bins) and showing frequency in each bin with bars. Unlike bar charts (for categories), histogram bars touch because they represent continuous ranges. Histograms reveal distribution shape, central tendency, and spread."},
        {"question": "What type of distribution has data symmetrically centered around the mean?", "answer": "Normal distribution", "alternatives": ["Bell curve", "Gaussian distribution"], "explanation": "Normal distribution is a symmetric, bell-shaped curve where data clusters around the mean. Many natural phenomena follow this pattern. In normal distribution, mean = median = mode, and about 68% of data falls within one standard deviation of the mean. It's foundational to many statistical methods."},
        {"question": "What measures the average squared deviation from the mean?", "answer": "Variance", "alternatives": ["Statistical variance"], "explanation": "Variance measures data spread by averaging squared deviations from the mean. Squaring ensures positive values and emphasizes larger deviations. While variance is mathematically useful, its units are squared (e.g., meters²), making interpretation difficult. Standard deviation (square root of variance) is more interpretable."},
        {"question": "What is the square root of variance?", "answer": "Standard deviation", "alternatives": ["SD", "Std dev"], "explanation": "Standard deviation measures typical distance of data points from the mean, in the same units as the data. It's the most common dispersion measure. Low standard deviation means data clusters near the mean; high standard deviation means data is spread out. In normal distributions, ±1 SD contains ~68% of data."}
    ],
    "average": [
        {"question": "What test determines if differences between groups are statistically significant?", "answer": "Hypothesis test", "alternatives": ["Statistical significance test"], "explanation": "Hypothesis testing evaluates whether observed differences are likely due to chance or represent real effects. It involves null hypothesis (no effect), alternative hypothesis (effect exists), and calculating probability (p-value) of observing the data if null is true. P < 0.05 typically indicates statistical significance."},
        {"question": "What is the probability of rejecting a true null hypothesis?", "answer": "Type I error", "alternatives": ["False positive", "Alpha error"], "explanation": "Type I error (false positive) occurs when we conclude an effect exists when it doesn't. The significance level (α, typically 0.05) is the acceptable Type I error rate. For example, claiming a drug works when it doesn't. Lowering α reduces Type I errors but increases Type II errors."},
        {"question": "What measure indicates strength and direction of linear relationship between variables?", "answer": "Correlation coefficient", "alternatives": ["Pearson correlation"], "explanation": "The correlation coefficient (r) measures linear relationship strength and direction between two variables, ranging from -1 (perfect negative) to +1 (perfect positive). Zero indicates no linear relationship. Correlation doesn't imply causation - correlated variables may share a common cause or correlation may be coincidental."},
        {"question": "What statistical technique models relationship between dependent and independent variables?", "answer": "Regression analysis", "alternatives": ["Regression"], "explanation": "Regression analysis models relationships between variables to predict outcomes. Linear regression fits a line to data; multiple regression uses several predictors. Regression provides coefficients showing each variable's effect, and R² indicating model fit. It's used for prediction, understanding relationships, and testing hypotheses."},
        {"question": "What distribution models the number of events in a fixed interval?", "answer": "Poisson distribution", "alternatives": ["Poisson"], "explanation": "Poisson distribution models count of events occurring in fixed intervals of time or space when events happen independently at a constant rate. Examples include customer arrivals per hour, defects per unit, or emails per day. It's characterized by parameter λ (lambda), the average rate of occurrence."},
        {"question": "What sampling method divides population into subgroups and samples from each?", "answer": "Stratified sampling", "alternatives": ["Stratified random sampling"], "explanation": "Stratified sampling divides the population into homogeneous subgroups (strata) based on characteristics, then randomly samples from each stratum. This ensures representation of all subgroups and can improve precision compared to simple random sampling. Proportional allocation maintains population proportions; equal allocation samples equally from each stratum."},
        {"question": "What measure is unaffected by extreme values in the data?", "answer": "Robust statistic", "alternatives": ["Resistant measure"], "explanation": "Robust statistics resist influence of outliers and extreme values. Median is robust (unlike mean); interquartile range is robust (unlike range); median absolute deviation is robust (unlike standard deviation). Robust measures provide more reliable estimates for skewed or contaminated data."},
        {"question": "What describes the shape of probability distribution tails?", "answer": "Kurtosis", "alternatives": ["Tail heaviness"], "explanation": "Kurtosis measures tail heaviness and peakedness of distributions. High kurtosis (leptokurtic) indicates heavy tails and sharp peak; low kurtosis (platykurtic) indicates light tails and flat peak. Normal distribution has kurtosis of 3. Excess kurtosis subtracts 3, making normal distribution have excess kurtosis of 0."},
        {"question": "What indicates whether distribution is symmetric or leans to one side?", "answer": "Skewness", "alternatives": ["Asymmetry"], "explanation": "Skewness measures distribution asymmetry. Positive skew (right-skewed) has a long right tail; negative skew (left-skewed) has a long left tail. Zero skewness indicates symmetry. In right-skewed data, mean > median > mode. Income distributions are typically right-skewed with few extremely high values."},
        {"question": "What theorem states that sample means approach normal distribution as sample size increases?", "answer": "Central Limit Theorem", "alternatives": ["CLT"], "explanation": "The Central Limit Theorem (CLT) states that regardless of population distribution, the distribution of sample means approaches normality as sample size increases (typically n ≥ 30). This is why normal distribution is so important - it justifies using normal-based inference methods even when underlying data isn't normal."}
    ],
    "difficult": [
        {"question": "What Bayesian concept updates probability based on new evidence?", "answer": "Posterior probability", "alternatives": ["Updated probability"], "explanation": "Posterior probability is the updated probability after considering new evidence, calculated using Bayes' theorem: P(H|E) = P(E|H) × P(H) / P(E). It combines prior probability (initial belief) with likelihood (evidence fit) to produce posterior probability (updated belief). Bayesian inference iteratively updates beliefs as new data arrives."},
        {"question": "What resampling technique estimates sampling distribution by repeatedly sampling from data?", "answer": "Bootstrap", "alternatives": ["Bootstrapping"], "explanation": "Bootstrap is a resampling method that repeatedly samples with replacement from observed data to estimate sampling distributions and calculate confidence intervals without distributional assumptions. By creating thousands of resamples, bootstrap approximates what would happen with repeated sampling from the population, providing robust uncertainty estimates."},
        {"question": "What technique tests multiple hypotheses while controlling false discovery rate?", "answer": "Multiple testing correction", "alternatives": ["Bonferroni correction", "FDR correction"], "explanation": "Multiple testing correction adjusts significance thresholds when testing many hypotheses simultaneously to control false positives. Bonferroni correction divides α by number of tests (conservative); False Discovery Rate (FDR) methods like Benjamini-Hochberg are less conservative. Without correction, testing 100 hypotheses at α=0.05 expects 5 false positives by chance."},
        {"question": "What distribution models time between events in a Poisson process?", "answer": "Exponential distribution", "alternatives": ["Exponential"], "explanation": "The exponential distribution models waiting time between events in a Poisson process where events occur continuously and independently at constant rate. It's memoryless - past waiting doesn't affect future waiting time. Examples include time between customer arrivals, equipment failures, or radioactive decay. It's characterized by rate parameter λ."},
        {"question": "What statistical framework treats parameters as random variables with prior distributions?", "answer": "Bayesian statistics", "alternatives": ["Bayesian inference"], "explanation": "Bayesian statistics treats unknown parameters as random variables with probability distributions expressing uncertainty. It combines prior knowledge (prior distribution) with data (likelihood) via Bayes' theorem to get posterior distribution. Unlike frequentist methods focusing on long-run frequencies, Bayesian methods provide probability statements about parameters directly."},
        {"question": "What method selects models balancing goodness of fit with complexity?", "answer": "AIC", "alternatives": ["Akaike Information Criterion"], "explanation": "Akaike Information Criterion (AIC) balances model fit with complexity, penalizing additional parameters to prevent overfitting. Lower AIC indicates better model. AIC = 2k - 2ln(L), where k is parameters and L is likelihood. BIC (Bayesian Information Criterion) is similar but penalizes complexity more heavily. These help choose among competing models."},
        {"question": "What test compares distributions between two samples without assuming normal distribution?", "answer": "Mann-Whitney U test", "alternatives": ["Wilcoxon rank-sum test"], "explanation": "The Mann-Whitney U test (or Wilcoxon rank-sum test) is a non-parametric test comparing two independent samples without assuming normality. It tests whether distributions differ by ranking all values and comparing rank sums. It's more robust than t-tests for non-normal or ordinal data but less powerful when normality holds."},
        {"question": "What process has independent increments and continuous sample paths?", "answer": "Brownian motion", "alternatives": ["Wiener process"], "explanation": "Brownian motion (Wiener process) is a continuous stochastic process with independent normally distributed increments. It models random movement like stock prices or particle diffusion. Properties include continuity everywhere, differentiability nowhere, and quadratic variation proportional to time. It's fundamental to stochastic calculus and financial mathematics."},
        {"question": "What time series model combines autoregression and moving average components?", "answer": "ARIMA", "alternatives": ["Autoregressive Integrated Moving Average"], "explanation": "ARIMA models forecast time series by combining AR (autoregressive - using past values), I (integrated - differencing for stationarity), and MA (moving average - using past errors). ARIMA(p,d,q) specifies order of each component. It's powerful for forecasting but requires stationary data and careful parameter selection. SARIMA adds seasonal components."},
        {"question": "What measures information gained from knowing one variable about another?", "answer": "Mutual information", "alternatives": ["Information gain"], "explanation": "Mutual information quantifies the amount of information one variable provides about another, measuring dependence between variables. Unlike correlation (which captures only linear relationships), mutual information detects any type of dependence. It's based on entropy from information theory and is used in feature selection, variable importance, and causal inference."}
    ]
}

ALL_ADDITIONAL_QUESTIONS['statistics'] = STATISTICS_QUESTIONS

# 13. Data Preprocessing
DATA_PREPROCESSING_QUESTIONS = {
    "easy": [
        {"question": "What is the process of identifying and removing errors from data?", "answer": "Data cleaning", "alternatives": ["Data cleansing"], "explanation": "Data cleaning identifies and corrects errors, inconsistencies, and inaccuracies in datasets. This includes handling missing values, removing duplicates, correcting typos, and fixing formatting issues. Clean data is essential for accurate analysis - garbage in, garbage out. Data scientists spend 60-80% of their time on data cleaning."},
        {"question": "What technique replaces missing values with substituted values?", "answer": "Imputation", "alternatives": ["Missing value imputation"], "explanation": "Imputation fills missing values with estimated values rather than deleting records. Common methods include mean/median/mode imputation (simple but ignores relationships), forward/backward fill (for time series), and advanced methods like k-NN or model-based imputation. The choice depends on data type, missingness pattern, and analysis goals."},
        {"question": "What process converts data to a common scale without distorting differences?", "answer": "Normalization", "alternatives": ["Feature scaling"], "explanation": "Normalization (or feature scaling) transforms features to a common scale, typically 0 to 1 or -1 to 1. Min-max scaling maps values to [0,1]; z-score standardization centers around 0 with unit variance. Normalization prevents features with large ranges from dominating and is essential for distance-based algorithms and neural networks."},
        {"question": "What are identical records that appear more than once in a dataset?", "answer": "Duplicates", "alternatives": ["Duplicate records"], "explanation": "Duplicates are exact or near-exact copies of records that artificially inflate dataset size and can bias analysis. They arise from data integration, entry errors, or system issues. Deduplication identifies and removes duplicates while preserving unique records. Fuzzy matching helps find near-duplicates with slight differences."},
        {"question": "What is the process of converting categorical variables into numerical format?", "answer": "Encoding", "alternatives": ["Categorical encoding"], "explanation": "Encoding transforms categorical data into numerical format for machine learning. Label encoding assigns integers (0, 1, 2...); one-hot encoding creates binary columns for each category; target encoding uses target statistics. The choice depends on whether categories have order and the algorithm requirements."},
        {"question": "What values are significantly different from other observations?", "answer": "Outliers", "alternatives": ["Anomalies"], "explanation": "Outliers are data points substantially different from others, potentially indicating errors, rare events, or interesting phenomena. They can skew statistics and affect model performance. Detection methods include z-score, IQR, or visualization. Treatment options include removal, transformation, or robust methods that minimize outlier influence."},
        {"question": "What process combines data from multiple sources into a unified view?", "answer": "Data integration", "alternatives": ["Data merging"], "explanation": "Data integration combines data from different sources (databases, files, APIs) into a coherent dataset. Challenges include schema matching, entity resolution, handling inconsistencies, and ensuring data quality. Common operations include joining tables on keys, appending rows, and resolving conflicts between sources."},
        {"question": "What technique groups continuous data into discrete intervals?", "answer": "Binning", "alternatives": ["Discretization"], "explanation": "Binning (or discretization) converts continuous variables into categorical bins or ranges. For example, age might be binned into child/teen/adult/senior. Binning can reduce noise, handle outliers, and reveal non-linear patterns, but it loses precision. Equal-width bins have same size; equal-frequency bins have same count."},
        {"question": "What is the process of reducing data size while preserving important information?", "answer": "Data reduction", "alternatives": ["Dimensionality reduction"], "explanation": "Data reduction decreases data volume while maintaining analytical integrity. Techniques include sampling (fewer records), feature selection (fewer features), dimensionality reduction (PCA, t-SNE), and data compression. Reduction improves processing speed, reduces storage, and can enhance model performance by removing noise."},
        {"question": "What process ensures data follows defined rules and formats?", "answer": "Data validation", "alternatives": ["Validation"], "explanation": "Data validation checks whether data meets defined criteria, formats, and business rules. Validation rules include type checking (is it a number?), range checking (within valid bounds?), format validation (proper email format?), and consistency checks (does state match zip code?). Validation prevents invalid data from entering systems."}
    ],
    "average": [
        {"question": "What statistical method replaces outliers based on distribution properties?", "answer": "Winsorization", "alternatives": ["Winsorizing"], "explanation": "Winsorization replaces extreme values with less extreme values at specified percentiles rather than removing them. For example, 5% winsorization replaces values below 5th percentile with the 5th percentile value and above 95th with the 95th. This reduces outlier impact while preserving sample size, unlike trimming which removes outliers entirely."},
        {"question": "What technique applies mathematical functions to reduce data skewness?", "answer": "Data transformation", "alternatives": ["Variable transformation"], "explanation": "Data transformation applies mathematical functions to modify variable distributions. Log transformation reduces right skew; square root moderates skew; Box-Cox finds optimal transformation. Transformations can normalize distributions, stabilize variance, linearize relationships, and make data suitable for parametric methods. The inverse transformation recovers original scale."},
        {"question": "What method selects important features while removing irrelevant ones?", "answer": "Feature selection", "alternatives": ["Variable selection"], "explanation": "Feature selection identifies and retains most relevant features while removing irrelevant or redundant ones. Methods include filter (statistical tests), wrapper (model performance), and embedded (built into algorithms like Lasso). Benefits include reduced overfitting, faster training, improved interpretability, and addressing curse of dimensionality."},
        {"question": "What creates new features from existing ones to improve model performance?", "answer": "Feature engineering", "alternatives": ["Feature construction"], "explanation": "Feature engineering creates new predictive features from raw data using domain knowledge and creativity. Examples include combining features (BMI from height/weight), extracting date components (day of week), aggregating (moving averages), or encoding interactions. Good feature engineering often improves model performance more than algorithm tuning."},
        {"question": "What technique handles imbalanced datasets by generating synthetic minority samples?", "answer": "SMOTE", "alternatives": ["Synthetic Minority Over-sampling Technique"], "explanation": "SMOTE (Synthetic Minority Over-sampling Technique) addresses class imbalance by creating synthetic examples of minority class. It identifies k-nearest neighbors for minority samples and creates new samples along lines between them. SMOTE improves model performance on minority class better than simple oversampling (duplication) while avoiding overfitting."},
        {"question": "What standardization method transforms data to have mean 0 and standard deviation 1?", "answer": "Z-score normalization", "alternatives": ["Standardization", "Z-score scaling"], "explanation": "Z-score normalization (standardization) transforms data by subtracting mean and dividing by standard deviation: z = (x - μ) / σ. Result has mean 0 and standard deviation 1. Unlike min-max scaling, z-score preserves outliers but is less bounded. It's preferred for algorithms assuming normally distributed features like linear regression."},
        {"question": "What sampling technique reduces majority class to balance with minority class?", "answer": "Undersampling", "alternatives": ["Downsampling"], "explanation": "Undersampling reduces majority class samples to balance with minority class in imbalanced datasets. Random undersampling removes samples randomly; advanced methods like Tomek links or near-miss remove specific samples strategically. While simple, undersampling discards potentially useful information. Combining with oversampling (SMOTE) often works best."},
        {"question": "What method uses multiple imputation to handle missing data uncertainty?", "answer": "Multiple imputation", "alternatives": ["MI"], "explanation": "Multiple imputation creates several complete datasets with different imputed values, analyzes each separately, then combines results accounting for imputation uncertainty. This captures uncertainty better than single imputation. The process: impute multiple times, analyze each dataset, pool results using special combination rules. MI provides valid statistical inference with missing data."},
        {"question": "What technique identifies and flags unusual patterns that don't conform to expected behavior?", "answer": "Anomaly detection", "alternatives": ["Outlier detection"], "explanation": "Anomaly detection identifies unusual patterns deviating from normal behavior. Methods include statistical (z-score, IQR), distance-based (k-NN), density-based (LOF), and machine learning (Isolation Forest, Autoencoders). Applications include fraud detection, network security, quality control, and system health monitoring. Anomalies may indicate errors or interesting discoveries."},
        {"question": "What process converts text data into numerical vectors for machine learning?", "answer": "Text vectorization", "alternatives": ["Text encoding"], "explanation": "Text vectorization transforms text into numerical vectors for machine learning. Methods include Bag of Words (word counts), TF-IDF (term importance), word embeddings (Word2Vec, GloVe), and transformer embeddings (BERT). Choice depends on context importance, semantic understanding needed, and computational resources. Vectorization enables text classification, sentiment analysis, and NLP tasks."}
    ],
    "difficult": [
        {"question": "What advanced imputation technique uses machine learning models to predict missing values?", "answer": "Model-based imputation", "alternatives": ["Predictive imputation"], "explanation": "Model-based imputation trains machine learning models (regression, k-NN, random forest) using complete cases to predict missing values based on other features. This captures complex relationships better than simple methods. Iterative imputation (MICE - Multiple Imputation by Chained Equations) sequentially imputes each variable using others, iterating until convergence."},
        {"question": "What technique reduces dimensionality while preserving local structure in high-dimensional data?", "answer": "t-SNE", "alternatives": ["t-Distributed Stochastic Neighbor Embedding"], "explanation": "t-SNE (t-Distributed Stochastic Neighbor Embedding) is a non-linear dimensionality reduction technique excellent for visualization. It preserves local structure (similar points stay close) by minimizing divergence between high and low-dimensional probability distributions. Unlike PCA, t-SNE captures non-linear patterns but doesn't provide explicit mapping for new data and is computationally intensive."},
        {"question": "What process automatically detects and corrects systematic errors in data?", "answer": "Data profiling", "alternatives": ["Statistical profiling"], "explanation": "Data profiling systematically examines data to understand structure, content, quality, and relationships. It generates statistics (distributions, patterns, anomalies), validates consistency, identifies quality issues, and discovers relationships. Profiling tools automate detection of data types, missing values, duplicates, outliers, and referential integrity violations, guiding cleaning strategies."},
        {"question": "What technique handles mixed data types by creating appropriate distance metrics?", "answer": "Gower distance", "alternatives": ["Gower similarity"], "explanation": "Gower distance computes dissimilarity for mixed-type data (numerical, categorical, binary) by combining different distance metrics appropriately. Numerical features use range-normalized Manhattan distance; categorical use simple matching. This enables clustering and similarity analysis on heterogeneous datasets common in real applications. Each feature contributes proportionally to overall distance."},
        {"question": "What method addresses missing data by modeling the missingness mechanism?", "answer": "Maximum likelihood estimation", "alternatives": ["ML estimation with missing data"], "explanation": "Maximum Likelihood Estimation (MLE) with missing data directly estimates parameters by maximizing likelihood over observed data, implicitly handling missing values. EM (Expectation-Maximization) algorithm alternates between imputing missing data (E-step) and estimating parameters (M-step). This is theoretically principled but computationally intensive, providing optimal estimates under correct model assumptions."},
        {"question": "What technique learns low-dimensional representation preserving data manifold structure?", "answer": "Manifold learning", "alternatives": ["Non-linear dimensionality reduction"], "explanation": "Manifold learning assumes high-dimensional data lies on a lower-dimensional manifold and learns this structure. Techniques include Isomap (preserves geodesic distances), Locally Linear Embedding (preserves local neighborhoods), and autoencoders (neural network-based). Unlike PCA, these capture non-linear structure but can be sensitive to noise and parameters."},
        {"question": "What advanced technique balances classes by combining oversampling and undersampling?", "answer": "SMOTEENN", "alternatives": ["SMOTE with Edited Nearest Neighbors"], "explanation": "SMOTEENN combines SMOTE oversampling with Edited Nearest Neighbors undersampling. First, SMOTE generates minority class synthetics. Then ENN removes samples whose class differs from majority of k-nearest neighbors, cleaning class overlap regions. This balances classes while removing noisy borderline cases, improving decision boundaries and model performance."},
        {"question": "What process uses domain knowledge to create derived features capturing complex patterns?", "answer": "Domain-specific feature engineering", "alternatives": ["Expert feature engineering"], "explanation": "Domain-specific feature engineering leverages expert knowledge to create meaningful features. Examples include financial ratios from raw accounting data, technical indicators from stock prices, or clinical scores from medical measurements. These engineered features encode domain understanding that models might not discover independently, often dramatically improving performance and interpretability."},
        {"question": "What technique reduces correlation between features while preserving information?", "answer": "Decorrelation", "alternatives": ["Whitening transformation"], "explanation": "Decorrelation (or whitening) transforms features to have zero correlation and unit variance. PCA-based whitening rotates data to principal components then scales; ZCA whitening maintains proximity to original data. Decorrelated features prevent redundancy, improve optimization in neural networks, and help algorithms that assume feature independence. This differs from simple standardization which doesn't remove correlations."},
        {"question": "What advanced method handles concept drift in streaming data?", "answer": "Adaptive preprocessing", "alternatives": ["Online preprocessing"], "explanation": "Adaptive preprocessing adjusts data transformation parameters over time to handle concept drift in streaming data. This includes incremental updating of normalization statistics, adaptive imputation based on recent patterns, and dynamic feature selection as relationships evolve. Online learning algorithms continuously update models as distributions change, crucial for real-time systems with non-stationary data."}
    ]
}

ALL_ADDITIONAL_QUESTIONS['data_preprocessing'] = DATA_PREPROCESSING_QUESTIONS

# 14. Data Visualization
DATA_VISUALIZATION_QUESTIONS = {
    "easy": [
        {"question": "What type of chart uses rectangular bars to compare values across categories?", "answer": "Bar chart", "alternatives": ["Bar graph"], "explanation": "Bar charts display categorical data with rectangular bars representing values. Bars can be vertical (column chart) or horizontal. They're ideal for comparing quantities across discrete categories like sales by region or preferences by group. Bar length encodes value, making comparisons intuitive."},
        {"question": "What chart type shows data points as dots on two axes?", "answer": "Scatter plot", "alternatives": ["Scatterplot"], "explanation": "Scatter plots display relationships between two numerical variables by plotting points on x and y axes. They reveal correlations, clusters, and outliers. Each point represents an observation with coordinates determined by variable values. Adding trend lines or colors for categories enhances insight."},
        {"question": "What visualization shows data distribution using quartiles and potential outliers?", "answer": "Box plot", "alternatives": ["Box-and-whisker plot"], "explanation": "Box plots display data distribution through five-number summary: minimum, Q1 (25th percentile), median (Q2), Q3 (75th percentile), and maximum. The box spans Q1 to Q3 (interquartile range), with a line at the median. Whiskers extend to extremes within 1.5×IQR; points beyond are outliers."},
        {"question": "What chart shows parts of a whole as slices of a circle?", "answer": "Pie chart", "alternatives": ["Pie graph"], "explanation": "Pie charts display proportions of a whole using circular slices where each slice's angle represents its percentage. While popular, they're limited to showing composition of a single total and comparing slices is difficult. Bar charts are often clearer for multiple categories or precise comparisons."},
        {"question": "What visualization displays data values as colors in a matrix?", "answer": "Heatmap", "alternatives": ["Heat map"], "explanation": "Heatmaps use color intensity to represent data values in a matrix format. They're excellent for revealing patterns in large datasets, showing correlations between variables, or visualizing data over two dimensions like time and categories. Color scales should be chosen carefully for accessibility and clarity."},
        {"question": "What chart connects data points with line segments to show trends?", "answer": "Line chart", "alternatives": ["Line graph"], "explanation": "Line charts connect sequential data points with lines, ideal for showing trends over time or continuous variables. Multiple lines can compare different series. Line charts emphasize overall patterns and direction of change. They're most effective when data has a natural order."},
        {"question": "What visualization shows the distribution of continuous data using adjacent bars?", "answer": "Histogram", "alternatives": ["Frequency histogram"], "explanation": "Histograms display continuous data distribution by grouping values into bins and showing frequency with bar heights. Unlike bar charts (categorical), histogram bars touch because they represent continuous ranges. Bin width affects appearance - too wide loses detail, too narrow looks noisy."},
        {"question": "What principle states that visualizations should maximize data-ink ratio?", "answer": "Data-ink ratio", "alternatives": ["Tufte's principle"], "explanation": "Data-ink ratio, introduced by Edward Tufte, is the proportion of ink dedicated to displaying actual data versus decoration. High data-ink ratio removes chart junk (unnecessary decorations) and maximizes information. This principle encourages minimalist, clear visualizations focusing on data rather than embellishment."},
        {"question": "What color scheme uses sequential shades of one color?", "answer": "Sequential color scale", "alternatives": ["Gradient color scale"], "explanation": "Sequential color scales use gradual changes in lightness/saturation of a single color to represent ordered data from low to high. They're ideal for showing magnitude or density. Examples include light blue to dark blue for elevation or temperature data."},
        {"question": "What type of visualization combines a box plot with actual data points?", "answer": "Violin plot", "alternatives": ["Violin chart"], "explanation": "Violin plots combine box plots with kernel density plots, showing distribution shape on both sides of the box. They reveal multimodality and distribution details better than box plots alone. Width at each value indicates density, providing a richer view of data distribution."}
    ],
    "average": [
        {"question": "What visualization technique shows hierarchical data as nested rectangles?", "answer": "Treemap", "alternatives": ["Tree map"], "explanation": "Treemaps display hierarchical data as nested rectangles where size represents a quantitative value and color can represent another dimension or category. They efficiently use space to show part-to-whole relationships across hierarchies. Common uses include file system visualization and market segment analysis."},
        {"question": "What chart type shows the relationship between three variables using bubble size?", "answer": "Bubble chart", "alternatives": ["Bubble plot"], "explanation": "Bubble charts extend scatter plots by adding a third dimension through bubble size. X and Y axes show two variables, while bubble area represents a third. Color can add a fourth dimension. They're effective for multidimensional data but can become cluttered with many points."},
        {"question": "What interactive visualization allows drilling down into hierarchical data?", "answer": "Sunburst diagram", "alternatives": ["Sunburst chart"], "explanation": "Sunburst diagrams visualize hierarchical data in a radial layout with concentric rings. The innermost ring represents the root, with each ring showing children of the previous level. Segments encode hierarchy and proportion. Interactive versions allow clicking to focus on subtrees, making them excellent for exploring deep hierarchies."},
        {"question": "What technique reduces overplotting by adding random noise to points?", "answer": "Jittering", "alternatives": ["Jitter"], "explanation": "Jittering adds small random displacements to data points to reduce overplotting when many points share similar values. This reveals overlapping points without substantially changing their position. Common in scatter plots and strip plots with discrete or rounded values. Too much jitter misrepresents data; too little doesn't help."},
        {"question": "What visualization shows correlation matrix as a colored grid?", "answer": "Correlation heatmap", "alternatives": ["Correlation matrix heatmap"], "explanation": "Correlation heatmaps display correlation matrices with color intensity representing correlation strength. Positive correlations might be blue, negative red, with intensity showing strength. Clustering similar variables together reveals patterns. They're essential for understanding relationships in multivariate data and identifying collinearity."},
        {"question": "What type of chart shows cumulative values over time?", "answer": "Area chart", "alternatives": ["Stacked area chart"], "explanation": "Area charts extend line charts by filling the area below lines, emphasizing cumulative magnitude. Stacked area charts show multiple series' contribution to total. They're effective for showing composition changes over time. However, only the bottom series has clear baseline making upper series harder to read."},
        {"question": "What principle states people perceive visualizations following specific patterns?", "answer": "Gestalt principles", "alternatives": ["Gestalt theory"], "explanation": "Gestalt principles describe how humans perceive visual elements as unified wholes. Key principles include proximity (nearby elements are grouped), similarity (similar elements are grouped), continuity (smooth paths are preferred), and closure (completing incomplete shapes). Applying these creates intuitive, effective visualizations."},
        {"question": "What chart compares values across two categorical dimensions in a grid?", "answer": "Heatmap grid", "alternatives": ["Categorical heatmap"], "explanation": "Categorical heatmaps display values for combinations of two categorical variables in a grid format. Rows and columns represent categories, and cell colors encode values. They're excellent for comparing patterns across category combinations, like sales by product and region or error rates by model and dataset."},
        {"question": "What technique creates multiple coordinated views of the same data?", "answer": "Small multiples", "alternatives": ["Trellis plots", "Faceting"], "explanation": "Small multiples (or trellis plots/faceting) create multiple charts with identical scales showing different subsets of data. This enables easy comparison across categories or time periods. Edward Tufte popularized this technique, which efficiently uses space while maintaining consistency for direct comparison."},
        {"question": "What color scheme uses diverging colors for data with critical midpoint?", "answer": "Diverging color scale", "alternatives": ["Divergent palette"], "explanation": "Diverging color scales use two contrasting colors for extremes with a neutral midpoint, ideal for data with meaningful zero or average. Examples include blue-white-red for temperature anomalies or green-yellow-red for performance. They emphasize deviation from center in both directions simultaneously."}
    ],
    "difficult": [
        {"question": "What advanced technique visualizes high-dimensional data through coordinated plots?", "answer": "Parallel coordinates plot", "alternatives": ["Parallel coordinates"], "explanation": "Parallel coordinates plot displays multivariate data by placing variables on parallel vertical axes. Each observation is a polyline connecting its values across axes. This reveals patterns, clusters, and correlations in high-dimensional data. Axis ordering affects pattern visibility, and interactive versions allow brushing and linking."},
        {"question": "What visualization technique shows network relationships as nodes and edges?", "answer": "Network diagram", "alternatives": ["Graph visualization", "Node-link diagram"], "explanation": "Network diagrams visualize relationships as nodes (entities) connected by edges (relationships). Layout algorithms position nodes to reveal structure: force-directed layouts show clusters, hierarchical layouts show dependencies, circular layouts show relationships. Size, color, and edge thickness encode additional dimensions. Applications include social networks, biological pathways, and system architectures."},
        {"question": "What technique creates interactive visualizations allowing data exploration?", "answer": "Linked brushing", "alternatives": ["Brushing and linking"], "explanation": "Linked brushing connects multiple visualizations so selecting data in one highlights corresponding data in others. This enables exploration of relationships across different views. For example, selecting points in a scatter plot highlights corresponding bars in a histogram. This interaction pattern is fundamental to exploratory data analysis tools."},
        {"question": "What method visualizes dimensionality reduction results in 2D or 3D space?", "answer": "Embedding visualization", "alternatives": ["Projection visualization"], "explanation": "Embedding visualizations display high-dimensional data projected to 2D or 3D using techniques like PCA, t-SNE, or UMAP. Points represent high-dimensional observations, with proximity indicating similarity in original space. These reveal clusters, outliers, and data structure patterns invisible in high dimensions, though some distortion is inevitable."},
        {"question": "What advanced chart shows distribution changes across a categorical axis?", "answer": "Ridgeline plot", "alternatives": ["Joy plot"], "explanation": "Ridgeline plots (formerly joy plots) display multiple density plots stacked vertically with partial overlap, enabling comparison of distributions across categories. They're effective for visualizing trends over time or comparing many distributions. The overlap creates compact, aesthetically pleasing visualizations while maintaining readability."},
        {"question": "What technique adjusts visual properties based on data to reveal patterns?", "answer": "Data-driven styling", "alternatives": ["Conditional formatting"], "explanation": "Data-driven styling dynamically adjusts visual properties (color, size, opacity, shape) based on data values or derived metrics. Examples include coloring points by cluster, sizing nodes by centrality, or varying opacity by confidence. This adds information density while maintaining clarity, revealing patterns that might otherwise be hidden."},
        {"question": "What visualization shows temporal patterns in cyclic data using radial layout?", "answer": "Radial plot", "alternatives": ["Circular plot", "Polar plot"], "explanation": "Radial plots use circular/polar coordinates to show cyclic patterns in temporal data. Examples include hour-of-day patterns, seasonal trends, or directional data. Wind roses show wind direction and speed; circular histograms show daily activity patterns. Radial layouts naturally represent cyclic continuity (e.g., hour 23 connects to hour 0)."},
        {"question": "What technique handles overplotting by showing density instead of individual points?", "answer": "Density visualization", "alternatives": ["2D density plot", "Hexbin plot"], "explanation": "Density visualizations replace individual points with density estimates, addressing overplotting in large datasets. Hexbin plots count points in hexagonal bins; 2D kernel density estimation shows smooth density surfaces; contour plots show density levels. These reveal patterns invisible in cluttered scatter plots while maintaining spatial relationships."},
        {"question": "What advanced method creates dynamic visualizations responding to user interaction?", "answer": "Interactive data visualization", "alternatives": ["Dynamic visualization"], "explanation": "Interactive visualizations enable exploration through user actions: zooming, filtering, details-on-demand, and linked views. Technologies include D3.js, Plotly, and Bokeh. Interaction patterns like drill-down, cross-filtering, and brushing-and-linking support hypothesis generation and insight discovery. Effective interaction design balances functionality with simplicity."},
        {"question": "What principle guides choosing appropriate visualization for data and message?", "answer": "Visual encoding", "alternatives": ["Graphical perception"], "explanation": "Visual encoding refers to mapping data values to visual properties (position, length, angle, area, color, etc.). Cleveland and McGill's research shows position is most accurately perceived, followed by length, angle, area, and color. Effective visualization chooses encodings matching data type (nominal, ordinal, quantitative) and emphasizing important comparisons."}
    ]
}

ALL_ADDITIONAL_QUESTIONS['data_visualization'] = DATA_VISUALIZATION_QUESTIONS

# 15. Machine Learning Basics
ML_BASICS_QUESTIONS = {
    "easy": [
        {"question": "What type of learning uses labeled training data to make predictions?", "answer": "Supervised learning", "alternatives": ["Supervised machine learning"], "explanation": "Supervised learning trains models on labeled data (input-output pairs) to predict outputs for new inputs. Examples include classification (predicting categories) and regression (predicting numbers). The model learns from examples, then applies learned patterns to unseen data. Most practical ML applications use supervised learning."},
        {"question": "What type of learning finds patterns in unlabeled data?", "answer": "Unsupervised learning", "alternatives": ["Unsupervised machine learning"], "explanation": "Unsupervised learning discovers patterns and structure in data without labels. Tasks include clustering (grouping similar items), dimensionality reduction (simplifying data), and anomaly detection (finding unusual patterns). It's used for customer segmentation, data exploration, and finding hidden structures in complex datasets."},
        {"question": "What is the data used to train a machine learning model?", "answer": "Training data", "alternatives": ["Training set"], "explanation": "Training data is the labeled dataset used to teach models. It contains examples with inputs (features) and correct outputs (labels). Models learn patterns and relationships from training data. Quality and quantity of training data significantly impact model performance - more diverse, representative data generally produces better models."},
        {"question": "What is the separate data used to evaluate model performance?", "answer": "Test data", "alternatives": ["Test set"], "explanation": "Test data evaluates model performance on unseen data after training. It's kept completely separate from training to assess generalization ability. Good test performance indicates the model learned true patterns rather than memorizing training data. Test accuracy estimates real-world performance."},
        {"question": "What problem occurs when a model memorizes training data instead of learning patterns?", "answer": "Overfitting", "alternatives": ["Overtraining"], "explanation": "Overfitting occurs when models learn training data too well, including noise and quirks, rather than underlying patterns. Overfitted models have high training accuracy but poor test performance. Causes include too many parameters, insufficient data, or excessive training. Solutions include regularization, cross-validation, and early stopping."},
        {"question": "What problem occurs when a model is too simple to capture data patterns?", "answer": "Underfitting", "alternatives": ["High bias"], "explanation": "Underfitting happens when models are too simple to capture data complexity, performing poorly on both training and test data. It indicates high bias - the model makes strong simplifying assumptions that don't match reality. Solutions include using more complex models, adding features, or reducing regularization."},
        {"question": "What are the individual measurable properties used as model inputs?", "answer": "Features", "alternatives": ["Input variables", "Attributes"], "explanation": "Features (or attributes) are measurable properties or characteristics used as model inputs. In house price prediction, features include square footage, bedrooms, location. Good feature selection and engineering are crucial for model performance - relevant, informative features enable accurate predictions while irrelevant features add noise."},
        {"question": "What is the value being predicted in supervised learning?", "answer": "Target", "alternatives": ["Label", "Output", "Response variable"], "explanation": "The target (or label/output) is what we're trying to predict in supervised learning. In classification, targets are categories (spam/not spam); in regression, they're continuous values (house price). Clear, accurate target labels are essential - incorrect labels lead to poor model performance regardless of algorithm sophistication."},
        {"question": "What measures how well a model performs on a task?", "answer": "Accuracy", "alternatives": ["Model accuracy"], "explanation": "Accuracy measures the proportion of correct predictions. For classification, it's (correct predictions)/(total predictions). While intuitive, accuracy can be misleading for imbalanced datasets - a model predicting everything as the majority class might have high accuracy but be useless. Other metrics like precision, recall, and F1-score provide additional insight."},
        {"question": "What technique splits data into training and test sets randomly?", "answer": "Train-test split", "alternatives": ["Data splitting"], "explanation": "Train-test split divides data randomly into training and test sets, typically 70-80% training, 20-30% test. This enables model evaluation on unseen data. Random splitting ensures both sets are representative. For small datasets, cross-validation is better. Stratification maintains class proportions in both sets."}
    ],
    "average": [
        {"question": "What validation technique splits data into k folds for robust evaluation?", "answer": "Cross-validation", "alternatives": ["K-fold cross-validation"], "explanation": "Cross-validation divides data into k folds (typically 5 or 10), trains on k-1 folds, and tests on the remaining fold, repeating k times. Each sample is tested exactly once. Results are averaged for reliable performance estimates, especially with limited data. This reduces variance from single train-test splits."},
        {"question": "What technique prevents overfitting by penalizing model complexity?", "answer": "Regularization", "alternatives": ["Model regularization"], "explanation": "Regularization adds penalty terms to the loss function based on model complexity (size of coefficients), preventing overfitting. L1 regularization (Lasso) encourages sparsity; L2 regularization (Ridge) shrinks coefficients smoothly. Regularization strength is controlled by hyperparameter lambda - higher values mean stronger penalization."},
        {"question": "What are the adjustable parameters set before training?", "answer": "Hyperparameters", "alternatives": ["Model hyperparameters"], "explanation": "Hyperparameters are configuration settings set before training that control the learning process, unlike model parameters learned during training. Examples include learning rate, number of trees, regularization strength. Hyperparameter tuning (via grid search or random search) finds optimal values for best performance."},
        {"question": "What metric balances precision and recall into a single score?", "answer": "F1 score", "alternatives": ["F1-score", "F-measure"], "explanation": "F1 score is the harmonic mean of precision and recall: F1 = 2 × (precision × recall)/(precision + recall). It ranges from 0 to 1, with 1 being perfect. F1 balances both metrics - high F1 requires both high precision and high recall. It's especially useful for imbalanced datasets where accuracy is misleading."},
        {"question": "What technique combines multiple models to improve predictions?", "answer": "Ensemble learning", "alternatives": ["Model ensembling"], "explanation": "Ensemble learning combines multiple models (often called weak learners) to create a stronger predictor. Techniques include bagging (parallel models with averaging), boosting (sequential models correcting predecessors), and stacking (meta-model combining base models). Ensembles often outperform individual models by reducing variance and bias."},
        {"question": "What process adjusts model weights based on training errors?", "answer": "Backpropagation", "alternatives": ["Back propagation"], "explanation": "Backpropagation is the algorithm for training neural networks by computing gradients of loss with respect to weights using the chain rule, then updating weights to minimize loss. It propagates errors backward through the network from output to input. Backpropagation with gradient descent enables training deep networks efficiently."},
        {"question": "What function maps model outputs to probabilities between 0 and 1?", "answer": "Sigmoid function", "alternatives": ["Logistic function"], "explanation": "The sigmoid function σ(x) = 1/(1 + e^(-x)) maps any real number to the range (0,1), making it useful for binary classification probability outputs. It has an S-shape, approaching 0 for very negative inputs and 1 for very positive inputs. Sigmoid is used in logistic regression and as an activation function in neural networks."},
        {"question": "What type of machine learning provides feedback as rewards or penalties?", "answer": "Reinforcement learning", "alternatives": ["RL"], "explanation": "Reinforcement learning trains agents to make sequences of decisions by providing rewards for good actions and penalties for bad ones. The agent explores the environment, learning policies that maximize cumulative reward. Applications include game playing (AlphaGo), robotics, and autonomous systems. It differs from supervised learning by not providing correct answers explicitly."},
        {"question": "What problem occurs when test data information leaks into training?", "answer": "Data leakage", "alternatives": ["Leakage"], "explanation": "Data leakage occurs when information from outside the training dataset improperly influences the model, artificially inflating performance estimates. Common causes include using test data in preprocessing, including future information, or features that directly encode the target. Leakage makes models appear better than they are and perform poorly in production."},
        {"question": "What measures the proportion of actual positives correctly identified?", "answer": "Recall", "alternatives": ["Sensitivity", "True positive rate"], "explanation": "Recall (sensitivity) is the proportion of actual positives correctly identified: TP/(TP + FN). High recall means few false negatives. Recall is crucial when missing positives is costly (e.g., disease detection). There's often a trade-off with precision - improving one degrades the other. The optimal balance depends on application requirements."}
    ],
    "difficult": [
        {"question": "What technique gradually reduces learning rate during training for better convergence?", "answer": "Learning rate scheduling", "alternatives": ["Learning rate decay"], "explanation": "Learning rate scheduling adjusts the learning rate during training, typically decreasing it over time. Starting with higher rates enables fast initial progress; lower rates later allow fine-tuning without overshooting. Schedules include step decay, exponential decay, and cosine annealing. Adaptive methods like Adam adjust rates automatically per parameter."},
        {"question": "What advanced regularization randomly drops units during training?", "answer": "Dropout", "alternatives": ["Dropout regularization"], "explanation": "Dropout prevents neural network overfitting by randomly setting a fraction of activations to zero during training. This forces the network to learn robust features that work with different subsets of neurons. At test time, all neurons are used with appropriately scaled weights. Dropout effectively trains an ensemble of networks sharing weights."},
        {"question": "What technique accelerates training by normalizing layer inputs?", "answer": "Batch normalization", "alternatives": ["BatchNorm"], "explanation": "Batch normalization normalizes inputs to each layer using batch statistics (mean and variance), stabilizing and accelerating training. It reduces internal covariate shift (changes in layer input distributions), allows higher learning rates, and provides regularization. BatchNorm is standard in modern deep networks, dramatically improving training dynamics."},
        {"question": "What loss function is used for probabilistic multi-class classification?", "answer": "Cross-entropy loss", "alternatives": ["Log loss", "Categorical cross-entropy"], "explanation": "Cross-entropy loss measures dissimilarity between predicted probability distributions and true distributions. For multi-class classification, categorical cross-entropy sums -log(predicted probability) for true classes. Binary cross-entropy is used for binary classification. Minimizing cross-entropy maximizes the likelihood of correct classes, making it the standard classification loss."},
        {"question": "What technique adjusts decision threshold to balance precision and recall?", "answer": "Threshold tuning", "alternatives": ["Operating point selection"], "explanation": "Threshold tuning adjusts the classification decision threshold (typically 0.5) to optimize for specific metrics. Lowering the threshold increases recall but decreases precision (more positive predictions); raising it does the opposite. ROC and precision-recall curves help select appropriate thresholds based on application needs. This is crucial for imbalanced or cost-sensitive problems."},
        {"question": "What evaluation metric is robust to class imbalance?", "answer": "Matthews Correlation Coefficient", "alternatives": ["MCC"], "explanation": "Matthews Correlation Coefficient (MCC) ranges from -1 to +1, measuring classification quality even with imbalanced classes. MCC = (TP×TN - FP×FN)/√[(TP+FP)(TP+FN)(TN+FP)(TN+FN)]. Unlike accuracy and F1, MCC accounts for all confusion matrix elements equally, providing reliable scores regardless of class distribution. MCC = 0 indicates random prediction."},
        {"question": "What technique initializes weights to prevent gradient vanishing or explosion?", "answer": "Xavier initialization", "alternatives": ["Glorot initialization"], "explanation": "Xavier (Glorot) initialization sets initial weights with variance 1/n_in for tanh/sigmoid activations, preventing gradients from vanishing or exploding during backpropagation. He initialization (variance 2/n_in) is used for ReLU. Proper initialization enables training deep networks - poor initialization can make networks untrainable regardless of architecture or data."},
        {"question": "What technique transfers knowledge from pre-trained models to new tasks?", "answer": "Transfer learning", "alternatives": ["Model transfer"], "explanation": "Transfer learning leverages models trained on large datasets (like ImageNet) for new tasks with limited data. Lower layers learn general features (edges, textures) while higher layers learn task-specific features. Fine-tuning adjusts pre-trained weights for the new task. Transfer learning dramatically reduces training time and data requirements while improving performance."},
        {"question": "What advanced technique handles class imbalance by adjusting loss weights?", "answer": "Class weighting", "alternatives": ["Weighted loss"], "explanation": "Class weighting assigns different importance to classes in the loss function, typically giving minority classes higher weight. This forces the model to pay more attention to underrepresented classes. Weights are often set inversely proportional to class frequencies. Combined with techniques like SMOTE and threshold tuning, class weighting effectively handles severe imbalance."},
        {"question": "What method optimizes neural networks using adaptive per-parameter learning rates?", "answer": "Adam optimizer", "alternatives": ["Adam"], "explanation": "Adam (Adaptive Moment Estimation) combines momentum (exponentially decaying average of gradients) with RMSprop (adapting learning rates per parameter). It maintains both first moment (mean) and second moment (variance) estimates of gradients, adjusting learning rates adaptively. Adam is robust, requires little tuning, and is now the default optimizer for many applications, often outperforming SGD."}
    ]
}

ALL_ADDITIONAL_QUESTIONS['ml_basics'] = ML_BASICS_QUESTIONS

# 16. Supervised Learning  
SUPERVISED_LEARNING_QUESTIONS = {
    "easy": [
        {"question": "What algorithm predicts continuous numerical outputs?", "answer": "Regression", "alternatives": ["Regression analysis"], "explanation": "Regression predicts continuous numeric outcomes from input features. Linear regression models relationships as straight lines; polynomial regression captures curves. Applications include house price prediction, sales forecasting, and risk assessment. Regression differs from classification which predicts categories."},
        {"question": "What algorithm predicts categorical class labels?", "answer": "Classification", "alternatives": ["Classifier"], "explanation": "Classification assigns inputs to discrete categories or classes. Binary classification has two classes (spam/not spam); multi-class has more (digit recognition). Algorithms include logistic regression, decision trees, neural networks, and SVM. Output is typically a class label or probability distribution over classes."},
        {"question": "What simple algorithm classifies based on majority vote of k nearest neighbors?", "answer": "K-Nearest Neighbors", "alternatives": ["k-NN", "KNN"], "explanation": "K-Nearest Neighbors (k-NN) classifies by finding k closest training examples and using majority vote. It's non-parametric (no training phase) and intuitive but slow for large datasets. Choice of k and distance metric are crucial - small k is sensitive to noise, large k smooths boundaries. Feature scaling is essential."},
        {"question": "What tree-based model splits data using feature thresholds?", "answer": "Decision Tree", "alternatives": ["Decision tree classifier"], "explanation": "Decision trees make predictions by learning decision rules from features, creating a tree structure of if-then-else decisions. They're interpretable, handle non-linear relationships, and require minimal preprocessing. However, they easily overfit without pruning or ensemble methods like Random Forest. They work for both classification and regression."},
        {"question": "What linear model uses logistic function for binary classification?", "answer": "Logistic Regression", "alternatives": ["Log regression"], "explanation": "Despite its name, logistic regression is a classification algorithm using the logistic/sigmoid function to model probability of binary outcomes. It's simple, interpretable, and provides probability estimates. Coefficients indicate feature importance and direction. Regularization (L1/L2) prevents overfitting. Multi-class variants include one-vs-rest and softmax regression."},
        {"question": "What algorithm finds optimal separating hyperplane between classes?", "answer": "Support Vector Machine", "alternatives": ["SVM"], "explanation": "Support Vector Machines (SVM) find the hyperplane that maximally separates classes. Only points near the boundary (support vectors) affect the decision boundary. Kernel trick enables non-linear classification by mapping to higher dimensions. SVMs work well for high-dimensional data but are sensitive to scaling and kernel choice."},
        {"question": "What probabilistic classifier assumes feature independence?", "answer": "Naive Bayes", "alternatives": ["Naive Bayes classifier"], "explanation": "Naive Bayes classifiers apply Bayes' theorem assuming features are conditionally independent given the class (the 'naive' assumption). Despite this unrealistic assumption, they work surprisingly well for text classification and spam filtering. They're fast, require little training data, and handle high dimensions well. Variants include Gaussian, Multinomial, and Bernoulli."},
        {"question": "What ensemble method combines multiple decision trees?", "answer": "Random Forest", "alternatives": ["Random forest classifier"], "explanation": "Random Forest creates many decision trees using random subsets of data and features, then averages their predictions. This reduces overfitting while maintaining trees' interpretability and handling non-linearity. Random Forests often work well out-of-the-box, provide feature importance estimates, and are robust to noise and outliers."},
        {"question": "What metric shows trade-off between true and false positive rates?", "answer": "ROC curve", "alternatives": ["Receiver Operating Characteristic"], "explanation": "ROC (Receiver Operating Characteristic) curves plot true positive rate (recall) versus false positive rate at various threshold settings. The area under ROC curve (AUC-ROC) summarizes performance - 0.5 is random, 1.0 is perfect. ROC curves are useful for comparing models and selecting operating points, especially with imbalanced data."},
        {"question": "What line equation describes the relationship between input and output?", "answer": "Linear regression", "alternatives": ["Linear model"], "explanation": "Linear regression models the relationship between features and target as a linear equation: y = b0 + b1x1 + b2x2 + ... Coefficients indicate feature effects; intercept is the baseline. Ordinary Least Squares finds coefficients minimizing squared prediction errors. Assumptions include linearity, independence, homoscedasticity, and normal residuals."}
    ],
    "average": [
        {"question": "What boosting algorithm builds trees sequentially to correct previous errors?", "answer": "Gradient Boosting", "alternatives": ["Gradient boosting machine", "GBM"], "explanation": "Gradient Boosting builds an ensemble of trees sequentially, each correcting errors of predecessors by fitting residuals. It uses gradient descent to minimize loss. Implementations include XGBoost, LightGBM, and CatBoost with various optimizations. Gradient boosting often achieves excellent performance but requires careful tuning and is prone to overfitting."},
        {"question": "What technique prevents overfitting by stopping tree growth?", "answer": "Tree pruning", "alternatives": ["Pruning"], "explanation": "Tree pruning removes branches from fully-grown decision trees to prevent overfitting. Pre-pruning stops growth early based on criteria like maximum depth or minimum samples per leaf. Post-pruning grows full tree then removes branches not improving validation performance. Pruning trades training accuracy for better generalization."},
        {"question": "What regression variant adds L1 penalty for feature selection?", "answer": "Lasso regression", "alternatives": ["L1 regularization", "Lasso"], "explanation": "Lasso (Least Absolute Shrinkage and Selection Operator) adds L1 penalty (sum of absolute coefficients) to linear regression, encouraging sparse solutions by shrinking some coefficients to exactly zero. This performs feature selection automatically. The regularization parameter controls sparsity - higher values mean more coefficients become zero."},
        {"question": "What regression variant adds L2 penalty to shrink coefficients?", "answer": "Ridge regression", "alternatives": ["L2 regularization", "Ridge"], "explanation": "Ridge regression adds L2 penalty (sum of squared coefficients) to linear regression, shrinking coefficients toward zero but not to exactly zero. This reduces variance and prevents overfitting, especially with correlated features. Unlike Lasso, Ridge doesn't perform feature selection. The regularization parameter controls shrinkage strength."},
        {"question": "What technique assigns higher importance to minority class samples?", "answer": "Cost-sensitive learning", "alternatives": ["Class weighting"], "explanation": "Cost-sensitive learning handles imbalanced data by assigning different misclassification costs to classes. Minority class errors cost more, forcing the model to prioritize them. This is implemented through class weights in loss functions or sampling methods. It's especially useful when false negatives are much more costly than false positives."},
        {"question": "What boosting algorithm trains weak learners on weighted training sets?", "answer": "AdaBoost", "alternatives": ["Adaptive Boosting"], "explanation": "AdaBoost (Adaptive Boosting) trains weak learners sequentially, increasing weights for misclassified examples so subsequent learners focus on difficult cases. Final prediction combines weighted votes from all learners. AdaBoost is sensitive to noisy data and outliers but often achieves high accuracy. It's typically used with decision stumps (one-level trees)."},
        {"question": "What algorithm combines predictions using learned meta-model?", "answer": "Stacking", "alternatives": ["Stacked generalization"], "explanation": "Stacking trains a meta-model to combine base model predictions. Base models' outputs become features for the meta-model, which learns optimal combination. This captures different models' strengths and often improves performance beyond single models or simple averaging. Common meta-models include linear regression or logistic regression."},
        {"question": "What measure quantifies residual error in linear regression?", "answer": "Residual standard error", "alternatives": ["RSE", "Standard error of regression"], "explanation": "Residual Standard Error (RSE) estimates the standard deviation of prediction errors in linear regression. It measures typical distance between observed and predicted values. Lower RSE indicates better fit. RSE = [S(y - y)/(n-p-1)] where n is samples and p is predictors. RSE is in the same units as the response variable."},
        {"question": "What technique creates training datasets by sampling with replacement?", "answer": "Bagging", "alternatives": ["Bootstrap aggregating"], "explanation": "Bagging (Bootstrap Aggregating) trains multiple models on different bootstrap samples (random sampling with replacement) and averages predictions. This reduces variance and prevents overfitting. Random Forest is bagging applied to decision trees with additional feature randomness. Bagging works best with unstable models like deep decision trees."},
        {"question": "What method selects best subset of features for a model?", "answer": "Feature selection", "alternatives": ["Variable selection"], "explanation": "Feature selection identifies relevant features while removing irrelevant ones. Methods include filter (correlation, mutual information), wrapper (forward/backward selection using model performance), and embedded (Lasso, tree importance). Benefits include reduced overfitting, faster training, improved interpretability, and addressing curse of dimensionality."}
    ],
    "difficult": [
        {"question": "What advanced boosting library uses histogram-based learning?", "answer": "LightGBM", "alternatives": ["Light Gradient Boosting Machine"], "explanation": "LightGBM is a gradient boosting framework using histogram-based algorithms that bin continuous features into discrete bins, dramatically speeding up training while maintaining accuracy. It uses leaf-wise tree growth (faster but can overfit) versus level-wise. LightGBM handles large datasets efficiently and includes native categorical feature support."},
        {"question": "What technique creates diverse ensemble members by feature randomness?", "answer": "Random subspace method", "alternatives": ["Feature bagging"], "explanation": "Random subspace method (feature bagging) creates ensemble diversity by training models on random subsets of features. Each model sees different feature combinations, preventing correlation between ensemble members. This is particularly effective for high-dimensional data. Random Forest combines random subspaces with bagging for powerful ensemble performance."},
        {"question": "What regression addresses heteroscedasticity using weighted least squares?", "answer": "Weighted least squares", "alternatives": ["WLS regression"], "explanation": "Weighted Least Squares (WLS) addresses heteroscedasticity (non-constant variance) in regression by assigning weights inversely proportional to variance at each point. Observations with higher variance get lower weight. WLS produces more efficient estimates than OLS when heteroscedasticity is present. Weights must be known or estimated from data."},
        {"question": "What method handles multiple outputs simultaneously in regression?", "answer": "Multi-output regression", "alternatives": ["Multi-task regression"], "explanation": "Multi-output (or multi-task) regression predicts multiple target variables simultaneously from shared features. This can improve performance versus separate models by learning relationships between outputs. Methods include multi-output linear regression, multi-output random forests, and neural networks with multiple output neurons. Useful when outputs are correlated."},
        {"question": "What advanced SVM handles non-separable data with soft margins?", "answer": "C-SVM", "alternatives": ["Soft margin SVM"], "explanation": "C-SVM (soft margin SVM) allows some misclassifications by introducing slack variables and regularization parameter C. Large C means hard margin (fewer violations), small C means soft margin (more tolerance). This handles noisy, non-separable data better than hard margin SVM. C balances margin width against training errors."},
        {"question": "What ensemble technique uses out-of-bag samples for validation?", "answer": "Out-of-bag estimation", "alternatives": ["OOB error"], "explanation": "Out-of-bag (OOB) estimation uses bootstrap samples not selected during bagging for validation. For each sample, prediction is made using only trees that didn't see it during training. This provides unbiased error estimates without separate validation sets. OOB error is nearly equivalent to cross-validation but computationally free in Random Forests."},
        {"question": "What technique learns from both labeled and unlabeled data?", "answer": "Semi-supervised learning", "alternatives": ["Semi-supervised training"], "explanation": "Semi-supervised learning leverages both labeled and unlabeled data when labels are expensive but unlabeled data is abundant. Unlabeled data helps learn data structure and improve decision boundaries. Techniques include self-training, co-training, and multi-view learning. Assumes unlabeled data shares structure with labeled data and that clusters correspond to classes."},
        {"question": "What advanced boosting handles categorical features natively?", "answer": "CatBoost", "alternatives": ["Categorical Boosting"], "explanation": "CatBoost is a gradient boosting library with specialized categorical feature handling using ordered target statistics instead of one-hot encoding. It addresses target leakage through ordered boosting and uses symmetric trees. CatBoost often works well with minimal tuning, handles missing values automatically, and provides fast prediction. It's particularly effective for datasets with many categorical features."},
        {"question": "What technique balances bias-variance trade-off in ensemble size?", "answer": "Early stopping", "alternatives": ["Ensemble size optimization"], "explanation": "Early stopping in ensemble learning monitors validation performance and stops adding models when performance plateaus or degrades. In boosting, this prevents overfitting as later iterations fit noise. The optimal ensemble size balances bias (too few models) and variance (too many models). Monitoring validation error or using out-of-bag error guides stopping decisions."},
        {"question": "What method calibrates probabilities for better uncertainty estimates?", "answer": "Probability calibration", "alternatives": ["Calibration", "Platt scaling"], "explanation": "Probability calibration adjusts classifier outputs to match true empirical probabilities. Platt scaling fits a logistic regression to map scores to calibrated probabilities; isotonic regression uses monotonic mapping. Well-calibrated models have predicted probabilities matching observed frequencies. Calibration is crucial for applications where probability magnitudes matter, not just rank ordering."}
    ]
}

ALL_ADDITIONAL_QUESTIONS['supervised_learning'] = SUPERVISED_LEARNING_QUESTIONS

# 17. Unsupervised Learning
UNSUPERVISED_LEARNING_QUESTIONS = {
    "easy": [
        {"question": "What technique groups similar data points together?", "answer": "Clustering", "alternatives": ["Cluster analysis"], "explanation": "Clustering partitions data into groups (clusters) where points within clusters are similar and points in different clusters are dissimilar. It's unsupervised (no labels) and discovers hidden structure. Applications include customer segmentation, document organization, image compression, and anomaly detection. Common algorithms include k-means, hierarchical, and DBSCAN."},
        {"question": "What popular algorithm partitions data into k clusters?", "answer": "K-means", "alternatives": ["K-means clustering"], "explanation": "K-means clusters data by iteratively assigning points to nearest centroid and updating centroids as cluster means. It's simple, scalable, and widely used. Requires specifying k beforehand; sensitive to initialization and outliers. Works best with spherical clusters of similar size. The elbow method helps choose k."},
        {"question": "What technique reduces dimensionality while preserving variance?", "answer": "PCA", "alternatives": ["Principal Component Analysis"], "explanation": "Principal Component Analysis (PCA) transforms data into orthogonal components ordered by explained variance. First components capture most variation; later ones capture noise. PCA reduces dimensions, removes correlations, and aids visualization. It's linear and assumes data lies on a low-dimensional linear subspace. Standardization is often needed."},
        {"question": "What method builds tree of nested clusters?", "answer": "Hierarchical clustering", "alternatives": ["Hierarchical cluster analysis"], "explanation": "Hierarchical clustering creates tree structure (dendrogram) showing nested cluster relationships. Agglomerative (bottom-up) starts with points as clusters and merges; divisive (top-down) starts with one cluster and splits. Doesn't require pre-specifying cluster count. Linkage criteria (single, complete, average) affect results. Computationally expensive for large datasets."},
        {"question": "What density-based algorithm finds clusters of arbitrary shape?", "answer": "DBSCAN", "alternatives": ["Density-Based Spatial Clustering"], "explanation": "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) groups points in dense regions separated by low-density areas. It discovers clusters of arbitrary shape and identifies outliers as noise. Parameters are epsilon (neighborhood radius) and minPts (minimum points for core). Unlike k-means, it doesn't require specifying cluster count."},
        {"question": "What metric measures cluster quality using cohesion and separation?", "answer": "Silhouette score", "alternatives": ["Silhouette coefficient"], "explanation": "Silhouette score evaluates clustering quality by comparing intra-cluster distance (cohesion) to nearest-cluster distance (separation) for each point. Values range from -1 to 1; higher is better. Score near 1 means well-clustered, near 0 means on border, negative means possibly wrong cluster. Average silhouette score summarizes overall quality."},
        {"question": "What technique discovers frequent itemsets in transactional data?", "answer": "Association rules", "alternatives": ["Market basket analysis"], "explanation": "Association rule mining discovers relationships between items in transactions, like 'customers who buy X often buy Y'. Rules have support (frequency), confidence (conditional probability), and lift (correlation). Apriori algorithm efficiently finds frequent itemsets. Applications include recommendation systems, cross-selling, and catalog design."},
        {"question": "What method determines optimal number of clusters?", "answer": "Elbow method", "alternatives": ["Elbow curve"], "explanation": "Elbow method plots within-cluster sum of squares (WCSS) versus number of clusters. WCSS decreases as k increases; the 'elbow' point where rate of decrease sharply changes suggests optimal k. It balances fit and complexity. However, elbows aren't always clear. Alternative methods include silhouette analysis and gap statistic."},
        {"question": "What technique identifies unusual patterns deviating from normal?", "answer": "Anomaly detection", "alternatives": ["Outlier detection"], "explanation": "Anomaly detection identifies rare items, events, or observations differing significantly from normal patterns. Applications include fraud detection, system health monitoring, and quality control. Methods include statistical (z-score), proximity-based (k-NN distance), and isolation-based (Isolation Forest). Anomalies can be point, contextual, or collective."},
        {"question": "What metric shows percentage of variance explained by principal components?", "answer": "Explained variance", "alternatives": ["Variance explained ratio"], "explanation": "Explained variance ratio indicates the proportion of total variance captured by each principal component in PCA. First PC explains most variance, subsequent PCs explain progressively less. Cumulative explained variance helps choose how many components to retain. Typically retain enough components to explain 80-95% of variance."}
    ],
    "average": [
        {"question": "What initialization method improves k-means clustering?", "answer": "K-means++", "alternatives": ["K-means plus plus"], "explanation": "K-means++ is an initialization algorithm that spreads initial centroids far apart, improving k-means convergence and final quality. It selects first centroid randomly, then chooses subsequent centroids probabilistically proportional to squared distance from nearest existing centroid. This avoids poor local optima common with random initialization. It's now standard in most implementations."},
        {"question": "What metric compares clustering results accounting for chance?", "answer": "Adjusted Rand Index", "alternatives": ["ARI"], "explanation": "Adjusted Rand Index (ARI) measures similarity between two clusterings, correcting for chance agreement. Values range from -1 to 1; 1 means identical, 0 means random, negative means worse than random. Unlike accuracy, ARI doesn't require cluster label matching. It's useful for evaluating clustering algorithms when ground truth labels are available for comparison."},
        {"question": "What technique detects outliers based on local density?", "answer": "Local Outlier Factor", "alternatives": ["LOF"], "explanation": "Local Outlier Factor (LOF) detects outliers by comparing local density of a point to its neighbors. Points in sparse regions have high LOF scores. Unlike global methods, LOF handles varying densities. It's effective for local anomalies that wouldn't be detected globally. LOF values around 1 indicate inliers, much greater than 1 indicate outliers."},
        {"question": "What dimensionality reduction preserves pairwise distances?", "answer": "MDS", "alternatives": ["Multidimensional Scaling"], "explanation": "Multidimensional Scaling (MDS) creates low-dimensional representation preserving pairwise distances from high-dimensional space. Classical MDS uses eigendecomposition; metric MDS optimizes stress (distance preservation error). Unlike PCA which preserves variance, MDS preserves distances. Useful for visualization and when relationships matter more than features themselves."},
        {"question": "What technique learns non-linear low-dimensional structure?", "answer": "Manifold learning", "alternatives": ["Non-linear dimensionality reduction"], "explanation": "Manifold learning techniques (Isomap, LLE, t-SNE) learn non-linear low-dimensional embeddings assuming data lies on curved manifold in high dimensions. Unlike linear PCA, they capture complex non-linear structure. Each method preserves different properties: Isomap (geodesic distances), LLE (local structure), t-SNE (local neighborhoods). Useful for visualization and preprocessing."},
        {"question": "What hierarchical clustering maximizes between-cluster distance?", "answer": "Complete linkage", "alternatives": ["Maximum linkage"], "explanation": "Complete linkage (maximum linkage) merges clusters based on maximum distance between any two points across clusters. It produces compact, spherical clusters and is less susceptible to noise than single linkage. However, it's sensitive to outliers and can break large clusters. Distance between clusters is defined by their two most distant points."},
        {"question": "What probabilistic clustering uses mixture of Gaussians?", "answer": "GMM", "alternatives": ["Gaussian Mixture Model"], "explanation": "Gaussian Mixture Model (GMM) assumes data comes from mixture of k Gaussian distributions with unknown parameters. Unlike hard k-means assignment, GMM provides soft probabilistic membership for each point. Expectation-Maximization algorithm learns parameters. GMM handles elliptical clusters and provides uncertainty estimates. It requires specifying number of components."},
        {"question": "What dimensionality reduction extracts latent topics?", "answer": "LDA", "alternatives": ["Latent Dirichlet Allocation"], "explanation": "Latent Dirichlet Allocation (LDA) is a generative probabilistic model discovering latent topics in document collections. Documents are mixtures of topics; topics are distributions over words. Unlike linear methods, LDA provides interpretable topics. It's widely used for text mining, document classification, and recommendation. Perplexity measures held-out likelihood for model selection."},
        {"question": "What method identifies hidden variables affecting observed data?", "answer": "Factor analysis", "alternatives": ["FA"], "explanation": "Factor analysis discovers underlying latent factors explaining correlations among observed variables. Unlike PCA which maximizes variance, FA models observed variables as linear combinations of factors plus noise. It provides interpretable factors in psychology, marketing, and social sciences. Rotation methods (varimax, promax) improve factor interpretability."},
        {"question": "What metric evaluates clustering with lower values better?", "answer": "Davies-Bouldin Index", "alternatives": ["DB Index"], "explanation": "Davies-Bouldin Index measures average similarity between each cluster and its most similar cluster, considering within-cluster scatter and between-cluster separation. Lower values indicate better clustering. Unlike silhouette score, it uses only cluster properties (no sample-level computation). DB Index is undefined for single cluster and sensitive to number of clusters."}
    ],
    "difficult": [
        {"question": "What density-based algorithm generalizes DBSCAN for varying densities?", "answer": "OPTICS", "alternatives": ["Ordering Points To Identify Clustering Structure"], "explanation": "OPTICS (Ordering Points To Identify Clustering Structure) extends DBSCAN to handle clusters of varying density by producing reachability plot instead of flat clustering. It orders points by density-reachability and computes reachability distances. The resulting plot reveals cluster structure at all density levels. Extract clusterings by cutting the plot at different heights. More robust than DBSCAN but harder to interpret."},
        {"question": "What clustering uses eigenvalues of similarity matrix?", "answer": "Spectral clustering", "alternatives": ["Graph-based clustering"], "explanation": "Spectral clustering treats data as graph and uses eigenvectors of graph Laplacian matrix for clustering. It performs k-means on eigenvector space, enabling non-convex cluster discovery. Effective for image segmentation and graph partitioning. Similarity matrix construction is crucial; common choices include k-NN graph and Gaussian kernel. Computationally expensive for large datasets."},
        {"question": "What ensemble method detects anomalies by isolation?", "answer": "Isolation Forest", "alternatives": ["iForest"], "explanation": "Isolation Forest detects anomalies by randomly partitioning data with trees; anomalies are isolated with fewer partitions (shorter paths). Unlike density or distance methods, it explicitly isolates outliers rather than profiling normal. It's efficient, handles high dimensions, and requires few parameters. Anomaly score is based on path length; shorter paths indicate anomalies. Works well even with contaminated training data."},
        {"question": "What neural network learns compressed representation unsupervised?", "answer": "Autoencoder", "alternatives": ["Autoassociative neural network"], "explanation": "Autoencoders are neural networks trained to reconstruct input through bottleneck hidden layer, learning compressed representation. Encoder compresses to latent space; decoder reconstructs. Variants include denoising (robust features), variational (probabilistic), and sparse (sparse codes). Applications include dimensionality reduction, denoising, anomaly detection, and generative modeling. Non-linear and learns hierarchical features."},
        {"question": "What technique preserves global and local structure in low dimensions?", "answer": "UMAP", "alternatives": ["Uniform Manifold Approximation and Projection"], "explanation": "UMAP (Uniform Manifold Approximation and Projection) is a manifold learning technique for dimensionality reduction balancing global and local structure preservation. Based on Riemannian geometry and algebraic topology, it's faster than t-SNE and preserves more global structure. Excellent for visualization and preprocessing. Parameters include n_neighbors (local/global balance) and min_dist (embedding tightness)."},
        {"question": "What clustering technique finds subsets of features and samples?", "answer": "Biclustering", "alternatives": ["Co-clustering"], "explanation": "Biclustering simultaneously clusters rows and columns of data matrix, finding subgroups of samples exhibiting similar behavior across subsets of features. Unlike traditional clustering operating on all features, biclustering discovers local patterns. Applications include gene expression analysis (finding genes and conditions), text mining, and collaborative filtering. Spectral and Cheng-Church are common algorithms."},
        {"question": "What method separates mixed signals into independent sources?", "answer": "ICA", "alternatives": ["Independent Component Analysis"], "explanation": "Independent Component Analysis (ICA) separates multivariate signal into additive, independent components. Unlike PCA seeking uncorrelated components, ICA seeks statistically independent ones. Assumes sources are non-Gaussian and statistically independent. Applications include blind source separation (cocktail party problem), fMRI analysis, and artifact removal from EEG. FastICA is popular algorithm."},
        {"question": "What hierarchical clustering minimizes distance increase at each merge?", "answer": "Single linkage", "alternatives": ["Minimum linkage"], "explanation": "Single linkage (minimum linkage) merges clusters based on minimum distance between any two points across clusters. It can discover clusters of arbitrary shape by chaining but suffers from chaining effect where clusters merge through noise points. Produces elongated clusters; sensitive to outliers. Distance between clusters is defined by their two closest points. Useful when clusters are clearly separated."},
        {"question": "What technique detects changes in statistical properties over time?", "answer": "Change point detection", "alternatives": ["Changepoint analysis"], "explanation": "Change point detection identifies times when statistical properties of time series change, such as mean, variance, or distribution shifts. Methods include CUSUM (cumulative sum), Bayesian approaches, and likelihood ratios. Applications include quality control, finance (regime changes), and system monitoring. Can detect abrupt or gradual changes. Multiple change points increase complexity."},
        {"question": "What approach learns representations from unlabeled data with neural networks?", "answer": "Deep unsupervised learning", "alternatives": ["Self-supervised learning"], "explanation": "Deep unsupervised learning uses deep neural networks to learn representations from unlabeled data. Techniques include autoencoders (reconstruction), GANs (adversarial), contrastive learning (SimCLR, MoCo), and masked modeling (BERT, MAE). Self-supervised pretraining on large unlabeled data followed by fine-tuning on small labeled data achieves state-of-the-art. Captures hierarchical features and semantic structure."}
    ]
}

ALL_ADDITIONAL_QUESTIONS['unsupervised_learning'] = UNSUPERVISED_LEARNING_QUESTIONS

# 18. Python Data Tools
PYTHON_DATA_TOOLS_QUESTIONS = {
    "easy": [
        {"question": "What NumPy object stores multi-dimensional data?", "answer": "ndarray", "alternatives": ["NumPy array"], "explanation": "NumPy's ndarray (n-dimensional array) is a fast, flexible container for large datasets with homogeneous data types. It enables vectorized operations (element-wise without loops), broadcasting, and efficient memory usage. Arrays are created via np.array(), np.zeros(), np.ones(), etc. Shape attribute shows dimensions; dtype shows data type."},
        {"question": "What Pandas structure stores labeled 2D data?", "answer": "DataFrame", "alternatives": ["Data Frame"], "explanation": "Pandas DataFrame is 2D labeled data structure with columns of potentially different types, similar to spreadsheet or SQL table. Columns are Series objects. Created via pd.DataFrame(dict), pd.read_csv(), etc. Supports indexing, slicing, grouping, merging, and time series operations. Has both row (index) and column labels."},
        {"question": "What Pandas object represents single column of data?", "answer": "Series", "alternatives": ["Pandas Series"], "explanation": "Pandas Series is 1D labeled array holding any data type (integers, strings, objects, etc.). It has index (labels) and values. Created via pd.Series(list) or by extracting DataFrame column. Supports vectorized operations, alignment by index, and methods like mean(), sum(), unique(). Series is building block of DataFrame."},
        {"question": "What Matplotlib function creates basic line plot?", "answer": "plt.plot()", "alternatives": ["plot()"], "explanation": "plt.plot() creates line plots from x and y coordinates. Basic usage: plt.plot(x, y). Customizable with parameters: color, linestyle, marker, linewidth, label. Multiple plots on same axes by calling plot() multiple times. Follow with plt.show() to display. Most fundamental Matplotlib function for visualization."},
        {"question": "What Seaborn function creates statistical scatter plot with regression?", "answer": "sns.regplot()", "alternatives": ["regplot()", "sns.lmplot()"], "explanation": "Seaborn's regplot() creates scatter plot with fitted regression line and confidence interval. Usage: sns.regplot(x='col1', y='col2', data=df). Shows relationship strength and trend. Parameters include order (polynomial degree), ci (confidence interval), scatter_kws, line_kws. lmplot() is similar but creates faceted plots."},
        {"question": "What library provides machine learning algorithms in Python?", "answer": "scikit-learn", "alternatives": ["sklearn"], "explanation": "scikit-learn (sklearn) is the primary machine learning library in Python. Provides consistent API for classification, regression, clustering, dimensionality reduction, and preprocessing. Built on NumPy, SciPy, and Matplotlib. Models follow fit/predict pattern. Includes model selection tools (cross-validation, grid search) and datasets."},
        {"question": "What Pandas method reads CSV files into DataFrame?", "answer": "pd.read_csv()", "alternatives": ["read_csv()"], "explanation": "pd.read_csv() reads comma-separated values file into Pandas DataFrame. Basic usage: df = pd.read_csv('file.csv'). Parameters include sep (delimiter), header (row for column names), index_col (column as index), parse_dates (datetime conversion), na_values (missing value indicators). Can read from URLs and compressed files."},
        {"question": "What Jupyter interface combines code, text, and visualizations?", "answer": "Jupyter Notebook", "alternatives": ["IPython Notebook"], "explanation": "Jupyter Notebook is interactive web-based environment for creating documents with live code, equations, visualizations, and narrative text. Supports Python and other kernels. Cells can be code (executable) or markdown (formatted text). Enables iterative development, documentation, and sharing. Files have .ipynb extension. JupyterLab is next-generation interface."},
        {"question": "What NumPy function creates array of evenly spaced values?", "answer": "np.linspace()", "alternatives": ["linspace()"], "explanation": "np.linspace(start, stop, num) creates array of num evenly spaced values from start to stop (inclusive). Useful for creating x-values for plotting. Example: np.linspace(0, 10, 50) creates 50 points between 0 and 10. Unlike np.arange() which uses step size, linspace uses count. Parameter endpoint controls whether stop is included."},
        {"question": "What Pandas method shows first few rows of DataFrame?", "answer": "df.head()", "alternatives": ["head()"], "explanation": "df.head(n) returns first n rows of DataFrame (default 5). Useful for quick inspection of data structure and sample values. Complemented by df.tail() for last rows. Common first step in data exploration. Example: df.head(10) shows first 10 rows. Also works on Series."}
    ],
    "average": [
        {"question": "What Pandas method applies function to groups?", "answer": "groupby()", "alternatives": ["df.groupby()"], "explanation": "groupby() splits data into groups based on criteria, applies function to each group, and combines results (split-apply-combine). Usage: df.groupby('column').agg_func(). Returns GroupBy object for further operations. Supports multiple grouping columns, custom aggregation functions, and transformations. Essential for group statistics and aggregations."},
        {"question": "What NumPy technique applies operations to arrays of different shapes?", "answer": "Broadcasting", "alternatives": ["Array broadcasting"], "explanation": "Broadcasting allows NumPy to perform operations on arrays of different shapes by automatically expanding smaller array. Rules: dimensions are compatible if equal or one is 1. Smaller array is stretched to match. Avoids explicit loops and memory copies. Example: array([1,2,3]) + 5 broadcasts 5 to [5,5,5]. Fundamental for vectorized operations."},
        {"question": "What Pandas method combines DataFrames horizontally or vertically?", "answer": "pd.concat()", "alternatives": ["concat()"], "explanation": "pd.concat() concatenates DataFrames along axis (0=rows, 1=columns). Usage: pd.concat([df1, df2], axis=0). Parameters include ignore_index (reset index), keys (hierarchical index), join (how to handle different columns). Unlike merge which combines on keys, concat stacks DataFrames. Can combine Series too."},
        {"question": "What method joins DataFrames on common columns like SQL?", "answer": "df.merge()", "alternatives": ["merge()", "pd.merge()"], "explanation": "merge() combines DataFrames based on common columns (SQL-like join). Usage: df1.merge(df2, on='key'). Join types: inner (intersection), outer (union), left, right. Parameters include left_on, right_on (different column names), how (join type), suffixes (column name conflicts). More flexible than concat for relational operations."},
        {"question": "What Matplotlib function creates subplots in grid?", "answer": "plt.subplots()", "alternatives": ["subplots()"], "explanation": "plt.subplots(nrows, ncols) creates figure with grid of subplots. Returns figure and axes array. Usage: fig, axes = plt.subplots(2, 3) creates 2x3 grid. Access individual subplot: axes[i, j].plot(). Parameters include figsize (figure size), sharex/sharey (shared axes). Preferred over plt.subplot() for multiple plots."},
        {"question": "What scikit-learn function splits data into training and test sets?", "answer": "train_test_split()", "alternatives": ["sklearn.model_selection.train_test_split"], "explanation": "train_test_split() from sklearn.model_selection randomly splits arrays into train and test subsets. Usage: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2). Parameters include test_size (fraction for test), random_state (reproducibility), stratify (preserve class proportions). Essential for model validation."},
        {"question": "What Pandas method reshapes data from wide to long format?", "answer": "pd.melt()", "alternatives": ["melt()"], "explanation": "pd.melt() unpivots DataFrame from wide to long format. Transforms columns into rows. Parameters: id_vars (identifier columns to keep), value_vars (columns to unpivot), var_name (name for variable column), value_name (name for value column). Useful for creating tidy data. Opposite of pivot()."},
        {"question": "What NumPy function performs matrix multiplication?", "answer": "np.dot()", "alternatives": ["dot()", "np.matmul()", "@"], "explanation": "np.dot() computes dot product of two arrays. For 2D arrays, it's matrix multiplication. Usage: np.dot(A, B) or A @ B (Python 3.5+). For 1D, it's inner product. np.matmul() is similar but handles batches differently. Matrix multiplication is not element-wise (that's *). Fundamental for linear algebra operations."},
        {"question": "What Seaborn function creates correlation heatmap?", "answer": "sns.heatmap()", "alternatives": ["heatmap()"], "explanation": "sns.heatmap() visualizes matrix data as colored grid. Common usage: sns.heatmap(df.corr(), annot=True) for correlation matrix. Parameters include annot (show values), cmap (color scheme), fmt (value format), linewidths (cell spacing). Useful for finding feature correlations, confusion matrices, and comparing categorical variables."},
        {"question": "What Pandas method handles missing data by removing?", "answer": "df.dropna()", "alternatives": ["dropna()"], "explanation": "dropna() removes missing values (NaN) from DataFrame/Series. Parameters: axis (0=rows, 1=columns), how ('any' drops if any NaN, 'all' if all NaN), subset (consider only specified columns), inplace (modify original). Returns copy by default. Complemented by fillna() for imputation. Key preprocessing step."}
    ],
    "difficult": [
        {"question": "What Pandas method applies function efficiently with vectorization?", "answer": "df.apply()", "alternatives": ["apply()"], "explanation": "apply() applies function along DataFrame axis. Usage: df.apply(func, axis=0). While convenient, it's often slower than vectorized operations. Use when vectorization impossible. For element-wise operations, applymap() (DataFrames) or map() (Series) are alternatives. Prefer NumPy vectorized operations or built-in Pandas methods when possible for performance."},
        {"question": "What technique creates MultiIndex for hierarchical data?", "answer": "pd.MultiIndex", "alternatives": ["Hierarchical indexing"], "explanation": "MultiIndex enables multiple index levels for complex data structures. Created via pd.MultiIndex.from_arrays(), from_tuples(), from_product(), or by grouping operations. Access via df.loc[(level1, level2)]. Methods include get_level_values(), swaplevel(), reset_index(). Essential for panel data, time series with multiple frequencies, and grouped statistics."},
        {"question": "What Pandas feature enables database-like operations on large datasets?", "answer": "Categorical data type", "alternatives": ["pd.Categorical"], "explanation": "Categorical data type stores data with limited unique values efficiently using integer codes plus categories mapping. Convert via df['col'].astype('category'). Reduces memory usage (especially for strings) and enables ordering. Methods include cat.codes, cat.categories, cat.ordered. Improves groupby performance. Ideal for repeated string values like gender, country, product type."},
        {"question": "What NumPy feature enables advanced array indexing?", "answer": "Fancy indexing", "alternatives": ["Advanced indexing"], "explanation": "Fancy indexing uses arrays of indices or boolean masks to access array elements. Integer array indexing: arr[[1,3,5]] selects specific indices. Boolean indexing: arr[arr > 5] selects elements meeting condition. Can combine with multiple dimensions. Always creates copy (not view). More flexible than slicing but slower. Essential for complex data selection."},
        {"question": "What pandas method performs SQL-style window functions?", "answer": "df.rolling()", "alternatives": ["rolling()"], "explanation": "rolling() performs rolling window calculations (moving statistics). Usage: df['col'].rolling(window=7).mean() for 7-period moving average. Parameters include window (size), min_periods (minimum observations), center (labels at center vs end). Supports custom functions. Complemented by expanding() (cumulative) and ewm() (exponential weighting). Essential for time series."},
        {"question": "What technique optimizes Pandas operations using chunking?", "answer": "Chunking", "alternatives": ["Chunksize parameter"], "explanation": "Chunking processes large files in smaller pieces to manage memory. Use chunksize parameter in read_csv(): for chunk in pd.read_csv('file.csv', chunksize=10000). Process each chunk iteratively, then combine results. Enables processing datasets larger than memory. Can parallelize chunks. Alternative to Dask for moderate-sized data."},
        {"question": "What NumPy function creates memory-efficient array views?", "answer": "np.view()", "alternatives": ["Array views"], "explanation": "NumPy slicing creates views (references to original data) rather than copies. Modifying view changes original. Views share memory with original, enabling memory-efficient operations on large arrays. Use arr.copy() for explicit copy. Some operations always create copies (fancy indexing). Check with arr.base to see if view. Understanding views prevents unintended modifications."},
        {"question": "What Pandas method optimizes memory by inferring best dtypes?", "answer": "df.infer_objects()", "alternatives": ["infer_objects()"], "explanation": "infer_objects() attempts to infer better data types for object columns. Combined with astype() and categorical types can dramatically reduce memory. Use df.memory_usage(deep=True) to check usage. Consider pd.to_numeric() with downcast, dates with parse_dates. Memory optimization crucial for large datasets. Complemented by df.info(memory_usage='deep')."},
        {"question": "What technique uses eval and query for efficient operations?", "answer": "Expression evaluation", "alternatives": ["df.eval()", "df.query()"], "explanation": "eval() and query() use string expressions for efficient operations. df.eval('C = A + B') creates column. df.query('A > B') filters rows. Uses numexpr for speed, especially with large DataFrames. Avoids creating intermediate arrays. Syntax limitation: only supports subset of Python. Beneficial when expressions involve multiple columns and chaining."},
        {"question": "What scikit-learn feature enables custom preprocessing pipelines?", "answer": "Pipeline", "alternatives": ["sklearn.pipeline.Pipeline"], "explanation": "Pipeline chains transformers and estimator into single object. Usage: Pipeline([('scaler', StandardScaler()), ('svm', SVC())]). Ensures consistent preprocessing for train/test. Simplifies cross-validation and grid search. All steps except last must have fit_transform; last has fit_predict. ColumnTransformer enables different preprocessing for different columns. Critical for preventing data leakage."}
    ]
}

ALL_ADDITIONAL_QUESTIONS['python_data_tools'] = PYTHON_DATA_TOOLS_QUESTIONS

# 19. Big Data
BIG_DATA_QUESTIONS = {
    "easy": [
        {"question": "What framework enables distributed processing of large datasets?", "answer": "Hadoop", "alternatives": ["Apache Hadoop"], "explanation": "Hadoop is an open-source framework for distributed storage and processing of large datasets across clusters. It includes HDFS (storage), MapReduce (processing), and YARN (resource management). Handles petabytes of data by distributing across commodity hardware. Fault-tolerant through replication. Foundation for big data ecosystem."},
        {"question": "What distributed file system stores data across multiple nodes?", "answer": "HDFS", "alternatives": ["Hadoop Distributed File System"], "explanation": "HDFS (Hadoop Distributed File System) stores large files by splitting into blocks (default 128MB) distributed across cluster nodes. Replicates blocks (default 3 copies) for fault tolerance. Optimized for sequential reads of large files. NameNode manages metadata; DataNodes store actual data. Write-once-read-many model."},
        {"question": "What programming model processes data in parallel?", "answer": "MapReduce", "alternatives": ["Map Reduce"], "explanation": "MapReduce is a programming model for processing large datasets in parallel. Map phase transforms input into key-value pairs; Reduce phase aggregates values for each key. Framework handles distribution, parallelization, and fault tolerance. Suitable for batch processing. Inspired Google's original paper. Now often replaced by Spark."},
        {"question": "What fast engine processes big data in memory?", "answer": "Spark", "alternatives": ["Apache Spark"], "explanation": "Apache Spark is a fast, general-purpose cluster computing engine processing data in memory (up to 100x faster than MapReduce). Supports batch, streaming, ML, and graph processing. Uses RDDs (Resilient Distributed Datasets) and DataFrames. Integrates with Hadoop, can run standalone or on YARN/Mesos. Provides APIs for Python, Scala, Java, R."},
        {"question": "What data structure enables distributed computing in Spark?", "answer": "RDD", "alternatives": ["Resilient Distributed Dataset"], "explanation": "RDD (Resilient Distributed Dataset) is Spark's fundamental data structure - immutable, distributed collection of objects processed in parallel. Supports transformations (map, filter) and actions (count, collect). Automatically recovers from node failures. Lazy evaluation defers computation until action called. Foundation for DataFrames and Datasets."},
        {"question": "What NoSQL database stores data in column families?", "answer": "HBase", "alternatives": ["Apache HBase"], "explanation": "Apache HBase is a distributed, scalable NoSQL database built on HDFS providing random, real-time read/write access to big data. Uses column-family storage model. Horizontally scalable to billions of rows and millions of columns. Based on Google's Bigtable. Integrates with Hadoop MapReduce. Good for sparse data and time-series."},
        {"question": "What streaming platform handles real-time data pipelines?", "answer": "Kafka", "alternatives": ["Apache Kafka"], "explanation": "Apache Kafka is a distributed streaming platform for building real-time data pipelines and applications. Publishes and subscribes to streams of records; stores streams durably and fault-tolerantly; processes streams as they occur. High throughput, low latency. Used for logs, metrics, event sourcing, stream processing. Integrates with Spark, Flink, Storm."},
        {"question": "What SQL engine queries data in HDFS?", "answer": "Hive", "alternatives": ["Apache Hive"], "explanation": "Apache Hive provides SQL-like interface (HiveQL) for querying data stored in HDFS. Translates queries to MapReduce/Tez/Spark jobs. Supports tables with schema-on-read. Useful for data warehousing, batch processing, and analytics. Metastore tracks table schemas. Not suitable for real-time queries or row-level updates."},
        {"question": "What term describes extremely large, complex datasets?", "answer": "Big Data", "alternatives": ["Big data"], "explanation": "Big Data refers to datasets too large or complex for traditional processing applications. Characterized by 3 Vs: Volume (scale), Velocity (speed), Variety (types). Later expanded to Veracity (uncertainty) and Value (worth). Requires distributed systems and specialized tools. Applications in business intelligence, science, IoT, social media analytics."},
        {"question": "What storage repository holds structured and unstructured data?", "answer": "Data lake", "alternatives": ["Data Lake"], "explanation": "Data lake is centralized repository storing raw data at any scale - structured, semi-structured, and unstructured. Unlike data warehouses requiring schema-on-write, data lakes use schema-on-read. Cost-effective storage on HDFS, S3, Azure Data Lake. Enables ML, analytics, and reporting. Requires governance to avoid becoming data swamp."}
    ],
    "average": [
        {"question": "What Spark component handles real-time stream processing?", "answer": "Spark Streaming", "alternatives": ["Structured Streaming"], "explanation": "Spark Streaming processes live data streams in micro-batches. Divides stream into small batches processed by Spark engine. Structured Streaming (newer) treats streams as unbounded tables. Integrates with Kafka, Flume, Kinesis. Supports stateful operations (windowing, sessionization). Provides fault tolerance and exactly-once semantics."},
        {"question": "What resource manager schedules jobs in Hadoop 2?", "answer": "YARN", "alternatives": ["Yet Another Resource Negotiator"], "explanation": "YARN (Yet Another Resource Negotiator) is Hadoop 2's resource management layer separating resource management from data processing. ResourceManager allocates resources; NodeManagers monitor containers. ApplicationMasters coordinate application execution. Enables multiple processing frameworks (MapReduce, Spark, Tez) to share cluster. Improves scalability and efficiency."},
        {"question": "What Spark API provides distributed SQL processing?", "answer": "Spark SQL", "alternatives": ["SparkSQL"], "explanation": "Spark SQL is Spark module for structured data processing using SQL queries or DataFrame API. Reads from Hive, Avro, Parquet, JSON, JDBC. Catalyst optimizer improves query performance. Tungsten execution engine uses code generation. Unifies SQL and programmatic data manipulation. DataFrames are distributed collections with schema."},
        {"question": "What library provides machine learning in Spark?", "answer": "MLlib", "alternatives": ["Spark MLlib"], "explanation": "MLlib is Spark's scalable machine learning library. Provides distributed implementations of classification, regression, clustering, collaborative filtering, dimensionality reduction. Includes featurization, pipelines, model persistence. Integrates with DataFrame API (spark.ml) and RDD API (spark.mllib - maintenance mode). Scales to large datasets across clusters."},
        {"question": "What columnar storage format optimizes analytics?", "answer": "Parquet", "alternatives": ["Apache Parquet"], "explanation": "Apache Parquet is columnar storage format optimized for analytics. Stores columns together (vs rows) enabling efficient compression and encoding. Skip irrelevant columns for queries. Supports complex nested data structures. Language-agnostic. Widely used in Hadoop ecosystem (Spark, Hive, Impala). Better compression ratios than row formats for analytical workloads."},
        {"question": "What stream processing framework has low-latency processing?", "answer": "Flink", "alternatives": ["Apache Flink"], "explanation": "Apache Flink is stream processing framework for stateful computations over unbounded and bounded data streams. True stream processing (not micro-batches). Provides event time processing, stateful operations, exactly-once semantics. Supports batch as special case of streaming. Includes CEP, ML, graphs. Lower latency than Spark Streaming for real-time."},
        {"question": "What technology enables SQL queries on distributed data?", "answer": "Presto", "alternatives": ["PrestoDB", "Trino"], "explanation": "Presto (now Trino) is distributed SQL query engine for running interactive analytic queries. Queries data where it lives (HDFS, S3, databases) without ETL. Low latency (seconds) vs Hive (minutes). In-memory pipelined execution. Supports joins, aggregations, window functions. Used by Facebook, Netflix, Airbnb. Does not use MapReduce."},
        {"question": "What pattern processes data in batch and streaming?", "answer": "Lambda architecture", "alternatives": ["Lambda"], "explanation": "Lambda architecture handles massive data quantities by combining batch and stream processing. Batch layer stores master dataset and computes batch views. Speed layer processes recent data for low latency. Serving layer merges views. Addresses CAP theorem limitations. Complexity: maintaining two processing paths. Kappa architecture simplifies by using only streaming."},
        {"question": "What coordinator service manages distributed applications?", "answer": "ZooKeeper", "alternatives": ["Apache ZooKeeper"], "explanation": "Apache ZooKeeper is centralized service for maintaining configuration, naming, distributed synchronization, and group services. Provides coordination for distributed systems. Used by Hadoop, HBase, Kafka, Storm. Maintains data in-memory for high throughput. Ensemble of servers prevents single point of failure. Critical for distributed system reliability."},
        {"question": "What technique partitions data across nodes?", "answer": "Sharding", "alternatives": ["Data sharding"], "explanation": "Sharding is horizontal partitioning that splits large database into smaller, faster pieces (shards) distributed across servers. Each shard contains subset of data. Improves performance and scalability. Sharding key determines data distribution. Challenges include query routing, rebalancing, and joins across shards. Common in NoSQL databases and distributed systems."}
    ],
    "difficult": [
        {"question": "What optimization reduces data shuffling in Spark?", "answer": "Partitioning", "alternatives": ["Data partitioning"], "explanation": "Partitioning controls how data is distributed across cluster. Proper partitioning minimizes shuffles (expensive data movement between nodes). HashPartitioner uses hash of keys; RangePartitioner sorts keys. Custom partitioners possible. Persist partitioned RDDs to reuse. Partition count affects parallelism - too few limits concurrency, too many adds overhead. Key for performance."},
        {"question": "What technique enables incremental view maintenance?", "answer": "Delta processing", "alternatives": ["Incremental processing"], "explanation": "Delta processing computes only changes since last batch instead of reprocessing entire dataset. Tracks changed data (inserts, updates, deletes). Dramatically reduces computation for incremental views. Implemented via watermarks, change data capture, or delta tables. Technologies include Delta Lake, Apache Iceberg. Essential for efficient data lake architectures and real-time analytics."},
        {"question": "What storage layer adds ACID transactions to data lakes?", "answer": "Delta Lake", "alternatives": ["Databricks Delta"], "explanation": "Delta Lake is open-source storage layer providing ACID transactions, schema enforcement, and time travel for data lakes. Built on Parquet. Handles concurrent reads/writes safely. Enables updates, deletes, merges on data lakes. Transaction log tracks all changes. Schema evolution support. Unifies batch and streaming. Developed by Databricks, now Linux Foundation project."},
        {"question": "What technique handles late-arriving data in streaming?", "answer": "Watermarking", "alternatives": ["Event time watermarking"], "explanation": "Watermarking handles out-of-order and late data in stream processing using event time. Watermark is threshold declaring all data with timestamps below threshold has arrived. Enables window computations to complete and produce results. Balance between waiting for late data (latency) and completeness (accuracy). Supported in Spark Structured Streaming, Flink. Critical for event-time processing."},
        {"question": "What pattern simplifies Lambda with only streaming?", "answer": "Kappa architecture", "alternatives": ["Kappa"], "explanation": "Kappa architecture simplifies Lambda by using only stream processing for both real-time and batch workloads. Treats batch as replay of streaming data. Single processing path reduces complexity. Relies on replayable message queue (Kafka). Reprocessing involves replaying from queue. Simpler than Lambda but requires streaming-first mindset. Suitable when batch views can be computed via streaming."},
        {"question": "What technique optimizes joins by broadcasting small tables?", "answer": "Broadcast join", "alternatives": ["Map-side join"], "explanation": "Broadcast join optimizes join performance by broadcasting small table to all nodes, avoiding shuffle. Each node has complete copy of small table, joins locally with large table partition. Dramatically faster than shuffle join for small-large table joins. Spark automatically broadcasts tables under spark.sql.autoBroadcastJoinThreshold (default 10MB). Also called map-side join in MapReduce."},
        {"question": "What scheduling optimizes resource utilization in YARN?", "answer": "Capacity Scheduler", "alternatives": ["Capacity scheduling"], "explanation": "Capacity Scheduler is YARN scheduler enabling multiple tenants to share large cluster while ensuring capacity guarantees. Hierarchical queues with capacity allocations. Supports priority, preemption, resource limits. Elasticity allows queue to consume spare capacity. Fair Scheduler is alternative focusing on fairness. Both support ACLs for secure multi-tenancy. Critical for shared production clusters."},
        {"question": "What technique minimizes data movement in distributed queries?", "answer": "Predicate pushdown", "alternatives": ["Filter pushdown"], "explanation": "Predicate pushdown optimizes queries by pushing filters close to data source, reading only relevant data. Reduces data transfer and processing. Columnar formats (Parquet) especially benefit. Presto, Spark SQL use pushdown extensively. Example: filtering in storage layer vs after loading. Also applies projections (column pruning). Catalyst optimizer in Spark implements pushdown. Fundamental query optimization."},
        {"question": "What consensus algorithm ensures distributed system agreement?", "answer": "Paxos", "alternatives": ["Raft"], "explanation": "Paxos is consensus algorithm enabling distributed systems to agree on values despite failures. Handles network partitions, message loss, node crashes. Proposers suggest values; acceptors vote; learners learn chosen value. Challenging to understand and implement. Raft is simpler alternative with same guarantees. Used in ZooKeeper (modified), Spanner. Critical for distributed consistency."},
        {"question": "What technique handles schema evolution in streaming?", "answer": "Schema registry", "alternatives": ["Confluent Schema Registry"], "explanation": "Schema registry centrally manages and enforces schemas for streaming data. Stores schema versions; validates producer/consumer compatibility. Supports backward, forward, full compatibility. Integrates with Kafka (Confluent Schema Registry). Enables schema evolution without breaking applications. Uses compact schema representations (Avro, Protobuf, JSON Schema). Critical for long-running streaming pipelines with evolving data."}
    ]
}

ALL_ADDITIONAL_QUESTIONS['big_data'] = BIG_DATA_QUESTIONS

# 20. Data Ethics
DATA_ETHICS_QUESTIONS = {
    "easy": [
        {"question": "What principle ensures individuals control their personal data?", "answer": "Privacy", "alternatives": ["Data privacy"], "explanation": "Privacy protects individuals' personal information from unauthorized access and use. Includes concepts of consent, anonymity, and confidentiality. Regulations like GDPR and CCPA enforce privacy rights. Organizations must implement security measures and transparent data practices. Privacy builds trust and is fundamental right in digital age."},
        {"question": "What systematic error makes AI models unfair?", "answer": "Bias", "alternatives": ["Algorithmic bias"], "explanation": "Bias in AI refers to systematic errors making predictions unfairly favor or discriminate against groups. Sources include biased training data, feature selection, or algorithm design. Can amplify existing societal biases. Affects decisions in hiring, lending, criminal justice. Mitigation requires diverse data, fairness metrics, and regular auditing."},
        {"question": "What principle ensures AI systems treat all groups equally?", "answer": "Fairness", "alternatives": ["Algorithmic fairness"], "explanation": "Fairness ensures AI systems don't discriminate against individuals or groups based on protected characteristics (race, gender, age). Multiple definitions exist: demographic parity, equalized odds, individual fairness. Trade-offs between fairness metrics. Requires defining fairness for specific context, measuring disparate impact, and implementing mitigation strategies."},
        {"question": "What regulation protects EU citizens' data?", "answer": "GDPR", "alternatives": ["General Data Protection Regulation"], "explanation": "GDPR (General Data Protection Regulation) is EU law protecting personal data and privacy. Key rights: access, rectification, erasure ('right to be forgotten'), portability. Requires explicit consent, data protection by design, breach notification. Applies to organizations processing EU residents' data regardless of location. Heavy fines for non-compliance. Influenced global data regulations."},
        {"question": "What process involves explaining AI decisions to humans?", "answer": "Explainability", "alternatives": ["Interpretability"], "explanation": "Explainability refers to making AI decisions understandable to humans. Important for trust, debugging, compliance, and fairness. Techniques include LIME, SHAP, attention visualization, decision trees. Trade-off between model performance (complex models) and interpretability (simple models). Critical for high-stakes decisions (healthcare, finance, justice). Regulations increasingly require explainability."},
        {"question": "What term describes using data ethically and responsibly?", "answer": "Data governance", "alternatives": ["Data stewardship"], "explanation": "Data governance establishes policies and procedures for managing data as strategic asset. Ensures quality, security, privacy, compliance. Defines roles (data owners, stewards, custodians), standards, and processes. Includes data lifecycle management, access control, audit trails. Prevents data becoming liability. Essential for ethical data use and regulatory compliance."},
        {"question": "What process gets permission before collecting data?", "answer": "Consent", "alternatives": ["Informed consent"], "explanation": "Consent is permission from individuals to collect, use, and share their personal data. Should be informed (understanding purpose), specific (not blanket), freely given (not coerced), and revocable. GDPR requires explicit consent for sensitive data. Consent fatigue is challenge. Ethical data practices prioritize meaningful consent over checkbox compliance."},
        {"question": "What practice removes identifying information from data?", "answer": "Anonymization", "alternatives": ["De-identification"], "explanation": "Anonymization removes or modifies personally identifiable information so individuals cannot be identified. Techniques include removing direct identifiers, aggregation, generalization, noise addition, pseudonymization. Challenge: re-identification attacks combining multiple datasets. K-anonymity, l-diversity, differential privacy provide formal guarantees. Critical for privacy-preserving data sharing and research."},
        {"question": "What obligation requires organizations to justify data use?", "answer": "Accountability", "alternatives": ["Data accountability"], "explanation": "Accountability means organizations are responsible for data practices and must demonstrate compliance. Includes documenting decisions, conducting audits, appointing data protection officers. Organizations must be able to justify data collection and use. GDPR emphasizes accountability principle. Requires culture of privacy and ethics beyond legal compliance. Essential for building trust."},
        {"question": "What ethical concern involves AI making important decisions?", "answer": "Algorithmic decision-making", "alternatives": ["Automated decision-making"], "explanation": "Algorithmic decision-making uses AI to make or significantly influence important decisions affecting individuals. Concerns include bias, lack of transparency, accountability gaps, and removing human judgment. GDPR grants right to human review of automated decisions. Requires careful consideration of when AI should decide versus assist humans. High-stakes domains need special scrutiny."}
    ],
    "average": [
        {"question": "What framework measures fairness across protected groups?", "answer": "Disparate impact", "alternatives": ["Adverse impact"], "explanation": "Disparate impact occurs when neutral policy or practice disproportionately harms protected group. 80% rule (four-fifths rule): selection rate for protected group should be at least 80% of highest group. Used in employment, lending, and ML fairness. Doesn't require intentional discrimination. Organizations must demonstrate business necessity. Metrics include demographic parity, equalized odds."},
        {"question": "What technique provides mathematical privacy guarantee?", "answer": "Differential privacy", "alternatives": ["DP"], "explanation": "Differential privacy provides mathematical guarantee that individual's data doesn't significantly affect query results. Adds calibrated noise to protect individuals while maintaining statistical accuracy. Privacy budget (epsilon) controls trade-off between privacy and utility. Used by Apple, Google, US Census. Composable across queries. Strongest privacy definition for statistical databases."},
        {"question": "What principle ensures AI systems are transparent?", "answer": "Algorithmic transparency", "alternatives": ["Model transparency"], "explanation": "Algorithmic transparency means AI systems' operations, decisions, and logic are open and understandable. Includes documenting data, features, models, and decision processes. Enables scrutiny, accountability, and trust. Balanced against trade secrets and security. Regulations increasingly require transparency. Ranges from simple disclosures to full code release. Essential for responsible AI."},
        {"question": "What ethical issue involves unauthorized use of data?", "answer": "Data misuse", "alternatives": ["Data abuse"], "explanation": "Data misuse involves using data in ways not intended or authorized by data subjects. Examples: selling data without consent, using for discriminatory purposes, inadequate security. Can violate privacy laws and erode trust. Organizations must clearly define acceptable use policies, implement access controls, audit usage. Employees handling data need ethics training."},
        {"question": "What practice ensures AI benefits are distributed fairly?", "answer": "Distributive justice", "alternatives": ["Equitable distribution"], "explanation": "Distributive justice in AI concerns fair distribution of benefits and burdens. Questions: Who benefits from AI? Who is harmed? Are benefits accessible to all? Addresses digital divide, automation impacts on employment, access to AI healthcare. Requires considering stakeholder impacts beyond economic efficiency. Policy interventions may be needed to ensure equitable outcomes."},
        {"question": "What process audits AI systems for bias?", "answer": "Fairness audit", "alternatives": ["Bias audit"], "explanation": "Fairness audit systematically evaluates AI systems for discriminatory outcomes. Involves testing across protected groups, measuring fairness metrics, identifying bias sources. Can be internal (self-audit) or external (third-party). Increasingly mandated by regulation (NYC bias audit law). Should be ongoing, not one-time. Includes documentation and remediation plans. Critical for responsible AI deployment."},
        {"question": "What framework guides responsible AI development?", "answer": "AI ethics principles", "alternatives": ["Responsible AI principles"], "explanation": "AI ethics principles provide guidelines for responsible development and deployment. Common principles: fairness, transparency, accountability, privacy, safety, human agency. Organizations like IEEE, EU, and companies publish frameworks. Challenge: translating principles to practice. Requires ethics by design, cross-disciplinary teams, stakeholder engagement. Living documents adapted as technology evolves."},
        {"question": "What concern involves AI models memorizing training data?", "answer": "Privacy leakage", "alternatives": ["Data leakage"], "explanation": "Privacy leakage occurs when models inadvertently reveal training data information. Membership inference attacks detect if specific data was in training set. Model inversion reconstructs training data. More severe with overfitting and memorization. Mitigations include differential privacy, regularization, and federated learning. Especially concerning for sensitive data (medical records, personal communications)."},
        {"question": "What right allows individuals to request data deletion?", "answer": "Right to be forgotten", "alternatives": ["Right to erasure"], "explanation": "Right to be forgotten (GDPR Article 17) allows individuals to request deletion of their personal data under certain conditions. Balances privacy against legitimate interests (legal obligations, freedom of expression). Challenges include backups, derived data, third-party sharing. Machine unlearning researches removing specific data's influence from trained models. Controversial in free speech contexts."},
        {"question": "What technique enables collaborative learning without sharing data?", "answer": "Federated learning", "alternatives": ["Federated ML"], "explanation": "Federated learning trains models across decentralized devices without exchanging raw data. Devices train locally; only model updates shared. Aggregation server combines updates into global model. Preserves privacy while enabling collaboration. Used by Google (Gboard), Apple (Siri). Challenges include communication costs, heterogeneity, and potential privacy attacks on updates. Growing area for privacy-preserving ML."}
    ],
    "difficult": [
        {"question": "What paradox shows fairness definitions can conflict?", "answer": "Fairness impossibility theorem", "alternatives": ["Impossibility of fairness"], "explanation": "Fairness impossibility theorems prove multiple fairness definitions cannot be simultaneously satisfied (except in trivial cases). Example: calibration and demographic parity conflict. Must choose which fairness notion to optimize. Choice depends on context and stakeholders. Highlights that fairness is socio-technical, not purely mathematical. Requires stakeholder engagement to define appropriate fairness for specific application."},
        {"question": "What attack reconstructs training data from model?", "answer": "Model inversion", "alternatives": ["Model inversion attack"], "explanation": "Model inversion attacks reconstruct training data from model parameters or predictions. Exploits models' tendency to memorize training examples. Demonstrated for face recognition (reconstructing faces from name), medical models, language models. Severity depends on model type and overfitting. Defenses include differential privacy, regularization, and limiting query access. Growing concern as models become more powerful."},
        {"question": "What technique removes specific data's influence from trained model?", "answer": "Machine unlearning", "alternatives": ["Data deletion"], "explanation": "Machine unlearning removes specific training data's influence from model without complete retraining. Required for right to be forgotten, fixing poisoned data, removing copyrighted content. Approaches include retraining from scratch (expensive), influence functions (approximate), and sharding (partition training). Active research area. Challenges include verifying complete removal and maintaining model performance."},
        {"question": "What framework ensures AI systems cause no harm?", "answer": "AI safety", "alternatives": ["Safe AI"], "explanation": "AI safety researches ensuring AI systems behave as intended and don't cause harm. Concerns include specification (what goal to optimize), robustness (handling distribution shift), interpretability (understanding decisions), and alignment (matching human values). Increasingly important as AI capabilities grow. Includes technical research and governance. Organizations like OpenAI, DeepMind, MIRI focus on safety."},
        {"question": "What ethical issue involves AI amplifying inequality?", "answer": "Automation inequality", "alternatives": ["Algorithmic inequality"], "explanation": "Automation inequality refers to AI systems disproportionately benefiting privileged groups while harming vulnerable populations. Privileged get helpful AI (recommendation, assistance); vulnerable get punitive AI (surveillance, scoring). Examples: predictive policing in minority neighborhoods, automated benefits denial. Results from biased data, misaligned incentives, and power imbalances. Requires policy interventions and equity-focused design."},
        {"question": "What technique proves algorithm satisfies fairness constraints?", "answer": "Formal verification", "alternatives": ["Algorithmic verification"], "explanation": "Formal verification uses mathematical methods to prove algorithms satisfy specified properties (including fairness constraints). Creates formal specifications, then proves implementation meets them. More rigorous than testing but limited scalability. Applied to critical systems (aviation, medical devices). Emerging for fairness verification. Provides strong guarantees but requires expert knowledge and may not capture all fairness notions."},
        {"question": "What concern involves AI making decisions without explanation?", "answer": "Black box problem", "alternatives": ["Opacity"], "explanation": "Black box problem refers to inability to understand how complex AI models (deep neural networks) make decisions. Creates accountability gaps, makes debugging difficult, limits trust, and complicates fairness audits. Explainable AI (XAI) attempts solutions through LIME, SHAP, attention mechanisms. Trade-off between performance and interpretability. Regulations may require explanations for consequential decisions."},
        {"question": "What framework considers long-term AI impacts on humanity?", "answer": "AI alignment", "alternatives": ["Value alignment"], "explanation": "AI alignment ensures advanced AI systems' goals align with human values and interests. Challenges include defining human values, avoiding unintended consequences, maintaining alignment as AI becomes more capable. Includes inverse reinforcement learning, value learning, corrigibility (accepting corrections). Critical for preventing existential risk from superintelligent AI. Active research area in AI safety."},
        {"question": "What technique enables privacy-preserving computation?", "answer": "Homomorphic encryption", "alternatives": ["FHE"], "explanation": "Homomorphic encryption enables computation on encrypted data without decryption. Results remain encrypted; only data owner can decrypt. Enables privacy-preserving cloud computing, secure outsourcing, confidential AI inference. Fully homomorphic encryption (FHE) supports arbitrary computations but is computationally expensive. Partially homomorphic encryption limits operations but is more practical. Active research making it more efficient."},
        {"question": "What framework holds organizations accountable for AI outcomes?", "answer": "Algorithmic accountability", "alternatives": ["AI accountability"], "explanation": "Algorithmic accountability means organizations are responsible for AI systems' decisions and impacts. Includes documentation, auditing, impact assessments, and remediation. Requires clear governance structures, designated responsible parties, and mechanisms for redress. Regulations increasingly mandate accountability (EU AI Act). Challenges include complexity, distributed responsibility, and technical opacity. Essential for trustworthy AI at scale."}
    ]
}

ALL_ADDITIONAL_QUESTIONS['data_ethics'] = DATA_ETHICS_QUESTIONS
