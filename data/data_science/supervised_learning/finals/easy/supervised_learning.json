{
  "subtopic_id": "supervised_learning",
  "subtopic_name": "Supervised Learning",
  "mode": "finals",
  "difficulty": "easy",
  "questions": [
    {
      "question": "What algorithm predicts continuous numerical outputs?",
      "answer": "Regression",
      "alternatives": [
        "Regression analysis"
      ],
      "explanation": "Regression predicts continuous numeric outcomes from input features. Linear regression models relationships as straight lines; polynomial regression captures curves. Applications include house price prediction, sales forecasting, and risk assessment. Regression differs from classification which predicts categories."
    },
    {
      "question": "What algorithm predicts categorical class labels?",
      "answer": "Classification",
      "alternatives": [
        "Classifier"
      ],
      "explanation": "Classification assigns inputs to discrete categories or classes. Binary classification has two classes (spam/not spam); multi-class has more (digit recognition). Algorithms include logistic regression, decision trees, neural networks, and SVM. Output is typically a class label or probability distribution over classes."
    },
    {
      "question": "What simple algorithm classifies based on majority vote of k nearest neighbors?",
      "answer": "K-Nearest Neighbors",
      "alternatives": [
        "k-NN",
        "KNN"
      ],
      "explanation": "K-Nearest Neighbors (k-NN) classifies by finding k closest training examples and using majority vote. It's non-parametric (no training phase) and intuitive but slow for large datasets. Choice of k and distance metric are crucial - small k is sensitive to noise, large k smooths boundaries. Feature scaling is essential."
    },
    {
      "question": "What tree-based model splits data using feature thresholds?",
      "answer": "Decision Tree",
      "alternatives": [
        "Decision tree classifier"
      ],
      "explanation": "Decision trees make predictions by learning decision rules from features, creating a tree structure of if-then-else decisions. They're interpretable, handle non-linear relationships, and require minimal preprocessing. However, they easily overfit without pruning or ensemble methods like Random Forest. They work for both classification and regression."
    },
    {
      "question": "What linear model uses logistic function for binary classification?",
      "answer": "Logistic Regression",
      "alternatives": [
        "Log regression"
      ],
      "explanation": "Despite its name, logistic regression is a classification algorithm using the logistic/sigmoid function to model probability of binary outcomes. It's simple, interpretable, and provides probability estimates. Coefficients indicate feature importance and direction. Regularization (L1/L2) prevents overfitting. Multi-class variants include one-vs-rest and softmax regression."
    },
    {
      "question": "What algorithm finds optimal separating hyperplane between classes?",
      "answer": "Support Vector Machine",
      "alternatives": [
        "SVM"
      ],
      "explanation": "Support Vector Machines (SVM) find the hyperplane that maximally separates classes. Only points near the boundary (support vectors) affect the decision boundary. Kernel trick enables non-linear classification by mapping to higher dimensions. SVMs work well for high-dimensional data but are sensitive to scaling and kernel choice."
    },
    {
      "question": "What probabilistic classifier assumes feature independence?",
      "answer": "Naive Bayes",
      "alternatives": [
        "Naive Bayes classifier"
      ],
      "explanation": "Naive Bayes classifiers apply Bayes' theorem assuming features are conditionally independent given the class (the 'naive' assumption). Despite this unrealistic assumption, they work surprisingly well for text classification and spam filtering. They're fast, require little training data, and handle high dimensions well. Variants include Gaussian, Multinomial, and Bernoulli."
    },
    {
      "question": "What ensemble method combines multiple decision trees?",
      "answer": "Random Forest",
      "alternatives": [
        "Random forest classifier"
      ],
      "explanation": "Random Forest creates many decision trees using random subsets of data and features, then averages their predictions. This reduces overfitting while maintaining trees' interpretability and handling non-linearity. Random Forests often work well out-of-the-box, provide feature importance estimates, and are robust to noise and outliers."
    },
    {
      "question": "What metric shows trade-off between true and false positive rates?",
      "answer": "ROC curve",
      "alternatives": [
        "Receiver Operating Characteristic"
      ],
      "explanation": "ROC (Receiver Operating Characteristic) curves plot true positive rate (recall) versus false positive rate at various threshold settings. The area under ROC curve (AUC-ROC) summarizes performance - 0.5 is random, 1.0 is perfect. ROC curves are useful for comparing models and selecting operating points, especially with imbalanced data."
    },
    {
      "question": "What line equation describes the relationship between input and output?",
      "answer": "Linear regression",
      "alternatives": [
        "Linear model"
      ],
      "explanation": "Linear regression models the relationship between features and target as a linear equation: y = b0 + b1x1 + b2x2 + ... Coefficients indicate feature effects; intercept is the baseline. Ordinary Least Squares finds coefficients minimizing squared prediction errors. Assumptions include linearity, independence, homoscedasticity, and normal residuals."
    }
  ]
}