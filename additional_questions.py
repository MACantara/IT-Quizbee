"""
Additional questions for Computer Architecture & IT Security subtopics.
This module extends the QUESTIONS_DB with more subtopic questions.
"""

# 3. Instruction Set Architecture
INSTRUCTION_SET_QUESTIONS = {
    "easy": [
        {"question": "What is the architecture design philosophy that uses a small set of simple, fast instructions?", "answer": "RISC (Reduced Instruction Set Computer)", "alternatives": ["Reduced Instruction Set Computer", "RISC Architecture"], "explanation": "RISC (Reduced Instruction Set Computer) philosophy emphasizes a small set of simple instructions that execute in a single clock cycle. This approach, used in ARM and MIPS processors, prioritizes simplicity and speed over instruction complexity, relying on compilers to generate efficient code sequences."},
        {"question": "What is the part of an instruction that specifies the operation to be performed?", "answer": "Opcode", "alternatives": ["Operation Code"], "explanation": "The opcode (operation code) is the portion of a machine instruction that specifies the operation to perform, such as ADD, LOAD, or JUMP. It tells the CPU what action to execute, while operands specify what data to use."},
        {"question": "What is the name of the interface that defines all instructions a CPU can execute?", "answer": "ISA (Instruction Set Architecture)", "alternatives": ["Instruction Set Architecture"], "explanation": "The Instruction Set Architecture (ISA) is the abstract interface between hardware and software, defining all instructions, registers, memory addressing modes, and data types a processor supports. It's crucial for software compatibility - programs compiled for one ISA won't run on a processor with a different ISA without emulation."},
        {"question": "What popular instruction set architecture is used in most smartphones and tablets?", "answer": "ARM", "alternatives": ["ARM Architecture", "Advanced RISC Machine"], "explanation": "ARM (Advanced RISC Machine, originally Acorn RISC Machine) is a RISC-based ISA dominant in mobile devices due to its power efficiency. ARM processors power most smartphones, tablets, and increasingly laptops, with Apple's M-series chips and Qualcomm's Snapdragon being notable examples."},
        {"question": "What is the low-level programming language that uses mnemonics for machine instructions?", "answer": "Assembly Language", "alternatives": ["Assembly", "Assembler Language"], "explanation": "Assembly language uses human-readable mnemonics (like MOV, ADD, JMP) instead of binary machine code, making it easier to write low-level programs. Each assembly instruction typically corresponds to one machine instruction, and an assembler translates assembly code into machine code."},
        {"question": "What instruction set architecture is used by Intel and AMD processors?", "answer": "x86", "alternatives": ["x86 Architecture", "IA-32"], "explanation": "x86 is a CISC instruction set architecture originally developed by Intel, used in most desktop and laptop computers. It includes 32-bit (IA-32) and 64-bit (x86-64/AMD64) versions. Despite being complex, x86's backward compatibility has made it the dominant desktop/server architecture."},
        {"question": "What is the binary representation of instructions that the CPU directly executes?", "answer": "Machine Code", "alternatives": ["Machine Language", "Binary Code"], "explanation": "Machine code consists of binary instructions (1s and 0s) that the CPU can directly execute without translation. Every program must ultimately be converted to machine code, whether from high-level languages (via compilation) or assembly language (via assembly)."},
        {"question": "What addressing mode uses a fixed memory address in the instruction?", "answer": "Direct Addressing", "alternatives": ["Absolute Addressing"], "explanation": "In direct addressing mode, the instruction contains the actual memory address of the operand. This is simple and fast but limits flexibility since addresses are hardcoded. For example, 'LOAD 1000' would load the value from memory address 1000."},
        {"question": "What is the term for the method used to specify where operands are located?", "answer": "Addressing Mode", "alternatives": ["Addressing Modes"], "explanation": "Addressing modes determine how the CPU interprets operand specifications in instructions. Common modes include immediate (value in instruction), direct (address in instruction), indirect (address of address), and register (operand in register). Different modes offer tradeoffs between simplicity, flexibility, and efficiency."},
        {"question": "What type of instruction set has variable-length instructions?", "answer": "CISC", "alternatives": ["Complex Instruction Set Computer"], "explanation": "CISC (Complex Instruction Set Computer) architectures like x86 use variable-length instructions, ranging from 1 to 15+ bytes. This allows compact code representation but complicates instruction decoding. In contrast, RISC architectures typically use fixed-length instructions (e.g., 32 bits) for simpler decoding."}
    ],
    "average": [
        {"question": "What RISC-based instruction set architecture is gaining popularity for its open-source, royalty-free nature?", "answer": "RISC-V", "alternatives": ["RISCV"], "explanation": "RISC-V is an open-source RISC instruction set architecture that anyone can use without royalties. Unlike ARM or x86, RISC-V specifications are freely available, allowing custom processor designs. It's gaining traction in embedded systems, research, and increasingly in commercial products, offering an alternative to proprietary ISAs."},
        {"question": "What addressing mode calculates the operand address by adding a constant to a register value?", "answer": "Base-Displacement addressing", "alternatives": ["Indexed addressing", "Register-Offset addressing"], "explanation": "Base-displacement (or indexed) addressing adds a constant offset to a base register value to calculate the operand address. This is extremely useful for accessing array elements, structure fields, and stack variables. For example, accessing array[5] might use base register + (5 × element_size)."},
        {"question": "What is the term for instructions that operate on operands located in CPU registers only?", "answer": "Register-to-Register instructions", "alternatives": ["Register operations", "R-type instructions"], "explanation": "Register-to-register (or R-type) instructions operate entirely on register operands without accessing memory. They're the fastest instructions since registers are immediately accessible. RISC architectures emphasize register operations, using separate load/store instructions for memory access (load-store architecture)."},
        {"question": "What technique allows a single instruction to operate on multiple data elements simultaneously?", "answer": "SIMD (Single Instruction Multiple Data)", "alternatives": ["Vector processing", "SIMD"], "explanation": "SIMD allows a single instruction to perform the same operation on multiple data elements in parallel. Modern processors include SIMD extensions (SSE, AVX for x86; NEON for ARM) that can process 4, 8, or 16 values simultaneously. This is crucial for multimedia processing, scientific computing, and machine learning."},
        {"question": "What is the name for instructions that transfer control to a different part of the program?", "answer": "Branch instructions", "alternatives": ["Jump instructions", "Control flow instructions"], "explanation": "Branch (or jump) instructions alter program control flow by changing the program counter to a different address. They include conditional branches (branch if condition true), unconditional jumps, function calls, and returns. Efficient branch handling is critical for performance, hence sophisticated branch prediction in modern CPUs."},
        {"question": "What addressing mode uses the contents of a register as the memory address?", "answer": "Register Indirect addressing", "alternatives": ["Indirect addressing", "Register deferred"], "explanation": "In register indirect addressing, a register contains the memory address of the operand, not the operand itself. This enables dynamic addressing, crucial for pointer operations, dynamic data structures, and implementing arrays. For example, if R1=1000, then 'LOAD (R1)' loads from address 1000."},
        {"question": "What is the term for ISA features that maintain compatibility with older software?", "answer": "Backward compatibility", "alternatives": ["Legacy support"], "explanation": "Backward compatibility ensures new processors can run software designed for older processors in the same family. x86 maintains remarkable backward compatibility - modern 64-bit processors can run 16-bit DOS programs. This is crucial for commercial success but constrains architectural innovation."},
        {"question": "What instruction format dedicates separate fields for opcode and operands with fixed sizes?", "answer": "Fixed-format instructions", "alternatives": ["RISC format", "Regular format"], "explanation": "Fixed-format instructions use predetermined field positions and sizes for opcode and operands. This simplifies decoding logic and enables faster instruction processing, typical of RISC architectures. All instructions are the same length (e.g., 32 bits), though this may waste encoding space compared to variable-length formats."},
        {"question": "What is the technique where the compiler, not hardware, schedules instructions to avoid hazards?", "answer": "Static scheduling", "alternatives": ["Compile-time scheduling"], "explanation": "Static scheduling relies on the compiler to arrange instructions to minimize pipeline stalls and hazards. This is common in RISC processors where the ISA exposes pipeline behavior to software. The compiler inserts NOPs or reorders independent instructions, reducing hardware complexity at the cost of larger code size."},
        {"question": "What addressing mode automatically increments or decrements a register after using it?", "answer": "Auto-increment/Auto-decrement addressing", "alternatives": ["Post-increment addressing", "Auto-indexing"], "explanation": "Auto-increment/decrement addressing automatically modifies the register after (post) or before (pre) using it as an address. This is extremely efficient for traversing arrays or stacks, eliminating separate increment instructions. For example, 'LOAD (R1)+' loads from R1's address then increments R1."}
    ],
    "difficult": [
        {"question": "What ISA feature allows conditional execution of instructions without branching?", "answer": "Predicated execution", "alternatives": ["Conditional execution", "Predication"], "explanation": "Predicated (or conditional) execution allows instructions to be conditionally executed based on a predicate condition without branching. ARM and IA-64 support this extensively. Instead of 'if (cond) x=a; else x=b;' requiring branches, both assignments execute with one's result discarded based on the condition, eliminating branch misprediction penalties."},
        {"question": "What ISA design principle advocates that frequently used operations should execute faster?", "answer": "Common case optimization", "alternatives": ["Make the common case fast"], "explanation": "The 'make the common case fast' principle guides ISA and microarchitecture design. Since simple operations like loads, stores, and arithmetic dominate program execution, they should be fast even if it makes rare operations slower. This justifies RISC's simple instructions and hardware optimization for common patterns."},
        {"question": "What technique allows processors to expose parallelism explicitly through very long instruction words?", "answer": "VLIW (Very Long Instruction Word)", "alternatives": ["Very Long Instruction Word", "Explicitly Parallel Instruction Computing"], "explanation": "VLIW architectures package multiple operations into a single long instruction word, with the compiler scheduling operations for parallel execution. Unlike superscalar processors that discover parallelism dynamically, VLIW relies entirely on compiler analysis. Intel's Itanium used VLIW (called EPIC), but compiler limitations hindered commercial success."},
        {"question": "What ISA feature allows instructions to specify multiple operations bundled together?", "answer": "Instruction fusion", "alternatives": ["Macro-op fusion", "Op fusion"], "explanation": "Instruction fusion (or macro-op fusion) combines multiple simple instructions into a single complex operation at decode time. For example, compare-and-branch might fuse into one micro-op. This reduces pressure on execution resources and improves performance while maintaining ISA simplicity. Modern x86 processors extensively use fusion."},
        {"question": "What design philosophy motivated RISC development in opposition to increasingly complex CISC processors?", "answer": "ISA minimalism", "alternatives": ["Simplicity principle", "RISC philosophy"], "explanation": "RISC emerged from research showing that CISC complexity provided diminishing returns - most programs used simple instructions primarily. RISC philosophy advocates for simple, uniform instructions that execute quickly, moving complexity to the compiler. This enables higher clock speeds, simpler pipelines, and more efficient hardware implementation."},
        {"question": "What addressing mode allows position-independent code by using the program counter as a base?", "answer": "PC-relative addressing", "alternatives": ["Program counter relative", "IP-relative"], "explanation": "PC-relative addressing calculates addresses relative to the current program counter, enabling position-independent code that works regardless of where it's loaded in memory. This is crucial for shared libraries, relocatable code, and modern security features like ASLR. Branch instructions typically use PC-relative addressing."},
        {"question": "What ISA extension adds instructions specifically for accelerating cryptographic operations?", "answer": "AES-NI", "alternatives": ["AES New Instructions", "Crypto extensions"], "explanation": "AES-NI (Advanced Encryption Standard New Instructions) provides hardware acceleration for AES encryption/decryption, offering 3-10× performance improvement over software implementation. Similar extensions exist for other crypto operations (SHA, etc.). These specialized instructions demonstrate the trend of adding domain-specific accelerators to general-purpose ISAs."},
        {"question": "What technique allows ISAs to add new instruction without breaking compatibility using instruction prefixes or modes?", "answer": "ISA extensions", "alternatives": ["Instruction set extensions", "ISA evolution"], "explanation": "ISA extensions add new capabilities while maintaining backward compatibility through various mechanisms: using previously undefined opcodes, adding instruction prefixes (x86's 0x0F prefix), or mode bits. x86 evolved from 16-bit to 64-bit through extensions (MMX, SSE, AVX), maintaining compatibility across decades while gradually increasing complexity."},
        {"question": "What addressing mode chains multiple indirections to reach the final operand location?", "answer": "Multi-level indirect addressing", "alternatives": ["Cascaded indirect", "Pointer chain"], "explanation": "Multi-level indirect addressing follows a chain of pointers through memory to reach the operand. While conceptually simple (like following linked structures), hardware support is rare due to multiple memory accesses. It's typically implemented in software using multiple instructions, though some historical architectures provided hardware support."},
        {"question": "What ISA characteristic measures the degree to which instructions are orthogonal and composable?", "answer": "ISA regularity", "alternatives": ["Orthogonality", "Instruction uniformity"], "explanation": "ISA regularity (or orthogonality) means instructions work consistently across addressing modes, data types, and operands. Highly regular ISAs like RISC are easier to learn, compile for, and implement in hardware. In contrast, x86's irregularity (special cases, exceptions, asymmetric registers) complicates both hardware and software but evolved from historical constraints."}
    ]
}

# 4. Pipelining 
PIPELINING_QUESTIONS = {
    "easy": [
        {"question": "What is the technique of breaking instruction execution into stages like an assembly line?", "answer": "Pipelining", "alternatives": ["Instruction pipelining", "Pipeline"], "explanation": "Pipelining divides instruction execution into stages (fetch, decode, execute, memory, write-back), allowing multiple instructions to be processed simultaneously at different stages, like an assembly line. This increases throughput without reducing individual instruction latency."},
        {"question": "What is the term for a situation where the pipeline must wait because the next instruction depends on a previous one?", "answer": "Pipeline stall", "alternatives": ["Bubble", "Stall", "Pipeline delay"], "explanation": "A pipeline stall (or bubble) occurs when the pipeline must pause because an instruction cannot proceed to the next stage. This wastes clock cycles and reduces throughput. Common causes include data hazards, control hazards, and structural hazards."},
        {"question": "What is the first stage of a typical instruction pipeline that retrieves instructions from memory?", "answer": "Fetch", "alternatives": ["Instruction Fetch", "IF stage"], "explanation": "The Fetch stage retrieves the next instruction from memory using the program counter address. It's the first stage in a typical 5-stage pipeline (Fetch, Decode, Execute, Memory, Write-back). Efficient fetching is crucial since it feeds the entire pipeline."},
        {"question": "What type of hazard occurs when an instruction needs data that hasn't been computed yet?", "answer": "Data hazard", "alternatives": ["Data dependency"], "explanation": "Data hazards occur when an instruction depends on the result of a previous instruction still in the pipeline. For example, if instruction 2 needs the result of instruction 1, but instruction 1 hasn't reached the write-back stage yet, a data hazard exists."},
        {"question": "What is the term for the wasted time in a pipeline due to hazards or dependencies?", "answer": "Pipeline overhead", "alternatives": ["Stall cycles", "Bubble cycles"], "explanation": "Pipeline overhead refers to wasted clock cycles where pipeline stages sit idle due to hazards, preventing the pipeline from achieving its theoretical maximum throughput. Minimizing overhead through hazard detection and resolution is crucial for pipeline efficiency."},
        {"question": "In a 5-stage pipeline, what is the stage that interprets the instruction and reads operands?", "answer": "Decode", "alternatives": ["Instruction Decode", "ID stage"], "explanation": "The Decode stage interprets the instruction opcode, identifies required operands, and reads values from registers. It's the second stage in a classic 5-stage RISC pipeline and prepares everything needed for the Execute stage."},
        {"question": "What is the maximum number of instructions that can be simultaneously in different stages of an N-stage pipeline?", "answer": "N", "alternatives": ["N instructions"], "explanation": "An N-stage pipeline can have at most N instructions in flight simultaneously, one per stage. For example, a 5-stage pipeline can process 5 instructions at once. This is the key to pipelining's performance improvement - overlapping execution of multiple instructions."},
        {"question": "What technique inserts wasted cycles (NOPs) to resolve pipeline hazards?", "answer": "Pipeline stalling", "alternatives": ["Inserting bubbles", "Adding NOPs"], "explanation": "Pipeline stalling intentionally inserts empty cycles (bubbles or NOPs) to delay dependent instructions until their operands are ready. While simple to implement, stalling wastes cycles and reduces pipeline efficiency. More sophisticated techniques like forwarding can often avoid stalls."},
        {"question": "What is the term for when two instructions try to use the same hardware resource simultaneously?", "answer": "Structural hazard", "alternatives": ["Resource conflict"], "explanation": "Structural hazards occur when hardware resources can't support all concurrent operations in a pipeline. For example, if there's only one memory port but both fetch and memory stages need to access memory simultaneously, a structural hazard exists. Solutions include adding more resources or scheduling to avoid conflicts."},
        {"question": "What pipeline stage performs arithmetic, logical, and comparison operations?", "answer": "Execute", "alternatives": ["EX stage", "Execution stage"], "explanation": "The Execute (EX) stage performs the actual operation specified by the instruction: arithmetic (add, subtract), logical (AND, OR), shifts, or comparisons. It's the third stage in a classic 5-stage pipeline and is where the ALU does its work."}
    ],
    "average": [
        {"question": "What technique bypasses the write-back stage to directly forward results to dependent instructions?", "answer": "Data forwarding", "alternatives": ["Bypassing", "Short-circuiting"], "explanation": "Data forwarding (or bypassing) provides results directly from one pipeline stage to another without waiting for write-back to registers. For example, the ALU output can be forwarded immediately to a dependent instruction's ALU input, avoiding stalls. This is a key technique for maintaining pipeline efficiency."},
        {"question": "What type of hazard occurs when pipeline decisions depend on branch outcomes not yet determined?", "answer": "Control hazard", "alternatives": ["Branch hazard"], "explanation": "Control hazards (or branch hazards) occur when the pipeline must make decisions before knowing whether a branch will be taken. The pipeline doesn't know which instruction to fetch next until the branch condition is evaluated, potentially several stages later, causing pipeline stalls or requiring speculative execution."},
        {"question": "What is the technique where the compiler rearranges instructions to minimize pipeline stalls?", "answer": "Instruction scheduling", "alternatives": ["Code scheduling", "Compiler scheduling"], "explanation": "Instruction scheduling is a compiler optimization that reorders independent instructions to minimize pipeline stalls. The compiler analyzes data dependencies and arranges instructions to maximize pipeline utilization, filling potential stall cycles with useful work. This is especially important for statically scheduled RISC processors."},
        {"question": "What is the RAW (Read After Write) hazard where an instruction needs data before it's written?", "answer": "True dependency", "alternatives": ["Data dependency", "RAW hazard"], "explanation": "A RAW (Read After Write) hazard is a true data dependency where an instruction must read a value that a previous instruction will write. For example: 'ADD R1, R2, R3' followed by 'SUB R4, R1, R5' has a RAW hazard on R1. The second instruction must wait for R1 to be written. Forwarding can often resolve RAW hazards."},
        {"question": "What technique speculatively executes instructions from both branch paths simultaneously?", "answer": "Multipath execution", "alternatives": ["Eager execution"], "explanation": "Multipath (or eager) execution speculatively executes both possible branch paths simultaneously, keeping both results until the branch resolves. While resource-intensive, this eliminates branch misprediction penalties entirely. It's practical only for short instruction sequences due to resource constraints."},
        {"question": "What is the maximum theoretical speedup achievable by an N-stage pipeline compared to unpipelined execution?", "answer": "N", "alternatives": ["N times", "Linear speedup"], "explanation": "The theoretical maximum speedup of an N-stage pipeline is N times the unpipelined performance, assuming perfect conditions (no hazards, balanced stages). In practice, hazards, unbalanced stages, and overhead reduce actual speedup. For example, a 5-stage pipeline theoretically provides 5× speedup."},
        {"question": "What WAW (Write After Write) hazard occurs when instructions write to the same register out of order?", "answer": "Output dependency", "alternatives": ["WAW hazard"], "explanation": "A WAW (Write After Write) hazard is an output dependency where two instructions write to the same register, and they might complete out of order. For example, if instruction 2 writes R1 before instruction 1 (which also writes R1), the final value would be incorrect. Register renaming typically resolves WAW hazards."},
        {"question": "What is the term for the time between injecting consecutive instructions into the pipeline?", "answer": "Pipeline initiation interval", "alternatives": ["Throughput rate"], "explanation": "The pipeline initiation interval (or issue rate) is the time between starting consecutive instructions. In an ideal pipeline, it's one clock cycle - a new instruction enters every cycle. The reciprocal gives the throughput. Hazards increase the initiation interval, reducing throughput."},
        {"question": "What technique delays branch resolution while continuing to execute subsequent instructions?", "answer": "Delayed branching", "alternatives": ["Branch delay slot"], "explanation": "Delayed branching exposes the branch delay to the ISA - the instruction(s) immediately following a branch execute regardless of the branch outcome. The compiler fills these 'delay slots' with useful instructions from before the branch or makes them NOPs. This was common in early RISC processors (MIPS, SPARC)."},
        {"question": "What is the WAR (Write After Read) hazard that occurs only with out-of-order execution?", "answer": "Anti-dependency", "alternatives": ["WAR hazard"], "explanation": "A WAR (Write After Read) anti-dependency occurs when an instruction writes a value before a previous instruction reads the old value. This only causes problems with out-of-order execution since in-order pipelines naturally avoid it. Register renaming eliminates WAR hazards by giving each write a unique destination."}
    ],
    "difficult": [
        {"question": "What advanced technique allows a pipeline to continue execution past conditional branches before the condition is known?", "answer": "Speculative execution", "alternatives": ["Branch speculation"], "explanation": "Speculative execution allows the pipeline to continue beyond branches by guessing the outcome and executing along the predicted path. If the prediction is correct, performance improves; if wrong, the speculatively executed work is discarded and the pipeline restarts from the correct path. This trades occasional penalties for usually avoiding stalls."},
        {"question": "What hardware structure tracks which pipeline stages have produced results available for forwarding?", "answer": "Scoreboard", "alternatives": ["Hazard detection unit"], "explanation": "A scoreboard is a hardware table that tracks register availability and pending operations, enabling hazard detection and resolution. It monitors which instructions are using which registers and controls forwarding and stalling. Scoreboards are central to Tomasulo's algorithm and other dynamic scheduling schemes."},
        {"question": "What is the phenomenon where deeper pipelines provide diminishing returns and eventually hurt performance?", "answer": "Pipeline depth limitation", "alternatives": ["Deep pipeline penalty"], "explanation": "Beyond a certain depth, adding pipeline stages hurts performance due to increased branch misprediction penalties, forwarding complexity, and overhead. Each added stage reduces per-stage work but increases pipeline latency, making branch mispredictions more costly. Modern designs balance depth (typically 10-20 stages) against these factors."},
        {"question": "What technique uses multiple parallel pipelines to issue multiple instructions per clock cycle?", "answer": "Superscalar execution", "alternatives": ["Multiple issue", "Superscalar"], "explanation": "Superscalar processors have multiple parallel pipelines, allowing them to fetch, decode, and execute multiple instructions per cycle. A 4-wide superscalar can theoretically complete 4 instructions per clock. This requires significant hardware for dependency checking, multiple execution units, and sophisticated scheduling."},
        {"question": "What is the latency penalty for a branch misprediction in a pipeline with N stages between fetch and execute?", "answer": "N cycles", "alternatives": ["N clock cycles"], "explanation": "When a branch mispredicts in an N-stage pipeline, all speculatively fetched instructions (up to N-1) must be flushed, wasting N cycles before correct-path instructions enter execution. This is why deep pipelines are vulnerable to control hazards and why accurate branch prediction is crucial for performance."},
        {"question": "What technique saves pipeline state at branch points to enable rapid recovery from mispredictions?", "answer": "Checkpoint recovery", "alternatives": ["State checkpointing"], "explanation": "Checkpoint recovery saves the processor state (registers, flags) at branch points. On misprediction, instead of flushing the entire pipeline and re-fetching instructions, the checkpoint is restored, reducing recovery time. This is especially valuable in deep pipelines where misprediction penalties are severe."},
        {"question": "What hardware structure allows out-of-order execution while maintaining in-order commit?", "answer": "Reorder buffer", "alternatives": ["ROB", "Completion buffer"], "explanation": "The reorder buffer (ROB) is a circular buffer that holds instructions in program order from dispatch until retirement. Instructions execute out-of-order but commit in-order from the ROB, maintaining precise exceptions and architectural state. The ROB enables aggressive out-of-order execution while preserving sequential semantics."},
        {"question": "What advanced scheduling technique issues instructions to reservation stations that execute when operands arrive?", "answer": "Tomasulo's algorithm", "alternatives": ["Tomasulo scheduling"], "explanation": "Tomasulo's algorithm uses reservation stations to track instructions waiting for operands. When results are produced, they're broadcast to all stations; ready instructions execute immediately. This enables out-of-order execution, register renaming, and dynamic scheduling without compiler support. Tomasulo's algorithm was pioneering in enabling high-performance out-of-order processors."},
        {"question": "What is the technique of executing multiple iterations of a loop in parallel across pipeline stages?", "answer": "Software pipelining", "alternatives": ["Loop pipelining", "Modulo scheduling"], "explanation": "Software pipelining (or modulo scheduling) rearranges loop iterations so different iterations execute in parallel pipeline stages. Instead of completing iteration N before starting N+1, operations from multiple iterations overlap. This is like loop unrolling but maintains compact code while exposing parallelism. It requires sophisticated compiler analysis of loop-carried dependencies."},
        {"question": "What phenomenon causes performance degradation when pipeline stages have significantly different execution times?", "answer": "Pipeline imbalance", "alternatives": ["Stage imbalance"], "explanation": "Pipeline imbalance occurs when stages have unequal delays, causing the slowest stage to limit overall throughput (the pipeline is only as fast as its slowest stage). Ideally, all stages should take equal time. Imbalance wastes potential performance since faster stages sit idle waiting for slower ones. Careful pipeline design aims to balance stage complexity."}
    ]
}

# Continue with remaining subtopics...
# This file can be imported by question_generator.py to extend the database

ALL_ADDITIONAL_QUESTIONS = {
    'instruction_set': INSTRUCTION_SET_QUESTIONS,
    'pipelining': PIPELINING_QUESTIONS
}

# 5. Parallel Processing
PARALLEL_PROCESSING_QUESTIONS = {
    "easy": [
        {"question": "What term describes performing multiple operations or tasks simultaneously?", "answer": "Parallelism", "alternatives": ["Parallel processing", "Concurrent processing"], "explanation": "Parallelism involves executing multiple operations simultaneously to improve performance. This can occur at different levels: instruction-level (executing multiple instructions at once), thread-level (running multiple threads), or task-level (distributing work across multiple processors)."},
        {"question": "What type of parallel processing has multiple processing units with shared memory?", "answer": "Multiprocessor", "alternatives": ["Shared memory multiprocessor", "SMP"], "explanation": "Multiprocessor systems have multiple CPUs sharing a common memory space. This allows different processors to access the same data directly, simplifying programming but requiring synchronization to prevent conflicts. Symmetric multiprocessors (SMP) are common in modern servers and workstations."},
        {"question": "What is the term for running multiple instructions from a single instruction stream in parallel?", "answer": "SIMD (Single Instruction Multiple Data)", "alternatives": ["Vector processing"], "explanation": "SIMD executes the same operation on multiple data elements simultaneously. One instruction operates on vectors of data rather than single values. This is efficient for data-parallel workloads like image processing, where the same operation applies to many pixels."},
        {"question": "What type of parallelism exists when multiple processors execute different programs on different data?", "answer": "MIMD (Multiple Instruction Multiple Data)", "alternatives": ["Multiple Instruction Multiple Data"], "explanation": "MIMD systems have multiple processors independently executing different instructions on different data. This is the most flexible parallel architecture, supporting both data and task parallelism. Modern multicore processors and computer clusters are MIMD systems."},
        {"question": "What is the term for dividing a large problem into smaller sub-problems that can be solved in parallel?", "answer": "Decomposition", "alternatives": ["Problem decomposition", "Parallelization"], "explanation": "Decomposition breaks a large problem into independent sub-problems that can execute in parallel. Effective decomposition is key to parallel programming success, requiring identification of independent computations and managing dependencies between sub-problems."},
        {"question": "What hardware feature allows a CPU to have multiple independent processing cores on a single chip?", "answer": "Multicore", "alternatives": ["Multi-core processor", "Multicore architecture"], "explanation": "Multicore processors integrate multiple complete CPU cores on a single chip, each capable of independent execution. This provides genuine parallelism within a single package, improving performance for multi-threaded applications while managing power consumption better than increasing clock speeds."},
        {"question": "What is the term for the overhead involved in distributing work among parallel processors?", "answer": "Parallelization overhead", "alternatives": ["Parallel overhead", "Coordination cost"], "explanation": "Parallelization overhead includes time spent distributing work, synchronizing processors, communicating between them, and combining results. This overhead can limit speedup - if it's too high, parallel execution might be slower than sequential. Efficient parallel algorithms minimize this overhead."},
        {"question": "What law states that speedup is limited by the sequential portion of a program?", "answer": "Amdahl's Law", "alternatives": ["Amdahl"], "explanation": "Amdahl's Law states that the maximum speedup of a program is limited by its sequential fraction. Even with infinite processors, if 10% of the program must execute sequentially, maximum speedup is 10×. This highlights the importance of maximizing the parallel portion of programs."},
        {"question": "What term describes multiple processors working on a single shared task?", "answer": "Data parallelism", "alternatives": ["Data-parallel processing"], "explanation": "Data parallelism divides data among processors, with each processor performing the same operation on different data subsets. This is effective for problems with large data sets where the same operation applies to all elements, like matrix operations or image filtering."},
        {"question": "What is the maximum theoretical speedup when using N processors?", "answer": "N", "alternatives": ["Linear speedup", "N times"], "explanation": "The theoretical maximum speedup with N processors is N× (linear speedup), meaning the parallel version runs N times faster than serial execution. However, real speedup is usually less due to parallelization overhead, sequential portions (Amdahl's Law), and communication costs."}
    ],
    "average": [
        {"question": "What type of memory architecture has each processor with its own local memory?", "answer": "Distributed memory", "alternatives": ["Distributed shared memory"], "explanation": "In distributed memory systems, each processor has its own local memory, and processors communicate by passing messages. This scales better than shared memory for large systems but requires explicit communication programming. Computer clusters and supercomputers typically use distributed memory."},
        {"question": "What synchronization primitive ensures mutual exclusion for critical sections?", "answer": "Lock", "alternatives": ["Mutex", "Mutual exclusion lock"], "explanation": "Locks (or mutexes) provide mutual exclusion, ensuring only one thread accesses a critical section at a time. Before entering protected code, a thread acquires the lock; afterward, it releases it. This prevents race conditions but requires careful programming to avoid deadlocks."},
        {"question": "What is the phenomenon where adding more processors provides diminishing returns?", "answer": "Scalability limitation", "alternatives": ["Parallel efficiency degradation"], "explanation": "As more processors are added, efficiency typically decreases due to increased communication overhead, synchronization costs, and load imbalance. This is why real speedup is sublinear - doubling processors doesn't double performance. Strong scaling (fixed problem size) hits limits faster than weak scaling (problem size grows with processors)."},
        {"question": "What parallel programming model passes messages between independent processes?", "answer": "Message passing", "alternatives": ["MPI (Message Passing Interface)"], "explanation": "Message passing is a parallel programming model where processes communicate by explicitly sending and receiving messages. Each process has its own memory space, and data sharing requires explicit communication. MPI (Message Passing Interface) is the standard API for message passing in HPC."},
        {"question": "What is the term for uneven distribution of work among processors?", "answer": "Load imbalance", "alternatives": ["Work imbalance"], "explanation": "Load imbalance occurs when some processors have more work than others, causing them to finish at different times. This wastes processor cycles as some sit idle waiting for others. Effective load balancing, either static or dynamic, distributes work evenly to maximize utilization and minimize total execution time."},
        {"question": "What technique allows threads to share memory while executing independently?", "answer": "Shared memory multithreading", "alternatives": ["Thread-level parallelism"], "explanation": "Shared memory multithreading allows multiple threads within a process to share memory while executing independently on different cores. This simplifies data sharing compared to message passing but requires synchronization (locks, barriers) to prevent race conditions. OpenMP is a popular shared-memory parallel programming API."},
        {"question": "What barrier synchronization point ensures all threads reach a certain point before any can proceed?", "answer": "Barrier", "alternatives": ["Synchronization barrier"], "explanation": "A barrier is a synchronization point where threads must wait until all threads reach it before any can proceed past it. This ensures all threads complete one phase before starting the next. Barriers are useful for algorithms with distinct phases that depend on all processors completing previous work."},
        {"question": "What metric measures the fraction of time processors spend doing useful work versus waiting?", "answer": "Parallel efficiency", "alternatives": ["Utilization"], "explanation": "Parallel efficiency is the speedup divided by the number of processors, indicating how effectively processors are utilized. Perfect efficiency (100%) means linear speedup; lower efficiency indicates overhead, load imbalance, or insufficient parallelism. Maintaining high efficiency as processor count increases is a major challenge."},
        {"question": "What technique duplicates data across processors to reduce communication?", "answer": "Data replication", "alternatives": ["Redundant storage"], "explanation": "Data replication stores copies of data on multiple processors, reducing the need for remote access and communication. The tradeoff is increased memory usage and the need to maintain consistency if data is modified. Replication is effective for read-mostly data shared across many processors."},
        {"question": "What architecture connects processors in a grid where each can communicate with neighbors?", "answer": "Mesh topology", "alternatives": ["2D mesh", "Grid network"], "explanation": "Mesh topology arranges processors in a 2D (or 3D) grid where each processor connects to its neighbors. This provides good locality for many algorithms and scales well, though communication between distant processors requires multiple hops. Many-core processors and NoC (Network-on-Chip) designs use mesh topologies."}
    ],
    "difficult": [
        {"question": "What consistency model guarantees that operations appear to execute atomically in some sequential order?", "answer": "Sequential consistency", "alternatives": ["Sequential memory consistency"], "explanation": "Sequential consistency guarantees that the result of parallel execution is equivalent to some sequential interleaving of operations from all processors, with operations from each processor in program order. This is intuitive but expensive to implement, requiring synchronization that can hurt performance. Many systems use weaker models."},
        {"question": "What phenomenon causes performance degradation when multiple cores access the same cache line?", "answer": "False sharing", "alternatives": ["Cache line contention"], "explanation": "False sharing occurs when threads on different cores modify different variables that happen to reside in the same cache line. Even though there's no true data sharing, the cache coherence protocol causes invalidations and performance degradation. Padding variables to different cache lines avoids this."},
        {"question": "What lock-free synchronization technique allows threads to retry operations if conflicts occur?", "answer": "Optimistic concurrency", "alternatives": ["Transactional memory", "Compare-and-swap"], "explanation": "Optimistic concurrency assumes conflicts are rare and allows threads to proceed without locking. If a conflict is detected (via compare-and-swap or similar atomics), the operation retries. This can outperform locks when contention is low but may waste work under high contention. Software transactional memory is an advanced form."},
        {"question": "What technique overlaps computation with communication to hide latency?", "answer": "Latency hiding", "alternatives": ["Communication-computation overlap"], "explanation": "Latency hiding initiates communication asynchronously and performs other computation while waiting for data to arrive. This overlaps communication latency with useful work, improving efficiency. Pre-fetching and non-blocking communication primitives enable latency hiding, crucial for distributed memory systems where communication is expensive."},
        {"question": "What synchronization mechanism allows producers and consumers to communicate through a fixed-size buffer?", "answer": "Bounded buffer", "alternatives": ["Producer-consumer queue", "Ring buffer"], "explanation": "A bounded buffer (or circular buffer) is a fixed-size queue for producer-consumer communication. Producers add items; consumers remove them. Synchronization ensures producers wait when full and consumers wait when empty. This decouples producers and consumers, allowing them to run at different rates, improving parallelism."},
        {"question": "What advanced processor feature allows cores to temporarily use another core's resources?", "answer": "Resource sharing", "alternatives": ["Dynamic resource allocation"], "explanation": "Modern processors allow resource sharing where idle execution units from one core can be used by another core. This is beyond simple SMT - it's dynamic reallocation of functional units, cache capacity, or bandwidth based on workload needs. This improves utilization but adds complexity to resource management."},
        {"question": "What technique partitions data so that communication only occurs at partition boundaries?", "answer": "Domain decomposition", "alternatives": ["Spatial decomposition"], "explanation": "Domain decomposition partitions the problem domain (e.g., physical space in simulations) across processors. Each processor works on its partition, communicating only with neighbors at boundaries. This minimizes communication volume and is fundamental to parallel scientific computing, enabling massive-scale simulations."},
        {"question": "What memory consistency model only guarantees ordering for synchronization operations?", "answer": "Weak consistency", "alternatives": ["Weak ordering"], "explanation": "Weak consistency models relax ordering guarantees, requiring synchronization only at explicit synchronization points. Ordinary loads and stores can be reordered freely; only synchronization operations (like barriers or atomic operations) enforce ordering. This allows aggressive optimizations but requires careful programming. Most modern systems use weak models."},
        {"question": "What technique adjusts the number of threads dynamically based on available resources?", "answer": "Dynamic parallelism", "alternatives": ["Adaptive parallelism"], "explanation": "Dynamic parallelism adjusts thread count at runtime based on workload characteristics and available resources. For example, reducing threads when other applications compete for cores, or increasing threads for larger problem sizes. This adapts to varying conditions but adds runtime overhead for thread management."},
        {"question": "What protocol coordinates cache coherence in multiprocessor systems?", "answer": "Cache coherence protocol", "alternatives": ["MESI", "Snooping protocol", "Directory protocol"], "explanation": "Cache coherence protocols ensure all processors see a consistent view of memory when data is cached in multiple places. Snooping protocols (like MESI) broadcast cache operations; directory protocols maintain a central directory of cache contents. Coherence is essential for shared-memory multiprocessors but adds overhead that limits scalability."}
    ]
}

# Update the dictionary at the end
ALL_ADDITIONAL_QUESTIONS['parallel_processing'] = PARALLEL_PROCESSING_QUESTIONS

# 6. Cryptography
CRYPTOGRAPHY_QUESTIONS = {
    "easy": [
        {"question": "What is the practice of securing information by transforming it into an unreadable format?", "answer": "Encryption", "alternatives": ["Cryptographic encryption"], "explanation": "Encryption transforms plaintext (readable data) into ciphertext (unreadable data) using an algorithm and a key. Only those with the correct key can decrypt the ciphertext back to plaintext. Encryption protects data confidentiality during storage and transmission."},
        {"question": "What type of encryption uses the same key for both encryption and decryption?", "answer": "Symmetric encryption", "alternatives": ["Secret key encryption", "Symmetric key cryptography"], "explanation": "Symmetric encryption uses a single shared key for both encryption and decryption. It's fast and efficient for large data volumes but requires secure key distribution. Examples include AES, DES, and ChaCha20. The main challenge is securely sharing the key between parties."},
        {"question": "What encryption method uses two different but mathematically related keys?", "answer": "Asymmetric encryption", "alternatives": ["Public key encryption", "Public key cryptography"], "explanation": "Asymmetric encryption uses a key pair: a public key (shared openly) for encryption and a private key (kept secret) for decryption. Anyone can encrypt with the public key, but only the private key holder can decrypt. RSA and ECC are common asymmetric algorithms."},
        {"question": "What fixed-size value represents data that has been processed through a hash function?", "answer": "Hash", "alternatives": ["Hash value", "Digest", "Checksum"], "explanation": "A hash (or digest) is a fixed-size output from a hash function that uniquely represents input data. Good hash functions are one-way (can't reverse to find input) and collision-resistant (different inputs produce different hashes). Common uses include password storage and data integrity verification."},
        {"question": "What key in public key cryptography can be freely shared with anyone?", "answer": "Public key", "alternatives": ["Public encryption key"], "explanation": "The public key in asymmetric cryptography can be freely distributed to anyone. Data encrypted with a public key can only be decrypted with the corresponding private key. This solves the key distribution problem of symmetric encryption and enables applications like secure email and HTTPS."},
        {"question": "What is the process of converting ciphertext back into readable plaintext?", "answer": "Decryption", "alternatives": ["Deciphering"], "explanation": "Decryption is the reverse of encryption - converting ciphertext back to its original plaintext form using the appropriate key and algorithm. In symmetric encryption, the same key encrypts and decrypts. In asymmetric encryption, the private key decrypts data encrypted with the public key."},
        {"question": "What technique protects passwords by storing only their hash values?", "answer": "Password hashing", "alternatives": ["Cryptographic hashing"], "explanation": "Password hashing stores only the hash of passwords, not the passwords themselves. When users log in, their entered password is hashed and compared to the stored hash. Even if the database is compromised, attackers only get hashes, not actual passwords. Salt (random data added before hashing) prevents rainbow table attacks."},
        {"question": "What cryptographic technique combines symmetric and asymmetric encryption for efficiency?", "answer": "Hybrid encryption", "alternatives": ["Hybrid cryptography"], "explanation": "Hybrid encryption uses asymmetric encryption to exchange a symmetric key, then uses that symmetric key for bulk data encryption. This combines the security of public key cryptography with the speed of symmetric encryption. HTTPS/TLS uses hybrid encryption."},
        {"question": "What algorithm is the current standard for symmetric encryption approved by NIST?", "answer": "AES", "alternatives": ["Advanced Encryption Standard", "AES encryption"], "explanation": "AES (Advanced Encryption Standard) is the current standard for symmetric encryption, adopted by NIST in 2001. It replaced DES and offers 128, 192, or 256-bit key sizes. AES is fast, secure, and widely implemented in hardware and software, used in everything from file encryption to VPNs."},
        {"question": "What is the secret piece of information used to encrypt or decrypt data?", "answer": "Key", "alternatives": ["Encryption key", "Cryptographic key"], "explanation": "A key is secret information (typically a string of bits) used by cryptographic algorithms to encrypt and decrypt data. Key strength (length and randomness) determines encryption security. Longer keys provide more security but may require more computation. Key management (generation, distribution, storage) is critical for system security."}
    ],
    "average": [
        {"question": "What attack tries all possible keys until finding the correct one?", "answer": "Brute force attack", "alternatives": ["Exhaustive key search"], "explanation": "A brute force attack systematically tries every possible key until finding one that decrypts the data correctly. Modern encryption uses key sizes large enough to make brute force computationally infeasible - AES-256 would require astronomical time even with all world's computing power. This is why key length is crucial for security."},
        {"question": "What cryptographic primitive ensures data has not been modified?", "answer": "Message Authentication Code", "alternatives": ["MAC", "HMAC"], "explanation": "A Message Authentication Code (MAC) is a tag computed from a message and a secret key, ensuring both integrity (data hasn't been modified) and authenticity (message came from someone with the key). HMAC (Hash-based MAC) is a common construction using hash functions. MACs detect tampering and forgery."},
        {"question": "What method adds random data to passwords before hashing to prevent rainbow table attacks?", "answer": "Salting", "alternatives": ["Salt"], "explanation": "Salting adds random data (salt) to each password before hashing. Even identical passwords have different hashes due to unique salts. This defeats precomputed rainbow tables since attackers must recompute hashes for each salt. Salt is stored with the hash and doesn't need to be secret, just unique and random."},
        {"question": "What public key cryptography algorithm is based on the difficulty of factoring large numbers?", "answer": "RSA", "alternatives": ["RSA encryption"], "explanation": "RSA (Rivest-Shamir-Adleman) is based on the difficulty of factoring the product of two large prime numbers. It's widely used for secure data transmission and digital signatures. While secure with sufficient key size (2048+ bits), RSA is computationally expensive compared to symmetric encryption, hence its use in hybrid schemes."},
        {"question": "What cryptographic technique allows verification of a message's sender without revealing their identity?", "answer": "Digital signature", "alternatives": ["Digital signing"], "explanation": "Digital signatures use asymmetric cryptography to verify message authenticity and integrity. The sender signs with their private key; anyone with the public key can verify. Unlike MACs, signatures provide non-repudiation - the signer can't deny creating the signature. Digital signatures are fundamental to PKI and secure communications."},
        {"question": "What attack exploits weaknesses in cryptographic implementation rather than the algorithm itself?", "answer": "Side-channel attack", "alternatives": ["Timing attack"], "explanation": "Side-channel attacks exploit information leaked during cryptographic operations - timing variations, power consumption, electromagnetic emissions, or acoustic signals. For example, timing attacks measure how long decryption takes to deduce information about the key. Constant-time implementations and other countermeasures defend against side-channels."},
        {"question": "What protocol establishes secure communication by negotiating encryption parameters?", "answer": "Key exchange protocol", "alternatives": ["Key agreement"], "explanation": "Key exchange protocols allow two parties to establish a shared secret key over an insecure channel. Diffie-Hellman is the classic example. Even if an attacker observes all communication, they can't determine the shared key. Key exchange is fundamental to protocols like TLS/SSL."},
        {"question": "What cryptographic concept ensures a sender cannot deny sending a message?", "answer": "Non-repudiation", "alternatives": ["Non-denial"], "explanation": "Non-repudiation prevents a sender from denying they sent a message. Digital signatures provide non-repudiation - the signature can only be created with the private key, proving the signer had that key. This is crucial for legal and financial applications where proof of origin is required."},
        {"question": "What type of cipher encrypts data one bit or byte at a time in a continuous stream?", "answer": "Stream cipher", "alternatives": ["Streaming encryption"], "explanation": "Stream ciphers encrypt data as a continuous stream, typically one byte or bit at a time using a keystream. They're fast and have no padding requirements, ideal for real-time communication. ChaCha20 and RC4 (now deprecated) are stream ciphers. They contrast with block ciphers that process fixed-size blocks."},
        {"question": "What infrastructure manages public keys and digital certificates?", "answer": "PKI (Public Key Infrastructure)", "alternatives": ["Public Key Infrastructure"], "explanation": "PKI is a framework for managing public key cryptography, including certificate authorities (CAs), registration authorities, and certificate repositories. PKI enables secure communication between parties who've never met by having trusted CAs vouch for key ownership through digital certificates. The web's HTTPS relies on PKI."}
    ],
    "difficult": [
        {"question": "What advanced cryptographic technique allows computations on encrypted data without decrypting it?", "answer": "Homomorphic encryption", "alternatives": ["FHE", "Fully homomorphic encryption"], "explanation": "Homomorphic encryption allows operations on encrypted data that produce encrypted results matching operations on plaintext. Fully homomorphic encryption (FHE) supports arbitrary computations, enabling cloud computing on sensitive data without exposing it. While theoretically revolutionary, FHE remains computationally expensive despite recent advances."},
        {"question": "What attack recovers plaintext by finding two inputs that produce the same hash?", "answer": "Collision attack", "alternatives": ["Hash collision"], "explanation": "Collision attacks find two different inputs producing the same hash output. If successful against a cryptographic hash, this breaks integrity guarantees. MD5 and SHA-1 have known collision attacks and are deprecated. Modern hashes like SHA-256 and SHA-3 resist collision attacks with sufficient output size."},
        {"question": "What cryptographic mode combines encryption with authentication in a single operation?", "answer": "Authenticated encryption", "alternatives": ["AEAD", "Authenticated Encryption with Associated Data", "GCM"], "explanation": "Authenticated Encryption with Associated Data (AEAD) provides both confidentiality and authenticity in one operation. GCM (Galois/Counter Mode) and ChaCha20-Poly1305 are popular AEAD modes. They're more efficient and secure than combining separate encryption and MAC operations, preventing subtle implementation vulnerabilities like padding oracle attacks."},
        {"question": "What mathematical problem underlies elliptic curve cryptography's security?", "answer": "Discrete logarithm problem", "alternatives": ["Elliptic curve discrete logarithm", "ECDLP"], "explanation": "Elliptic Curve Cryptography (ECC) security relies on the difficulty of the elliptic curve discrete logarithm problem. ECC achieves equivalent security to RSA with much shorter keys - 256-bit ECC equals 3072-bit RSA. This makes ECC efficient for resource-constrained environments like mobile devices and IoT, explaining its growing adoption."},
        {"question": "What cryptographic technique allows proving knowledge of a secret without revealing the secret itself?", "answer": "Zero-knowledge proof", "alternatives": ["ZKP", "Zero-knowledge protocol"], "explanation": "Zero-knowledge proofs allow one party (prover) to convince another (verifier) they know a secret without revealing the secret itself. This seems paradoxical but is mathematically rigorous. Applications include privacy-preserving authentication and cryptocurrencies. zk-SNARKs and zk-STARKs are advanced zero-knowledge constructions."},
        {"question": "What attack exploits mathematical relationships between plaintext, ciphertext, and keys?", "answer": "Cryptanalysis", "alternatives": ["Cryptanalytic attack"], "explanation": "Cryptanalysis is the science of breaking cryptographic systems through mathematical analysis rather than brute force. Techniques include differential cryptanalysis (analyzing how differences in input affect output), linear cryptanalysis (finding linear approximations), and algebraic attacks. Modern ciphers are designed to resist known cryptanalytic attacks."},
        {"question": "What protocol allows two parties to agree on a shared secret without prior communication?", "answer": "Diffie-Hellman key exchange", "alternatives": ["Diffie-Hellman", "DH key exchange"], "explanation": "Diffie-Hellman enables two parties to establish a shared secret over an insecure channel with no prior shared secrets. Its security relies on the discrete logarithm problem. While not providing authentication (vulnerable to man-in-the-middle without additional measures), DH is fundamental to TLS and VPNs."},
        {"question": "What quantum computing algorithm threatens current public key cryptography?", "answer": "Shor's algorithm", "alternatives": ["Shor"], "explanation": "Shor's algorithm allows quantum computers to factor large numbers and solve discrete logarithms efficiently, breaking RSA and ECC. While large-scale quantum computers don't yet exist, their potential has driven development of post-quantum cryptography - algorithms resistant to both classical and quantum attacks, like lattice-based and hash-based schemes."},
        {"question": "What technique derives multiple keys from a single master secret?", "answer": "Key derivation function", "alternatives": ["KDF"], "explanation": "Key Derivation Functions (KDFs) generate one or more cryptographic keys from a secret value like a password or master key. PBKDF2, bcrypt, scrypt, and Argon2 are KDFs designed for password hashing, using iteration and memory-hardness to slow brute force attacks. Other KDFs like HKDF expand keys for protocol use."},
        {"question": "What mode of operation turns a block cipher into a stream cipher using XOR with keystream?", "answer": "Counter mode", "alternatives": ["CTR mode", "CTR"], "explanation": "Counter (CTR) mode encrypts a counter value with the block cipher to generate a keystream, which is XORed with plaintext. This turns block ciphers like AES into stream ciphers, enabling parallel encryption/decryption and random access. CTR is faster and more flexible than CBC mode and is used in modern protocols like GCM."}
    ]
}

# Continue with additional_questions.py updates
ALL_ADDITIONAL_QUESTIONS['cryptography'] = CRYPTOGRAPHY_QUESTIONS
