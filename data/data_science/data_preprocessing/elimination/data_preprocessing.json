{
  "subtopic_id": "data_preprocessing",
  "subtopic_name": "Data Preprocessing",
  "mode": "elimination",
  "questions": [
    {
      "question": "What is the process of replacing missing values in a dataset?",
      "options": [
        "Data transformation",
        "Data imputation",
        "Data normalization",
        "Data encoding"
      ],
      "correct": 1,
      "explanation": "Data imputation fills in missing values using methods like mean, median, or forward fill.",
      "id": "dat_sci_dat_pre_elim_000"
    },
    {
      "question": "Which technique scales features to a range between 0 and 1?",
      "options": [
        "Standardization",
        "Log transformation",
        "Min-max normalization",
        "Binning"
      ],
      "correct": 2,
      "explanation": "Min-max normalization rescales data to a fixed range, typically 0 to 1.",
      "id": "dat_sci_dat_pre_elim_001"
    },
    {
      "question": "What method converts categorical variables into numerical format?",
      "options": [
        "Feature scaling",
        "Dimensionality reduction",
        "Outlier detection",
        "One-hot encoding"
      ],
      "correct": 3,
      "explanation": "One-hot encoding creates binary columns for each category value.",
      "id": "dat_sci_dat_pre_elim_002"
    },
    {
      "question": "What is the purpose of standardization?",
      "options": [
        "Transform data to have mean 0 and standard deviation 1",
        "Remove duplicate records",
        "Convert text to lowercase",
        "Fill missing values"
      ],
      "correct": 0,
      "explanation": "Standardization rescales data to have zero mean and unit variance.",
      "id": "dat_sci_dat_pre_elim_003"
    },
    {
      "question": "Which technique reduces the number of features while preserving information?",
      "options": [
        "Data cleaning",
        "Feature engineering",
        "Dimensionality reduction",
        "Data validation"
      ],
      "correct": 2,
      "explanation": "Dimensionality reduction techniques like PCA reduce features while retaining variance.",
      "id": "dat_sci_dat_pre_elim_004"
    },
    {
      "question": "What is the process of combining features to create new ones?",
      "options": [
        "Data sampling",
        "Feature engineering",
        "Data splitting",
        "Data aggregation"
      ],
      "correct": 1,
      "explanation": "Feature engineering creates new features from existing ones to improve model performance.",
      "id": "dat_sci_dat_pre_elim_005"
    },
    {
      "question": "Which method handles imbalanced datasets by creating synthetic samples?",
      "options": [
        "Undersampling",
        "Cross-validation",
        "SMOTE",
        "Bootstrapping"
      ],
      "correct": 2,
      "explanation": "SMOTE generates synthetic minority class samples to balance the dataset.",
      "id": "dat_sci_dat_pre_elim_006"
    },
    {
      "question": "What technique removes duplicate records from a dataset?",
      "options": [
        "Deduplication",
        "Normalization",
        "Discretization",
        "Aggregation"
      ],
      "correct": 0,
      "explanation": "Deduplication identifies and removes identical or near-identical records.",
      "id": "dat_sci_dat_pre_elim_007"
    },
    {
      "question": "Which transformation is used to handle skewed distributions?",
      "options": [
        "Z-score normalization",
        "Log transformation",
        "Label encoding",
        "Polynomial features"
      ],
      "correct": 1,
      "explanation": "Log transformation reduces skewness by compressing large values.",
      "id": "dat_sci_dat_pre_elim_008"
    },
    {
      "question": "What is the process of dividing continuous data into discrete bins?",
      "options": [
        "Encoding",
        "Scaling",
        "Binning",
        "Sampling"
      ],
      "correct": 2,
      "explanation": "Binning groups continuous values into discrete intervals or categories.",
      "id": "dat_sci_dat_pre_elim_009"
    }
  ]
}