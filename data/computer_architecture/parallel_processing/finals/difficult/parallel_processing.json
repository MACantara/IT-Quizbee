{
  "subtopic_id": "parallel_processing",
  "subtopic_name": "Parallel Processing",
  "mode": "finals",
  "difficulty": "difficult",
  "questions": [
    {
      "question": "What is NUMA architecture?",
      "answer": "Non-Uniform Memory Access",
      "alternatives": ["NUMA"],
      "explanation": "NUMA gives processors faster access to local memory than remote memory in multiprocessor systems."
    },
    {
      "question": "What is the false sharing problem?",
      "answer": "Threads using different data on same cache line",
      "alternatives": ["Cache line contention", "False cache sharing"],
      "explanation": "False sharing causes performance degradation when threads modify different variables on the same cache line."
    },
    {
      "question": "What is work stealing in parallel computing?",
      "answer": "Idle processors take tasks from busy ones",
      "alternatives": ["Task stealing", "Load redistribution"],
      "explanation": "Work stealing is a scheduling strategy where idle processors steal pending tasks from busy processors."
    },
    {
      "question": "What is memory fence or memory barrier?",
      "answer": "Instruction ensuring memory operation order",
      "alternatives": ["Memory barrier", "Fence instruction"],
      "explanation": "Memory fences enforce ordering constraints on memory operations across multiple processors."
    },
    {
      "question": "What is the PRAM model?",
      "answer": "Parallel Random Access Machine",
      "alternatives": ["PRAM"],
      "explanation": "PRAM is a theoretical model for analyzing parallel algorithms assuming simultaneous memory access."
    },
    {
      "question": "What is the difference between weak and strong scaling?",
      "answer": "Weak keeps work per processor constant, strong keeps total work constant",
      "alternatives": ["Scaling models"],
      "explanation": "Weak scaling fixes work per processor while strong scaling fixes total problem size as processors increase."
    },
    {
      "question": "What is dataflow architecture?",
      "answer": "Instructions execute when operands available",
      "alternatives": ["Data-driven execution"],
      "explanation": "Dataflow architectures execute instructions as soon as their input data becomes available."
    },
    {
      "question": "What is fine-grained parallelism?",
      "answer": "Small parallel tasks with frequent synchronization",
      "alternatives": ["Fine-grain parallelism"],
      "explanation": "Fine-grained parallelism divides computation into many small tasks requiring frequent communication."
    },
    {
      "question": "What is coarse-grained parallelism?",
      "answer": "Large parallel tasks with infrequent synchronization",
      "alternatives": ["Coarse-grain parallelism"],
      "explanation": "Coarse-grained parallelism uses larger independent tasks with minimal inter-task communication."
    },
    {
      "question": "What is transactional memory?",
      "answer": "Atomic execution of memory operation groups",
      "alternatives": ["TM", "Hardware transactional memory"],
      "explanation": "Transactional memory simplifies concurrent programming by treating groups of operations as atomic transactions."
    }
  ]
}