{
  "subtopic_id": "supervised_learning",
  "subtopic_name": "Supervised Learning",
  "mode": "finals",
  "difficulty": "average",
  "questions": [
    {
      "question": "What boosting algorithm builds trees sequentially?",
      "answer": "Gradient Boosting",
      "alternatives": [
        "Gradient boosting",
        "GBM"
      ],
      "explanation": "Gradient Boosting iteratively adds trees that correct previous errors.",
      "id": "sup_lea_sup_lea_a_finals_000"
    },
    {
      "question": "What metric represents the square root of MSE?",
      "answer": "RMSE",
      "alternatives": [
        "Root Mean Squared Error"
      ],
      "explanation": "RMSE provides error measurement in the same units as the target variable.",
      "id": "sup_lea_sup_lea_a_finals_001"
    },
    {
      "question": "What SVM parameter controls the margin-error tradeoff?",
      "answer": "C parameter",
      "alternatives": [
        "C",
        "Regularization parameter"
      ],
      "explanation": "The C parameter balances maximizing margin and minimizing errors.",
      "id": "sup_lea_sup_lea_a_finals_002"
    },
    {
      "question": "What technique transforms features for non-linear SVM?",
      "answer": "Kernel trick",
      "alternatives": [
        "Kernel method"
      ],
      "explanation": "The kernel trick maps data to higher dimensions without explicit computation.",
      "id": "sup_lea_sup_lea_a_finals_003"
    },
    {
      "question": "What metric measures absolute average prediction error?",
      "answer": "MAE",
      "alternatives": [
        "Mean Absolute Error"
      ],
      "explanation": "MAE calculates the average magnitude of errors without squaring.",
      "id": "sup_lea_sup_lea_a_finals_004"
    },
    {
      "question": "What prevents decision trees from growing too deep?",
      "answer": "Pruning",
      "alternatives": [
        "Tree pruning"
      ],
      "explanation": "Pruning removes branches to prevent overfitting.",
      "id": "sup_lea_sup_lea_a_finals_005"
    },
    {
      "question": "What XGBoost feature handles missing values automatically?",
      "answer": "Sparse awareness",
      "alternatives": [
        "Sparsity awareness"
      ],
      "explanation": "XGBoost learns optimal directions for missing values during training.",
      "id": "sup_lea_sup_lea_a_finals_006"
    },
    {
      "question": "What regression metric represents explained variance proportion?",
      "answer": "R-squared",
      "alternatives": [
        "R2",
        "Coefficient of determination"
      ],
      "explanation": "R-squared indicates how well predictions match actual values.",
      "id": "sup_lea_sup_lea_a_finals_007"
    },
    {
      "question": "What distance metric does KNN commonly use?",
      "answer": "Euclidean distance",
      "alternatives": [
        "Euclidean"
      ],
      "explanation": "Euclidean distance measures straight-line distance in feature space.",
      "id": "sup_lea_sup_lea_a_finals_008"
    },
    {
      "question": "What ensemble method trains models on bootstrap samples?",
      "answer": "Bagging",
      "alternatives": [
        "Bootstrap aggregating"
      ],
      "explanation": "Bagging reduces variance by averaging predictions from resampled datasets.",
      "id": "sup_lea_sup_lea_a_finals_009"
    }
  ]
}