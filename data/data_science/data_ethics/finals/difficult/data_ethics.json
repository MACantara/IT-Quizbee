{
  "subtopic_id": "data_ethics",
  "subtopic_name": "Data Ethics",
  "mode": "finals",
  "difficulty": "difficult",
  "questions": [
    {
      "question": "What paradox shows fairness definitions can conflict?",
      "answer": "Fairness impossibility theorem",
      "alternatives": [
        "Impossibility of fairness"
      ],
      "explanation": "Fairness impossibility theorems prove multiple fairness definitions cannot be simultaneously satisfied (except in trivial cases). Example: calibration and demographic parity conflict. Must choose which fairness notion to optimize. Choice depends on context and stakeholders. Highlights that fairness is socio-technical, not purely mathematical. Requires stakeholder engagement to define appropriate fairness for specific application."
    },
    {
      "question": "What attack reconstructs training data from model?",
      "answer": "Model inversion",
      "alternatives": [
        "Model inversion attack"
      ],
      "explanation": "Model inversion attacks reconstruct training data from model parameters or predictions. Exploits models' tendency to memorize training examples. Demonstrated for face recognition (reconstructing faces from name), medical models, language models. Severity depends on model type and overfitting. Defenses include differential privacy, regularization, and limiting query access. Growing concern as models become more powerful."
    },
    {
      "question": "What technique removes specific data's influence from trained model?",
      "answer": "Machine unlearning",
      "alternatives": [
        "Data deletion"
      ],
      "explanation": "Machine unlearning removes specific training data's influence from model without complete retraining. Required for right to be forgotten, fixing poisoned data, removing copyrighted content. Approaches include retraining from scratch (expensive), influence functions (approximate), and sharding (partition training). Active research area. Challenges include verifying complete removal and maintaining model performance."
    },
    {
      "question": "What framework ensures AI systems cause no harm?",
      "answer": "AI safety",
      "alternatives": [
        "Safe AI"
      ],
      "explanation": "AI safety researches ensuring AI systems behave as intended and don't cause harm. Concerns include specification (what goal to optimize), robustness (handling distribution shift), interpretability (understanding decisions), and alignment (matching human values). Increasingly important as AI capabilities grow. Includes technical research and governance. Organizations like OpenAI, DeepMind, MIRI focus on safety."
    },
    {
      "question": "What ethical issue involves AI amplifying inequality?",
      "answer": "Automation inequality",
      "alternatives": [
        "Algorithmic inequality"
      ],
      "explanation": "Automation inequality refers to AI systems disproportionately benefiting privileged groups while harming vulnerable populations. Privileged get helpful AI (recommendation, assistance); vulnerable get punitive AI (surveillance, scoring). Examples: predictive policing in minority neighborhoods, automated benefits denial. Results from biased data, misaligned incentives, and power imbalances. Requires policy interventions and equity-focused design."
    },
    {
      "question": "What technique proves algorithm satisfies fairness constraints?",
      "answer": "Formal verification",
      "alternatives": [
        "Algorithmic verification"
      ],
      "explanation": "Formal verification uses mathematical methods to prove algorithms satisfy specified properties (including fairness constraints). Creates formal specifications, then proves implementation meets them. More rigorous than testing but limited scalability. Applied to critical systems (aviation, medical devices). Emerging for fairness verification. Provides strong guarantees but requires expert knowledge and may not capture all fairness notions."
    },
    {
      "question": "What concern involves AI making decisions without explanation?",
      "answer": "Black box problem",
      "alternatives": [
        "Opacity"
      ],
      "explanation": "Black box problem refers to inability to understand how complex AI models (deep neural networks) make decisions. Creates accountability gaps, makes debugging difficult, limits trust, and complicates fairness audits. Explainable AI (XAI) attempts solutions through LIME, SHAP, attention mechanisms. Trade-off between performance and interpretability. Regulations may require explanations for consequential decisions."
    },
    {
      "question": "What framework considers long-term AI impacts on humanity?",
      "answer": "AI alignment",
      "alternatives": [
        "Value alignment"
      ],
      "explanation": "AI alignment ensures advanced AI systems' goals align with human values and interests. Challenges include defining human values, avoiding unintended consequences, maintaining alignment as AI becomes more capable. Includes inverse reinforcement learning, value learning, corrigibility (accepting corrections). Critical for preventing existential risk from superintelligent AI. Active research area in AI safety."
    },
    {
      "question": "What technique enables privacy-preserving computation?",
      "answer": "Homomorphic encryption",
      "alternatives": [
        "FHE"
      ],
      "explanation": "Homomorphic encryption enables computation on encrypted data without decryption. Results remain encrypted; only data owner can decrypt. Enables privacy-preserving cloud computing, secure outsourcing, confidential AI inference. Fully homomorphic encryption (FHE) supports arbitrary computations but is computationally expensive. Partially homomorphic encryption limits operations but is more practical. Active research making it more efficient."
    },
    {
      "question": "What framework holds organizations accountable for AI outcomes?",
      "answer": "Algorithmic accountability",
      "alternatives": [
        "AI accountability"
      ],
      "explanation": "Algorithmic accountability means organizations are responsible for AI systems' decisions and impacts. Includes documentation, auditing, impact assessments, and remediation. Requires clear governance structures, designated responsible parties, and mechanisms for redress. Regulations increasingly mandate accountability (EU AI Act). Challenges include complexity, distributed responsibility, and technical opacity. Essential for trustworthy AI at scale."
    }
  ]
}