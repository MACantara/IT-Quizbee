{
  "subtopic_id": "unsupervised_learning",
  "subtopic_name": "Unsupervised Learning",
  "mode": "finals",
  "difficulty": "easy",
  "questions": [
    {
      "question": "What technique groups similar data points together?",
      "answer": "Clustering",
      "alternatives": [
        "Cluster analysis"
      ],
      "explanation": "Clustering partitions data into groups (clusters) where points within clusters are similar and points in different clusters are dissimilar. It's unsupervised (no labels) and discovers hidden structure. Applications include customer segmentation, document organization, image compression, and anomaly detection. Common algorithms include k-means, hierarchical, and DBSCAN."
    },
    {
      "question": "What popular algorithm partitions data into k clusters?",
      "answer": "K-means",
      "alternatives": [
        "K-means clustering"
      ],
      "explanation": "K-means clusters data by iteratively assigning points to nearest centroid and updating centroids as cluster means. It's simple, scalable, and widely used. Requires specifying k beforehand; sensitive to initialization and outliers. Works best with spherical clusters of similar size. The elbow method helps choose k."
    },
    {
      "question": "What technique reduces dimensionality while preserving variance?",
      "answer": "PCA",
      "alternatives": [
        "Principal Component Analysis"
      ],
      "explanation": "Principal Component Analysis (PCA) transforms data into orthogonal components ordered by explained variance. First components capture most variation; later ones capture noise. PCA reduces dimensions, removes correlations, and aids visualization. It's linear and assumes data lies on a low-dimensional linear subspace. Standardization is often needed."
    },
    {
      "question": "What method builds tree of nested clusters?",
      "answer": "Hierarchical clustering",
      "alternatives": [
        "Hierarchical cluster analysis"
      ],
      "explanation": "Hierarchical clustering creates tree structure (dendrogram) showing nested cluster relationships. Agglomerative (bottom-up) starts with points as clusters and merges; divisive (top-down) starts with one cluster and splits. Doesn't require pre-specifying cluster count. Linkage criteria (single, complete, average) affect results. Computationally expensive for large datasets."
    },
    {
      "question": "What density-based algorithm finds clusters of arbitrary shape?",
      "answer": "DBSCAN",
      "alternatives": [
        "Density-Based Spatial Clustering"
      ],
      "explanation": "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) groups points in dense regions separated by low-density areas. It discovers clusters of arbitrary shape and identifies outliers as noise. Parameters are epsilon (neighborhood radius) and minPts (minimum points for core). Unlike k-means, it doesn't require specifying cluster count."
    },
    {
      "question": "What metric measures cluster quality using cohesion and separation?",
      "answer": "Silhouette score",
      "alternatives": [
        "Silhouette coefficient"
      ],
      "explanation": "Silhouette score evaluates clustering quality by comparing intra-cluster distance (cohesion) to nearest-cluster distance (separation) for each point. Values range from -1 to 1; higher is better. Score near 1 means well-clustered, near 0 means on border, negative means possibly wrong cluster. Average silhouette score summarizes overall quality."
    },
    {
      "question": "What technique discovers frequent itemsets in transactional data?",
      "answer": "Association rules",
      "alternatives": [
        "Market basket analysis"
      ],
      "explanation": "Association rule mining discovers relationships between items in transactions, like 'customers who buy X often buy Y'. Rules have support (frequency), confidence (conditional probability), and lift (correlation). Apriori algorithm efficiently finds frequent itemsets. Applications include recommendation systems, cross-selling, and catalog design."
    },
    {
      "question": "What method determines optimal number of clusters?",
      "answer": "Elbow method",
      "alternatives": [
        "Elbow curve"
      ],
      "explanation": "Elbow method plots within-cluster sum of squares (WCSS) versus number of clusters. WCSS decreases as k increases; the 'elbow' point where rate of decrease sharply changes suggests optimal k. It balances fit and complexity. However, elbows aren't always clear. Alternative methods include silhouette analysis and gap statistic."
    },
    {
      "question": "What technique identifies unusual patterns deviating from normal?",
      "answer": "Anomaly detection",
      "alternatives": [
        "Outlier detection"
      ],
      "explanation": "Anomaly detection identifies rare items, events, or observations differing significantly from normal patterns. Applications include fraud detection, system health monitoring, and quality control. Methods include statistical (z-score), proximity-based (k-NN distance), and isolation-based (Isolation Forest). Anomalies can be point, contextual, or collective."
    },
    {
      "question": "What metric shows percentage of variance explained by principal components?",
      "answer": "Explained variance",
      "alternatives": [
        "Variance explained ratio"
      ],
      "explanation": "Explained variance ratio indicates the proportion of total variance captured by each principal component in PCA. First PC explains most variance, subsequent PCs explain progressively less. Cumulative explained variance helps choose how many components to retain. Typically retain enough components to explain 80-95% of variance."
    }
  ]
}