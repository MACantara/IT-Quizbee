"""
Additional questions for Computer Architecture & IT Security subtopics.
This module extends the QUESTIONS_DB with more subtopic questions.
"""

# 3. Instruction Set Architecture
INSTRUCTION_SET_QUESTIONS = {
    "easy": [
        {"question": "What is the architecture design philosophy that uses a small set of simple, fast instructions?", "answer": "RISC (Reduced Instruction Set Computer)", "alternatives": ["Reduced Instruction Set Computer", "RISC Architecture"], "explanation": "RISC (Reduced Instruction Set Computer) philosophy emphasizes a small set of simple instructions that execute in a single clock cycle. This approach, used in ARM and MIPS processors, prioritizes simplicity and speed over instruction complexity, relying on compilers to generate efficient code sequences."},
        {"question": "What is the part of an instruction that specifies the operation to be performed?", "answer": "Opcode", "alternatives": ["Operation Code"], "explanation": "The opcode (operation code) is the portion of a machine instruction that specifies the operation to perform, such as ADD, LOAD, or JUMP. It tells the CPU what action to execute, while operands specify what data to use."},
        {"question": "What is the name of the interface that defines all instructions a CPU can execute?", "answer": "ISA (Instruction Set Architecture)", "alternatives": ["Instruction Set Architecture"], "explanation": "The Instruction Set Architecture (ISA) is the abstract interface between hardware and software, defining all instructions, registers, memory addressing modes, and data types a processor supports. It's crucial for software compatibility - programs compiled for one ISA won't run on a processor with a different ISA without emulation."},
        {"question": "What popular instruction set architecture is used in most smartphones and tablets?", "answer": "ARM", "alternatives": ["ARM Architecture", "Advanced RISC Machine"], "explanation": "ARM (Advanced RISC Machine, originally Acorn RISC Machine) is a RISC-based ISA dominant in mobile devices due to its power efficiency. ARM processors power most smartphones, tablets, and increasingly laptops, with Apple's M-series chips and Qualcomm's Snapdragon being notable examples."},
        {"question": "What is the low-level programming language that uses mnemonics for machine instructions?", "answer": "Assembly Language", "alternatives": ["Assembly", "Assembler Language"], "explanation": "Assembly language uses human-readable mnemonics (like MOV, ADD, JMP) instead of binary machine code, making it easier to write low-level programs. Each assembly instruction typically corresponds to one machine instruction, and an assembler translates assembly code into machine code."},
        {"question": "What instruction set architecture is used by Intel and AMD processors?", "answer": "x86", "alternatives": ["x86 Architecture", "IA-32"], "explanation": "x86 is a CISC instruction set architecture originally developed by Intel, used in most desktop and laptop computers. It includes 32-bit (IA-32) and 64-bit (x86-64/AMD64) versions. Despite being complex, x86's backward compatibility has made it the dominant desktop/server architecture."},
        {"question": "What is the binary representation of instructions that the CPU directly executes?", "answer": "Machine Code", "alternatives": ["Machine Language", "Binary Code"], "explanation": "Machine code consists of binary instructions (1s and 0s) that the CPU can directly execute without translation. Every program must ultimately be converted to machine code, whether from high-level languages (via compilation) or assembly language (via assembly)."},
        {"question": "What addressing mode uses a fixed memory address in the instruction?", "answer": "Direct Addressing", "alternatives": ["Absolute Addressing"], "explanation": "In direct addressing mode, the instruction contains the actual memory address of the operand. This is simple and fast but limits flexibility since addresses are hardcoded. For example, 'LOAD 1000' would load the value from memory address 1000."},
        {"question": "What is the term for the method used to specify where operands are located?", "answer": "Addressing Mode", "alternatives": ["Addressing Modes"], "explanation": "Addressing modes determine how the CPU interprets operand specifications in instructions. Common modes include immediate (value in instruction), direct (address in instruction), indirect (address of address), and register (operand in register). Different modes offer tradeoffs between simplicity, flexibility, and efficiency."},
        {"question": "What type of instruction set has variable-length instructions?", "answer": "CISC", "alternatives": ["Complex Instruction Set Computer"], "explanation": "CISC (Complex Instruction Set Computer) architectures like x86 use variable-length instructions, ranging from 1 to 15+ bytes. This allows compact code representation but complicates instruction decoding. In contrast, RISC architectures typically use fixed-length instructions (e.g., 32 bits) for simpler decoding."}
    ],
    "average": [
        {"question": "What RISC-based instruction set architecture is gaining popularity for its open-source, royalty-free nature?", "answer": "RISC-V", "alternatives": ["RISCV"], "explanation": "RISC-V is an open-source RISC instruction set architecture that anyone can use without royalties. Unlike ARM or x86, RISC-V specifications are freely available, allowing custom processor designs. It's gaining traction in embedded systems, research, and increasingly in commercial products, offering an alternative to proprietary ISAs."},
        {"question": "What addressing mode calculates the operand address by adding a constant to a register value?", "answer": "Base-Displacement addressing", "alternatives": ["Indexed addressing", "Register-Offset addressing"], "explanation": "Base-displacement (or indexed) addressing adds a constant offset to a base register value to calculate the operand address. This is extremely useful for accessing array elements, structure fields, and stack variables. For example, accessing array[5] might use base register + (5 × element_size)."},
        {"question": "What is the term for instructions that operate on operands located in CPU registers only?", "answer": "Register-to-Register instructions", "alternatives": ["Register operations", "R-type instructions"], "explanation": "Register-to-register (or R-type) instructions operate entirely on register operands without accessing memory. They're the fastest instructions since registers are immediately accessible. RISC architectures emphasize register operations, using separate load/store instructions for memory access (load-store architecture)."},
        {"question": "What technique allows a single instruction to operate on multiple data elements simultaneously?", "answer": "SIMD (Single Instruction Multiple Data)", "alternatives": ["Vector processing", "SIMD"], "explanation": "SIMD allows a single instruction to perform the same operation on multiple data elements in parallel. Modern processors include SIMD extensions (SSE, AVX for x86; NEON for ARM) that can process 4, 8, or 16 values simultaneously. This is crucial for multimedia processing, scientific computing, and machine learning."},
        {"question": "What is the name for instructions that transfer control to a different part of the program?", "answer": "Branch instructions", "alternatives": ["Jump instructions", "Control flow instructions"], "explanation": "Branch (or jump) instructions alter program control flow by changing the program counter to a different address. They include conditional branches (branch if condition true), unconditional jumps, function calls, and returns. Efficient branch handling is critical for performance, hence sophisticated branch prediction in modern CPUs."},
        {"question": "What addressing mode uses the contents of a register as the memory address?", "answer": "Register Indirect addressing", "alternatives": ["Indirect addressing", "Register deferred"], "explanation": "In register indirect addressing, a register contains the memory address of the operand, not the operand itself. This enables dynamic addressing, crucial for pointer operations, dynamic data structures, and implementing arrays. For example, if R1=1000, then 'LOAD (R1)' loads from address 1000."},
        {"question": "What is the term for ISA features that maintain compatibility with older software?", "answer": "Backward compatibility", "alternatives": ["Legacy support"], "explanation": "Backward compatibility ensures new processors can run software designed for older processors in the same family. x86 maintains remarkable backward compatibility - modern 64-bit processors can run 16-bit DOS programs. This is crucial for commercial success but constrains architectural innovation."},
        {"question": "What instruction format dedicates separate fields for opcode and operands with fixed sizes?", "answer": "Fixed-format instructions", "alternatives": ["RISC format", "Regular format"], "explanation": "Fixed-format instructions use predetermined field positions and sizes for opcode and operands. This simplifies decoding logic and enables faster instruction processing, typical of RISC architectures. All instructions are the same length (e.g., 32 bits), though this may waste encoding space compared to variable-length formats."},
        {"question": "What is the technique where the compiler, not hardware, schedules instructions to avoid hazards?", "answer": "Static scheduling", "alternatives": ["Compile-time scheduling"], "explanation": "Static scheduling relies on the compiler to arrange instructions to minimize pipeline stalls and hazards. This is common in RISC processors where the ISA exposes pipeline behavior to software. The compiler inserts NOPs or reorders independent instructions, reducing hardware complexity at the cost of larger code size."},
        {"question": "What addressing mode automatically increments or decrements a register after using it?", "answer": "Auto-increment/Auto-decrement addressing", "alternatives": ["Post-increment addressing", "Auto-indexing"], "explanation": "Auto-increment/decrement addressing automatically modifies the register after (post) or before (pre) using it as an address. This is extremely efficient for traversing arrays or stacks, eliminating separate increment instructions. For example, 'LOAD (R1)+' loads from R1's address then increments R1."}
    ],
    "difficult": [
        {"question": "What ISA feature allows conditional execution of instructions without branching?", "answer": "Predicated execution", "alternatives": ["Conditional execution", "Predication"], "explanation": "Predicated (or conditional) execution allows instructions to be conditionally executed based on a predicate condition without branching. ARM and IA-64 support this extensively. Instead of 'if (cond) x=a; else x=b;' requiring branches, both assignments execute with one's result discarded based on the condition, eliminating branch misprediction penalties."},
        {"question": "What ISA design principle advocates that frequently used operations should execute faster?", "answer": "Common case optimization", "alternatives": ["Make the common case fast"], "explanation": "The 'make the common case fast' principle guides ISA and microarchitecture design. Since simple operations like loads, stores, and arithmetic dominate program execution, they should be fast even if it makes rare operations slower. This justifies RISC's simple instructions and hardware optimization for common patterns."},
        {"question": "What technique allows processors to expose parallelism explicitly through very long instruction words?", "answer": "VLIW (Very Long Instruction Word)", "alternatives": ["Very Long Instruction Word", "Explicitly Parallel Instruction Computing"], "explanation": "VLIW architectures package multiple operations into a single long instruction word, with the compiler scheduling operations for parallel execution. Unlike superscalar processors that discover parallelism dynamically, VLIW relies entirely on compiler analysis. Intel's Itanium used VLIW (called EPIC), but compiler limitations hindered commercial success."},
        {"question": "What ISA feature allows instructions to specify multiple operations bundled together?", "answer": "Instruction fusion", "alternatives": ["Macro-op fusion", "Op fusion"], "explanation": "Instruction fusion (or macro-op fusion) combines multiple simple instructions into a single complex operation at decode time. For example, compare-and-branch might fuse into one micro-op. This reduces pressure on execution resources and improves performance while maintaining ISA simplicity. Modern x86 processors extensively use fusion."},
        {"question": "What design philosophy motivated RISC development in opposition to increasingly complex CISC processors?", "answer": "ISA minimalism", "alternatives": ["Simplicity principle", "RISC philosophy"], "explanation": "RISC emerged from research showing that CISC complexity provided diminishing returns - most programs used simple instructions primarily. RISC philosophy advocates for simple, uniform instructions that execute quickly, moving complexity to the compiler. This enables higher clock speeds, simpler pipelines, and more efficient hardware implementation."},
        {"question": "What addressing mode allows position-independent code by using the program counter as a base?", "answer": "PC-relative addressing", "alternatives": ["Program counter relative", "IP-relative"], "explanation": "PC-relative addressing calculates addresses relative to the current program counter, enabling position-independent code that works regardless of where it's loaded in memory. This is crucial for shared libraries, relocatable code, and modern security features like ASLR. Branch instructions typically use PC-relative addressing."},
        {"question": "What ISA extension adds instructions specifically for accelerating cryptographic operations?", "answer": "AES-NI", "alternatives": ["AES New Instructions", "Crypto extensions"], "explanation": "AES-NI (Advanced Encryption Standard New Instructions) provides hardware acceleration for AES encryption/decryption, offering 3-10× performance improvement over software implementation. Similar extensions exist for other crypto operations (SHA, etc.). These specialized instructions demonstrate the trend of adding domain-specific accelerators to general-purpose ISAs."},
        {"question": "What technique allows ISAs to add new instruction without breaking compatibility using instruction prefixes or modes?", "answer": "ISA extensions", "alternatives": ["Instruction set extensions", "ISA evolution"], "explanation": "ISA extensions add new capabilities while maintaining backward compatibility through various mechanisms: using previously undefined opcodes, adding instruction prefixes (x86's 0x0F prefix), or mode bits. x86 evolved from 16-bit to 64-bit through extensions (MMX, SSE, AVX), maintaining compatibility across decades while gradually increasing complexity."},
        {"question": "What addressing mode chains multiple indirections to reach the final operand location?", "answer": "Multi-level indirect addressing", "alternatives": ["Cascaded indirect", "Pointer chain"], "explanation": "Multi-level indirect addressing follows a chain of pointers through memory to reach the operand. While conceptually simple (like following linked structures), hardware support is rare due to multiple memory accesses. It's typically implemented in software using multiple instructions, though some historical architectures provided hardware support."},
        {"question": "What ISA characteristic measures the degree to which instructions are orthogonal and composable?", "answer": "ISA regularity", "alternatives": ["Orthogonality", "Instruction uniformity"], "explanation": "ISA regularity (or orthogonality) means instructions work consistently across addressing modes, data types, and operands. Highly regular ISAs like RISC are easier to learn, compile for, and implement in hardware. In contrast, x86's irregularity (special cases, exceptions, asymmetric registers) complicates both hardware and software but evolved from historical constraints."}
    ]
}

# 4. Pipelining 
PIPELINING_QUESTIONS = {
    "easy": [
        {"question": "What is the technique of breaking instruction execution into stages like an assembly line?", "answer": "Pipelining", "alternatives": ["Instruction pipelining", "Pipeline"], "explanation": "Pipelining divides instruction execution into stages (fetch, decode, execute, memory, write-back), allowing multiple instructions to be processed simultaneously at different stages, like an assembly line. This increases throughput without reducing individual instruction latency."},
        {"question": "What is the term for a situation where the pipeline must wait because the next instruction depends on a previous one?", "answer": "Pipeline stall", "alternatives": ["Bubble", "Stall", "Pipeline delay"], "explanation": "A pipeline stall (or bubble) occurs when the pipeline must pause because an instruction cannot proceed to the next stage. This wastes clock cycles and reduces throughput. Common causes include data hazards, control hazards, and structural hazards."},
        {"question": "What is the first stage of a typical instruction pipeline that retrieves instructions from memory?", "answer": "Fetch", "alternatives": ["Instruction Fetch", "IF stage"], "explanation": "The Fetch stage retrieves the next instruction from memory using the program counter address. It's the first stage in a typical 5-stage pipeline (Fetch, Decode, Execute, Memory, Write-back). Efficient fetching is crucial since it feeds the entire pipeline."},
        {"question": "What type of hazard occurs when an instruction needs data that hasn't been computed yet?", "answer": "Data hazard", "alternatives": ["Data dependency"], "explanation": "Data hazards occur when an instruction depends on the result of a previous instruction still in the pipeline. For example, if instruction 2 needs the result of instruction 1, but instruction 1 hasn't reached the write-back stage yet, a data hazard exists."},
        {"question": "What is the term for the wasted time in a pipeline due to hazards or dependencies?", "answer": "Pipeline overhead", "alternatives": ["Stall cycles", "Bubble cycles"], "explanation": "Pipeline overhead refers to wasted clock cycles where pipeline stages sit idle due to hazards, preventing the pipeline from achieving its theoretical maximum throughput. Minimizing overhead through hazard detection and resolution is crucial for pipeline efficiency."},
        {"question": "In a 5-stage pipeline, what is the stage that interprets the instruction and reads operands?", "answer": "Decode", "alternatives": ["Instruction Decode", "ID stage"], "explanation": "The Decode stage interprets the instruction opcode, identifies required operands, and reads values from registers. It's the second stage in a classic 5-stage RISC pipeline and prepares everything needed for the Execute stage."},
        {"question": "What is the maximum number of instructions that can be simultaneously in different stages of an N-stage pipeline?", "answer": "N", "alternatives": ["N instructions"], "explanation": "An N-stage pipeline can have at most N instructions in flight simultaneously, one per stage. For example, a 5-stage pipeline can process 5 instructions at once. This is the key to pipelining's performance improvement - overlapping execution of multiple instructions."},
        {"question": "What technique inserts wasted cycles (NOPs) to resolve pipeline hazards?", "answer": "Pipeline stalling", "alternatives": ["Inserting bubbles", "Adding NOPs"], "explanation": "Pipeline stalling intentionally inserts empty cycles (bubbles or NOPs) to delay dependent instructions until their operands are ready. While simple to implement, stalling wastes cycles and reduces pipeline efficiency. More sophisticated techniques like forwarding can often avoid stalls."},
        {"question": "What is the term for when two instructions try to use the same hardware resource simultaneously?", "answer": "Structural hazard", "alternatives": ["Resource conflict"], "explanation": "Structural hazards occur when hardware resources can't support all concurrent operations in a pipeline. For example, if there's only one memory port but both fetch and memory stages need to access memory simultaneously, a structural hazard exists. Solutions include adding more resources or scheduling to avoid conflicts."},
        {"question": "What pipeline stage performs arithmetic, logical, and comparison operations?", "answer": "Execute", "alternatives": ["EX stage", "Execution stage"], "explanation": "The Execute (EX) stage performs the actual operation specified by the instruction: arithmetic (add, subtract), logical (AND, OR), shifts, or comparisons. It's the third stage in a classic 5-stage pipeline and is where the ALU does its work."}
    ],
    "average": [
        {"question": "What technique bypasses the write-back stage to directly forward results to dependent instructions?", "answer": "Data forwarding", "alternatives": ["Bypassing", "Short-circuiting"], "explanation": "Data forwarding (or bypassing) provides results directly from one pipeline stage to another without waiting for write-back to registers. For example, the ALU output can be forwarded immediately to a dependent instruction's ALU input, avoiding stalls. This is a key technique for maintaining pipeline efficiency."},
        {"question": "What type of hazard occurs when pipeline decisions depend on branch outcomes not yet determined?", "answer": "Control hazard", "alternatives": ["Branch hazard"], "explanation": "Control hazards (or branch hazards) occur when the pipeline must make decisions before knowing whether a branch will be taken. The pipeline doesn't know which instruction to fetch next until the branch condition is evaluated, potentially several stages later, causing pipeline stalls or requiring speculative execution."},
        {"question": "What is the technique where the compiler rearranges instructions to minimize pipeline stalls?", "answer": "Instruction scheduling", "alternatives": ["Code scheduling", "Compiler scheduling"], "explanation": "Instruction scheduling is a compiler optimization that reorders independent instructions to minimize pipeline stalls. The compiler analyzes data dependencies and arranges instructions to maximize pipeline utilization, filling potential stall cycles with useful work. This is especially important for statically scheduled RISC processors."},
        {"question": "What is the RAW (Read After Write) hazard where an instruction needs data before it's written?", "answer": "True dependency", "alternatives": ["Data dependency", "RAW hazard"], "explanation": "A RAW (Read After Write) hazard is a true data dependency where an instruction must read a value that a previous instruction will write. For example: 'ADD R1, R2, R3' followed by 'SUB R4, R1, R5' has a RAW hazard on R1. The second instruction must wait for R1 to be written. Forwarding can often resolve RAW hazards."},
        {"question": "What technique speculatively executes instructions from both branch paths simultaneously?", "answer": "Multipath execution", "alternatives": ["Eager execution"], "explanation": "Multipath (or eager) execution speculatively executes both possible branch paths simultaneously, keeping both results until the branch resolves. While resource-intensive, this eliminates branch misprediction penalties entirely. It's practical only for short instruction sequences due to resource constraints."},
        {"question": "What is the maximum theoretical speedup achievable by an N-stage pipeline compared to unpipelined execution?", "answer": "N", "alternatives": ["N times", "Linear speedup"], "explanation": "The theoretical maximum speedup of an N-stage pipeline is N times the unpipelined performance, assuming perfect conditions (no hazards, balanced stages). In practice, hazards, unbalanced stages, and overhead reduce actual speedup. For example, a 5-stage pipeline theoretically provides 5× speedup."},
        {"question": "What WAW (Write After Write) hazard occurs when instructions write to the same register out of order?", "answer": "Output dependency", "alternatives": ["WAW hazard"], "explanation": "A WAW (Write After Write) hazard is an output dependency where two instructions write to the same register, and they might complete out of order. For example, if instruction 2 writes R1 before instruction 1 (which also writes R1), the final value would be incorrect. Register renaming typically resolves WAW hazards."},
        {"question": "What is the term for the time between injecting consecutive instructions into the pipeline?", "answer": "Pipeline initiation interval", "alternatives": ["Throughput rate"], "explanation": "The pipeline initiation interval (or issue rate) is the time between starting consecutive instructions. In an ideal pipeline, it's one clock cycle - a new instruction enters every cycle. The reciprocal gives the throughput. Hazards increase the initiation interval, reducing throughput."},
        {"question": "What technique delays branch resolution while continuing to execute subsequent instructions?", "answer": "Delayed branching", "alternatives": ["Branch delay slot"], "explanation": "Delayed branching exposes the branch delay to the ISA - the instruction(s) immediately following a branch execute regardless of the branch outcome. The compiler fills these 'delay slots' with useful instructions from before the branch or makes them NOPs. This was common in early RISC processors (MIPS, SPARC)."},
        {"question": "What is the WAR (Write After Read) hazard that occurs only with out-of-order execution?", "answer": "Anti-dependency", "alternatives": ["WAR hazard"], "explanation": "A WAR (Write After Read) anti-dependency occurs when an instruction writes a value before a previous instruction reads the old value. This only causes problems with out-of-order execution since in-order pipelines naturally avoid it. Register renaming eliminates WAR hazards by giving each write a unique destination."}
    ],
    "difficult": [
        {"question": "What advanced technique allows a pipeline to continue execution past conditional branches before the condition is known?", "answer": "Speculative execution", "alternatives": ["Branch speculation"], "explanation": "Speculative execution allows the pipeline to continue beyond branches by guessing the outcome and executing along the predicted path. If the prediction is correct, performance improves; if wrong, the speculatively executed work is discarded and the pipeline restarts from the correct path. This trades occasional penalties for usually avoiding stalls."},
        {"question": "What hardware structure tracks which pipeline stages have produced results available for forwarding?", "answer": "Scoreboard", "alternatives": ["Hazard detection unit"], "explanation": "A scoreboard is a hardware table that tracks register availability and pending operations, enabling hazard detection and resolution. It monitors which instructions are using which registers and controls forwarding and stalling. Scoreboards are central to Tomasulo's algorithm and other dynamic scheduling schemes."},
        {"question": "What is the phenomenon where deeper pipelines provide diminishing returns and eventually hurt performance?", "answer": "Pipeline depth limitation", "alternatives": ["Deep pipeline penalty"], "explanation": "Beyond a certain depth, adding pipeline stages hurts performance due to increased branch misprediction penalties, forwarding complexity, and overhead. Each added stage reduces per-stage work but increases pipeline latency, making branch mispredictions more costly. Modern designs balance depth (typically 10-20 stages) against these factors."},
        {"question": "What technique uses multiple parallel pipelines to issue multiple instructions per clock cycle?", "answer": "Superscalar execution", "alternatives": ["Multiple issue", "Superscalar"], "explanation": "Superscalar processors have multiple parallel pipelines, allowing them to fetch, decode, and execute multiple instructions per cycle. A 4-wide superscalar can theoretically complete 4 instructions per clock. This requires significant hardware for dependency checking, multiple execution units, and sophisticated scheduling."},
        {"question": "What is the latency penalty for a branch misprediction in a pipeline with N stages between fetch and execute?", "answer": "N cycles", "alternatives": ["N clock cycles"], "explanation": "When a branch mispredicts in an N-stage pipeline, all speculatively fetched instructions (up to N-1) must be flushed, wasting N cycles before correct-path instructions enter execution. This is why deep pipelines are vulnerable to control hazards and why accurate branch prediction is crucial for performance."},
        {"question": "What technique saves pipeline state at branch points to enable rapid recovery from mispredictions?", "answer": "Checkpoint recovery", "alternatives": ["State checkpointing"], "explanation": "Checkpoint recovery saves the processor state (registers, flags) at branch points. On misprediction, instead of flushing the entire pipeline and re-fetching instructions, the checkpoint is restored, reducing recovery time. This is especially valuable in deep pipelines where misprediction penalties are severe."},
        {"question": "What hardware structure allows out-of-order execution while maintaining in-order commit?", "answer": "Reorder buffer", "alternatives": ["ROB", "Completion buffer"], "explanation": "The reorder buffer (ROB) is a circular buffer that holds instructions in program order from dispatch until retirement. Instructions execute out-of-order but commit in-order from the ROB, maintaining precise exceptions and architectural state. The ROB enables aggressive out-of-order execution while preserving sequential semantics."},
        {"question": "What advanced scheduling technique issues instructions to reservation stations that execute when operands arrive?", "answer": "Tomasulo's algorithm", "alternatives": ["Tomasulo scheduling"], "explanation": "Tomasulo's algorithm uses reservation stations to track instructions waiting for operands. When results are produced, they're broadcast to all stations; ready instructions execute immediately. This enables out-of-order execution, register renaming, and dynamic scheduling without compiler support. Tomasulo's algorithm was pioneering in enabling high-performance out-of-order processors."},
        {"question": "What is the technique of executing multiple iterations of a loop in parallel across pipeline stages?", "answer": "Software pipelining", "alternatives": ["Loop pipelining", "Modulo scheduling"], "explanation": "Software pipelining (or modulo scheduling) rearranges loop iterations so different iterations execute in parallel pipeline stages. Instead of completing iteration N before starting N+1, operations from multiple iterations overlap. This is like loop unrolling but maintains compact code while exposing parallelism. It requires sophisticated compiler analysis of loop-carried dependencies."},
        {"question": "What phenomenon causes performance degradation when pipeline stages have significantly different execution times?", "answer": "Pipeline imbalance", "alternatives": ["Stage imbalance"], "explanation": "Pipeline imbalance occurs when stages have unequal delays, causing the slowest stage to limit overall throughput (the pipeline is only as fast as its slowest stage). Ideally, all stages should take equal time. Imbalance wastes potential performance since faster stages sit idle waiting for slower ones. Careful pipeline design aims to balance stage complexity."}
    ]
}

# Continue with remaining subtopics...
# This file can be imported by question_generator.py to extend the database

ALL_ADDITIONAL_QUESTIONS = {
    'instruction_set': INSTRUCTION_SET_QUESTIONS,
    'pipelining': PIPELINING_QUESTIONS
}
