{
  "subtopic_id": "unsupervised_learning",
  "subtopic_name": "Unsupervised Learning",
  "mode": "elimination",
  "questions": [
    {
      "question": "Which algorithm groups similar data points into clusters?",
      "options": [
        "Linear regression",
        "K-Means",
        "Logistic regression",
        "Decision tree"
      ],
      "correct": 1,
      "explanation": "K-Means partitions data into k clusters based on similarity.",
      "id": "dat_sci_uns_lea_elim_000"
    },
    {
      "question": "What unsupervised technique reduces dataset dimensionality?",
      "options": [
        "Classification",
        "Regression",
        "PCA",
        "Gradient Boosting"
      ],
      "correct": 2,
      "explanation": "PCA transforms data into fewer dimensions while preserving variance.",
      "id": "dat_sci_uns_lea_elim_001"
    },
    {
      "question": "Which clustering algorithm builds a hierarchy of clusters?",
      "options": [
        "K-Means",
        "DBSCAN",
        "Mean Shift",
        "Hierarchical clustering"
      ],
      "correct": 3,
      "explanation": "Hierarchical clustering creates tree-like nested cluster structures.",
      "id": "dat_sci_uns_lea_elim_002"
    },
    {
      "question": "What algorithm discovers density-based clusters?",
      "options": [
        "DBSCAN",
        "K-Means",
        "PCA",
        "t-SNE"
      ],
      "correct": 0,
      "explanation": "DBSCAN finds clusters as high-density regions separated by low-density areas.",
      "id": "dat_sci_uns_lea_elim_003"
    },
    {
      "question": "Which technique visualizes high-dimensional data in 2D or 3D?",
      "options": [
        "Logistic Regression",
        "t-SNE",
        "Random Forest",
        "SVM"
      ],
      "correct": 1,
      "explanation": "t-SNE reduces dimensions while preserving local structure for visualization.",
      "id": "dat_sci_uns_lea_elim_004"
    },
    {
      "question": "What is the main goal of clustering?",
      "options": [
        "Predict labels",
        "Group similar data points",
        "Reduce overfitting",
        "Increase accuracy"
      ],
      "correct": 1,
      "explanation": "Clustering finds natural groupings in unlabeled data.",
      "id": "dat_sci_uns_lea_elim_005"
    },
    {
      "question": "Which metric measures how well K-Means performs?",
      "options": [
        "Accuracy",
        "Precision",
        "Inertia",
        "Recall"
      ],
      "correct": 2,
      "explanation": "Inertia measures the sum of squared distances from points to their cluster centers.",
      "id": "dat_sci_uns_lea_elim_006"
    },
    {
      "question": "What method determines optimal number of clusters using variance?",
      "options": [
        "Confusion matrix",
        "ROC curve",
        "Elbow method",
        "Cross-validation"
      ],
      "correct": 2,
      "explanation": "The elbow method plots inertia vs. number of clusters to find the optimal k.",
      "id": "dat_sci_uns_lea_elim_007"
    },
    {
      "question": "Which technique discovers association rules in transactions?",
      "options": [
        "K-Means",
        "PCA",
        "Apriori",
        "DBSCAN"
      ],
      "correct": 2,
      "explanation": "Apriori finds frequent itemsets and association rules in transactional data.",
      "id": "dat_sci_uns_lea_elim_008"
    },
    {
      "question": "What algorithm detects unusual patterns or anomalies?",
      "options": [
        "Decision Tree",
        "Linear Regression",
        "Isolation Forest",
        "Logistic Regression"
      ],
      "correct": 2,
      "explanation": "Isolation Forest identifies anomalies by isolating outliers in the data.",
      "id": "dat_sci_uns_lea_elim_009"
    }
  ]
}