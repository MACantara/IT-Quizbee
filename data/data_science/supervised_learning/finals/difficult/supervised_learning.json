{
  "subtopic_id": "supervised_learning",
  "subtopic_name": "Supervised Learning",
  "mode": "finals",
  "difficulty": "difficult",
  "questions": [
    {
      "question": "What boosting library uses histogram-based learning?",
      "answer": "LightGBM",
      "alternatives": ["Light GBM"],
      "explanation": "LightGBM speeds up training with histogram-based algorithms."
    },
    {
      "question": "What SVM kernel handles non-linear boundaries efficiently?",
      "answer": "RBF kernel",
      "alternatives": ["Radial Basis Function", "Gaussian kernel"],
      "explanation": "RBF kernel maps features to infinite-dimensional space."
    },
    {
      "question": "What technique stacks model predictions as meta-features?",
      "answer": "Stacking",
      "alternatives": ["Stacked generalization"],
      "explanation": "Stacking trains a meta-model on predictions from base models."
    },
    {
      "question": "What decision tree criterion measures information gain?",
      "answer": "Entropy",
      "alternatives": ["Information entropy"],
      "explanation": "Entropy quantifies uncertainty used in ID3 and C4.5 algorithms."
    },
    {
      "question": "What gradient boosting variant uses second-order derivatives?",
      "answer": "XGBoost",
      "alternatives": ["Extreme Gradient Boosting"],
      "explanation": "XGBoost uses Newton-Raphson method for optimization."
    },
    {
      "question": "What handles multi-class classification in SVM?",
      "answer": "One-vs-Rest",
      "alternatives": ["OvR", "One-vs-All"],
      "explanation": "One-vs-Rest trains binary classifiers for each class."
    },
    {
      "question": "What algorithm optimizes hyperplanes with slack variables?",
      "answer": "Soft-margin SVM",
      "alternatives": ["Soft margin SVM"],
      "explanation": "Soft-margin SVM allows some misclassifications for better generalization."
    },
    {
      "question": "What regression handles multiple outputs simultaneously?",
      "answer": "Multi-output regression",
      "alternatives": ["Multioutput regression"],
      "explanation": "Multi-output regression predicts multiple target variables jointly."
    },
    {
      "question": "What boosting technique uses categorical features natively?",
      "answer": "CatBoost",
      "alternatives": ["Cat Boost"],
      "explanation": "CatBoost handles categorical data without preprocessing."
    },
    {
      "question": "What loss function is used for probabilistic regression?",
      "answer": "Log loss",
      "alternatives": ["Cross-entropy", "Logarithmic loss"],
      "explanation": "Log loss penalizes confident wrong predictions heavily."
    }
  ]
}