{
  "subtopic_id": "supervised_learning",
  "subtopic_name": "Supervised Learning",
  "mode": "elimination",
  "questions": [
    {
      "question": "Which algorithm predicts continuous numerical values?",
      "options": [
        "Classification",
        "Linear regression",
        "Clustering",
        "Association"
      ],
      "correct": 1,
      "explanation": "Linear regression models the relationship between variables to predict numerical outputs.",
      "id": "dat_sci_sup_lea_elim_000"
    },
    {
      "question": "What type of supervised learning predicts categorical labels?",
      "options": [
        "Regression",
        "Dimensionality reduction",
        "Classification",
        "Anomaly detection"
      ],
      "correct": 2,
      "explanation": "Classification assigns input data to predefined categories or classes.",
      "id": "dat_sci_sup_lea_elim_001"
    },
    {
      "question": "Which algorithm finds the hyperplane that best separates classes?",
      "options": [
        "K-Means",
        "Decision Tree",
        "Naive Bayes",
        "Support Vector Machine"
      ],
      "correct": 3,
      "explanation": "SVM finds the optimal decision boundary with maximum margin between classes.",
      "id": "dat_sci_sup_lea_elim_002"
    },
    {
      "question": "What classification algorithm uses a tree-like structure of decisions?",
      "options": [
        "Decision Tree",
        "Logistic Regression",
        "K-Nearest Neighbors",
        "Neural Network"
      ],
      "correct": 0,
      "explanation": "Decision trees split data based on feature values to make predictions.",
      "id": "dat_sci_sup_lea_elim_003"
    },
    {
      "question": "Which algorithm classifies based on the majority vote of k nearest neighbors?",
      "options": [
        "SVM",
        "K-Nearest Neighbors",
        "Random Forest",
        "Gradient Boosting"
      ],
      "correct": 1,
      "explanation": "KNN assigns labels based on the most common class among k closest training examples.",
      "id": "dat_sci_sup_lea_elim_004"
    },
    {
      "question": "What is an ensemble of decision trees called?",
      "options": [
        "Neural network",
        "Naive Bayes",
        "Random Forest",
        "Logistic Regression"
      ],
      "correct": 2,
      "explanation": "Random Forest combines multiple decision trees to improve accuracy and reduce overfitting.",
      "id": "dat_sci_sup_lea_elim_005"
    },
    {
      "question": "Which algorithm uses probability and Bayes theorem for classification?",
      "options": [
        "Decision Tree",
        "SVM",
        "Linear Regression",
        "Naive Bayes"
      ],
      "correct": 3,
      "explanation": "Naive Bayes applies Bayes theorem assuming feature independence.",
      "id": "dat_sci_sup_lea_elim_006"
    },
    {
      "question": "What metric is commonly used for regression model evaluation?",
      "options": [
        "Mean Squared Error",
        "Accuracy",
        "Precision",
        "F1-score"
      ],
      "correct": 0,
      "explanation": "MSE measures the average squared difference between predicted and actual values.",
      "id": "dat_sci_sup_lea_elim_007"
    },
    {
      "question": "Which algorithm uses sigmoid function for binary classification?",
      "options": [
        "Logistic Regression",
        "Linear Regression",
        "K-Means",
        "PCA"
      ],
      "correct": 0,
      "explanation": "Logistic regression models probability using the sigmoid activation function.",
      "id": "dat_sci_sup_lea_elim_008"
    },
    {
      "question": "What boosting algorithm builds trees sequentially to minimize errors?",
      "options": [
        "Bagging",
        "Random Forest",
        "Gradient Boosting",
        "Stacking"
      ],
      "correct": 2,
      "explanation": "Gradient Boosting iteratively adds trees that focus on previous errors.",
      "id": "dat_sci_sup_lea_elim_009"
    }
  ]
}