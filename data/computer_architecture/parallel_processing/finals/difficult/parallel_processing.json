{
  "subtopic_id": "parallel_processing",
  "subtopic_name": "Parallel Processing",
  "mode": "finals",
  "difficulty": "difficult",
  "questions": [
    {
      "question": "What is NUMA architecture?",
      "answer": "Non-Uniform Memory Access",
      "alternatives": [
        "NUMA"
      ],
      "explanation": "NUMA gives processors faster access to local memory than remote memory in multiprocessor systems.",
      "id": "par_pro_par_pro_d_finals_000"
    },
    {
      "question": "What performance problem occurs when threads modify different variables on the same cache line?",
      "answer": "False sharing",
      "alternatives": [
        "Cache line contention",
        "False cache sharing"
      ],
      "explanation": "False sharing causes performance degradation when threads modify different variables on the same cache line.",
      "id": "par_pro_par_pro_d_finals_001"
    },
    {
      "question": "What scheduling strategy allows idle processors to take tasks from busy ones?",
      "answer": "Work stealing",
      "alternatives": [
        "Task stealing",
        "Load redistribution"
      ],
      "explanation": "Work stealing is a scheduling strategy where idle processors steal pending tasks from busy processors.",
      "id": "par_pro_par_pro_d_finals_002"
    },
    {
      "question": "What instruction enforces ordering constraints on memory operations across processors?",
      "answer": "Memory fence",
      "alternatives": [
        "Memory barrier",
        "Fence instruction"
      ],
      "explanation": "Memory fences enforce ordering constraints on memory operations across multiple processors.",
      "id": "par_pro_par_pro_d_finals_003"
    },
    {
      "question": "What is the PRAM model?",
      "answer": "Parallel Random Access Machine",
      "alternatives": [
        "PRAM"
      ],
      "explanation": "PRAM is a theoretical model for analyzing parallel algorithms assuming simultaneous memory access.",
      "id": "par_pro_par_pro_d_finals_004"
    },
    {
      "question": "What scaling model keeps total problem size constant as processors increase?",
      "answer": "Strong scaling",
      "alternatives": [
        "Strong scalability"
      ],
      "explanation": "Strong scaling fixes total problem size while weak scaling fixes work per processor as processors increase.",
      "id": "par_pro_par_pro_d_finals_005"
    },
    {
      "question": "What architecture executes instructions as soon as operands become available?",
      "answer": "Dataflow architecture",
      "alternatives": [
        "Data-driven architecture",
        "Dataflow"
      ],
      "explanation": "Dataflow architectures execute instructions as soon as their input data becomes available.",
      "id": "par_pro_par_pro_d_finals_006"
    },
    {
      "question": "What parallelism type uses small tasks with frequent synchronization?",
      "answer": "Fine-grained parallelism",
      "alternatives": [
        "Fine-grain parallelism",
        "Fine-grained"
      ],
      "explanation": "Fine-grained parallelism divides computation into many small tasks requiring frequent communication.",
      "id": "par_pro_par_pro_d_finals_007"
    },
    {
      "question": "What parallelism type uses large tasks with infrequent synchronization?",
      "answer": "Coarse-grained parallelism",
      "alternatives": [
        "Coarse-grain parallelism",
        "Coarse-grained"
      ],
      "explanation": "Coarse-grained parallelism uses larger independent tasks with minimal inter-task communication.",
      "id": "par_pro_par_pro_d_finals_008"
    },
    {
      "question": "What concurrency mechanism treats groups of operations as atomic transactions?",
      "answer": "Transactional memory",
      "alternatives": [
        "TM",
        "Hardware transactional memory"
      ],
      "explanation": "Transactional memory simplifies concurrent programming by treating groups of operations as atomic transactions.",
      "id": "par_pro_par_pro_d_finals_009"
    }
  ]
}