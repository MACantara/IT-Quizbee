{
  "subtopic_id": "parallel_processing",
  "subtopic_name": "Parallel Processing",
  "mode": "finals",
  "difficulty": "easy",
  "questions": [
    {
      "question": "What term describes performing multiple operations or tasks simultaneously?",
      "answer": "Parallelism",
      "alternatives": [
        "Parallel processing",
        "Concurrent processing"
      ],
      "explanation": "Parallelism involves executing multiple operations simultaneously to improve performance. This can occur at different levels: instruction-level (executing multiple instructions at once), thread-level (running multiple threads), or task-level (distributing work across multiple processors)."
    },
    {
      "question": "What type of parallel processing has multiple processing units with shared memory?",
      "answer": "Multiprocessor",
      "alternatives": [
        "Shared memory multiprocessor",
        "SMP"
      ],
      "explanation": "Multiprocessor systems have multiple CPUs sharing a common memory space. This allows different processors to access the same data directly, simplifying programming but requiring synchronization to prevent conflicts. Symmetric multiprocessors (SMP) are common in modern servers and workstations."
    },
    {
      "question": "What is the term for running multiple instructions from a single instruction stream in parallel?",
      "answer": "SIMD (Single Instruction Multiple Data)",
      "alternatives": [
        "Vector processing"
      ],
      "explanation": "SIMD executes the same operation on multiple data elements simultaneously. One instruction operates on vectors of data rather than single values. This is efficient for data-parallel workloads like image processing, where the same operation applies to many pixels."
    },
    {
      "question": "What type of parallelism exists when multiple processors execute different programs on different data?",
      "answer": "MIMD (Multiple Instruction Multiple Data)",
      "alternatives": [
        "Multiple Instruction Multiple Data"
      ],
      "explanation": "MIMD systems have multiple processors independently executing different instructions on different data. This is the most flexible parallel architecture, supporting both data and task parallelism. Modern multicore processors and computer clusters are MIMD systems."
    },
    {
      "question": "What is the term for dividing a large problem into smaller sub-problems that can be solved in parallel?",
      "answer": "Decomposition",
      "alternatives": [
        "Problem decomposition",
        "Parallelization"
      ],
      "explanation": "Decomposition breaks a large problem into independent sub-problems that can execute in parallel. Effective decomposition is key to parallel programming success, requiring identification of independent computations and managing dependencies between sub-problems."
    },
    {
      "question": "What hardware feature allows a CPU to have multiple independent processing cores on a single chip?",
      "answer": "Multicore",
      "alternatives": [
        "Multi-core processor",
        "Multicore architecture"
      ],
      "explanation": "Multicore processors integrate multiple complete CPU cores on a single chip, each capable of independent execution. This provides genuine parallelism within a single package, improving performance for multi-threaded applications while managing power consumption better than increasing clock speeds."
    },
    {
      "question": "What is the term for the overhead involved in distributing work among parallel processors?",
      "answer": "Parallelization overhead",
      "alternatives": [
        "Parallel overhead",
        "Coordination cost"
      ],
      "explanation": "Parallelization overhead includes time spent distributing work, synchronizing processors, communicating between them, and combining results. This overhead can limit speedup - if it's too high, parallel execution might be slower than sequential. Efficient parallel algorithms minimize this overhead."
    },
    {
      "question": "What law states that speedup is limited by the sequential portion of a program?",
      "answer": "Amdahl's Law",
      "alternatives": [
        "Amdahl"
      ],
      "explanation": "Amdahl's Law states that the maximum speedup of a program is limited by its sequential fraction. Even with infinite processors, if 10% of the program must execute sequentially, maximum speedup is 10×. This highlights the importance of maximizing the parallel portion of programs."
    },
    {
      "question": "What term describes multiple processors working on a single shared task?",
      "answer": "Data parallelism",
      "alternatives": [
        "Data-parallel processing"
      ],
      "explanation": "Data parallelism divides data among processors, with each processor performing the same operation on different data subsets. This is effective for problems with large data sets where the same operation applies to all elements, like matrix operations or image filtering."
    },
    {
      "question": "What is the maximum theoretical speedup when using N processors?",
      "answer": "N",
      "alternatives": [
        "Linear speedup",
        "N times"
      ],
      "explanation": "The theoretical maximum speedup with N processors is N× (linear speedup), meaning the parallel version runs N times faster than serial execution. However, real speedup is usually less due to parallelization overhead, sequential portions (Amdahl's Law), and communication costs."
    }
  ]
}