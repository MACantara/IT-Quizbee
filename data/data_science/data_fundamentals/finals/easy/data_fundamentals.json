{
  "subtopic_id": "data_fundamentals",
  "subtopic_name": "Data Fundamentals",
  "mode": "finals",
  "difficulty": "easy",
  "questions": [
    {
      "question": "What is data science?",
      "answer": "An interdisciplinary field using scientific methods, algorithms, and systems to extract knowledge and insights from data",
      "alternatives": [],
      "explanation": "Data science is an interdisciplinary field that combines statistics, mathematics, programming, domain expertise, and machine learning to extract meaningful insights and knowledge from structured and unstructured data. It involves collecting, cleaning, analyzing, and visualizing data to solve complex problems and support decision-making. Key skills include Python/R programming, statistics, machine learning, and data visualization."
    },
    {
      "question": "What is the difference between structured and unstructured data?",
      "answer": "Structured is organized in tables; unstructured lacks predefined format (text, images, videos)",
      "alternatives": [],
      "explanation": "Structured data is highly organized, stored in fixed fields (like databases, spreadsheets) with clear schema, making it easy to search and analyze (e.g., customer records, financial transactions). Unstructured data lacks predefined format or organization (emails, social media posts, images, videos, audio) and requires advanced techniques (NLP, computer vision) to extract insights. Semi-structured data (JSON, XML) falls in between."
    },
    {
      "question": "What are the main types of data analytics?",
      "answer": "Descriptive, Diagnostic, Predictive, and Prescriptive analytics",
      "alternatives": [],
      "explanation": "The four types are: 1) Descriptive analytics (what happened? - reports, dashboards, summary statistics), 2) Diagnostic analytics (why did it happen? - drill-down, root cause analysis), 3) Predictive analytics (what will happen? - forecasting, ML models), 4) Prescriptive analytics (what should we do? - optimization, recommendations). Each builds on the previous, increasing complexity and value."
    },
    {
      "question": "What is a dataset?",
      "answer": "A collection of related data organized for analysis",
      "alternatives": [],
      "explanation": "A dataset is a structured collection of related data, typically organized in rows (observations/records) and columns (variables/features). Datasets can be stored in various formats (CSV, Excel, SQL database, JSON). Example: A customer dataset with columns for name, age, email, purchase history. Quality datasets are fundamental for data science—'garbage in, garbage out' applies."
    },
    {
      "question": "What is a data pipeline?",
      "answer": "A series of data processing steps that automate the flow of data from source to destination",
      "alternatives": [],
      "explanation": "A data pipeline is an automated workflow that moves and transforms data from sources (databases, APIs, files) through various processing steps (cleaning, transformation, aggregation) to destinations (data warehouse, analytics platform, ML model). Pipelines enable ETL (Extract, Transform, Load) processes, ensure data quality, and automate repetitive tasks. Tools include Apache Airflow, Luigi, and cloud services (AWS Glue, Azure Data Factory)."
    },
    {
      "question": "What is the difference between data and information?",
      "answer": "Data is raw facts; information is processed data that provides meaning and context",
      "alternatives": [],
      "explanation": "Data consists of raw, unprocessed facts and figures without context (numbers, text, observations). Information is data that has been processed, organized, and interpreted to provide meaning and answer questions. Example: '32, 45, 28' is data; 'Average temperature last week was 35°C' is information. The data science process transforms data into information, then into knowledge and insights."
    },
    {
      "question": "What is data quality and why is it important?",
      "answer": "The accuracy, completeness, consistency, and reliability of data for its intended use",
      "alternatives": [],
      "explanation": "Data quality refers to data being fit for purpose, measured by dimensions like accuracy (correctness), completeness (no missing values), consistency (uniform format), timeliness (up-to-date), and validity (meets business rules). Poor data quality leads to incorrect analyses, flawed decisions, and wasted resources. The '1-10-100 rule' suggests: $1 to prevent, $10 to correct, $100 cost of failure. Data cleaning addresses quality issues."
    },
    {
      "question": "What is exploratory data analysis (EDA)?",
      "answer": "The initial investigation of data using summary statistics and visualizations to understand patterns and relationships",
      "alternatives": [],
      "explanation": "EDA is the critical first step in data analysis where you explore the dataset to understand its structure, detect patterns, spot anomalies, test hypotheses, and check assumptions. Techniques include summary statistics (mean, median, standard deviation), visualizations (histograms, scatter plots, box plots), and correlation analysis. EDA helps identify data quality issues, informs feature engineering, and guides modeling decisions. Popularized by statistician John Tukey."
    },
    {
      "question": "What is a data warehouse?",
      "answer": "A centralized repository that stores integrated data from multiple sources for analysis and reporting",
      "alternatives": [],
      "explanation": "A data warehouse is a large, centralized repository designed for storing and analyzing structured data from multiple sources (operational databases, CRM, ERP). Unlike transactional databases (OLTP) optimized for writes, data warehouses (OLAP) are optimized for complex queries and analytics. Data is cleaned, transformed, and organized in schemas (star, snowflake) for business intelligence and reporting. Examples: Amazon Redshift, Google BigQuery, Snowflake."
    },
    {
      "question": "What is the data science lifecycle?",
      "answer": "Problem definition, data collection, cleaning, exploration, modeling, evaluation, deployment, and monitoring",
      "alternatives": [],
      "explanation": "The data science lifecycle is an iterative process: 1) Problem definition (understand business question), 2) Data collection (gather relevant data), 3) Data cleaning/preprocessing (handle missing values, outliers), 4) Exploratory analysis (understand patterns), 5) Feature engineering (create relevant features), 6) Modeling (build predictive/descriptive models), 7) Evaluation (assess performance), 8) Deployment (put into production), 9) Monitoring (track performance). Not strictly linear—often involves backtracking."
    }
  ]
}