{
  "subtopic_id": "parallel_processing",
  "subtopic_name": "Parallel Processing",
  "mode": "finals",
  "difficulty": "average",
  "questions": [
    {
      "question": "What law describes the maximum speedup from parallelization?",
      "answer": "Amdahl's Law",
      "alternatives": [
        "Amdahl"
      ],
      "explanation": "Amdahl's Law calculates theoretical speedup limit based on the fraction of code that can be parallelized.",
      "id": "par_pro_par_pro_a_finals_000"
    },
    {
      "question": "What is parallel efficiency?",
      "answer": "Speedup divided by number of processors",
      "alternatives": [
        "Efficiency ratio"
      ],
      "explanation": "Parallel efficiency measures how effectively additional processors improve performance, ideally approaching 1.0.",
      "id": "par_pro_par_pro_a_finals_001"
    },
    {
      "question": "What is a race condition?",
      "answer": "Multiple threads accessing shared data incorrectly",
      "alternatives": [
        "Data race",
        "Race hazard"
      ],
      "explanation": "Race conditions occur when concurrent threads access shared data without proper synchronization, causing unpredictable results.",
      "id": "par_pro_par_pro_a_finals_002"
    },
    {
      "question": "What is a mutex?",
      "answer": "Mutual exclusion lock",
      "alternatives": [
        "Mutual exclusion",
        "Lock mechanism"
      ],
      "explanation": "A mutex prevents multiple threads from simultaneously accessing a shared resource.",
      "id": "par_pro_par_pro_a_finals_003"
    },
    {
      "question": "What is deadlock?",
      "answer": "Threads waiting for each other indefinitely",
      "alternatives": [
        "Circular wait",
        "Resource deadlock"
      ],
      "explanation": "Deadlock occurs when threads are stuck waiting for resources held by each other.",
      "id": "par_pro_par_pro_a_finals_004"
    },
    {
      "question": "What is cache coherence?",
      "answer": "Consistent cache values across cores",
      "alternatives": [
        "Cache consistency"
      ],
      "explanation": "Cache coherence ensures all processor cores see a consistent view of shared memory.",
      "id": "par_pro_par_pro_a_finals_005"
    },
    {
      "question": "What is vectorization?",
      "answer": "Converting loops to SIMD operations",
      "alternatives": [
        "Vector processing",
        "SIMD optimization"
      ],
      "explanation": "Vectorization transforms scalar operations into vector operations that process multiple data elements simultaneously.",
      "id": "par_pro_par_pro_a_finals_006"
    },
    {
      "question": "What is load balancing in parallel computing?",
      "answer": "Distributing work evenly across processors",
      "alternatives": [
        "Work distribution",
        "Task balancing"
      ],
      "explanation": "Load balancing ensures all processors have roughly equal amounts of work to maximize efficiency.",
      "id": "par_pro_par_pro_a_finals_007"
    },
    {
      "question": "What is a barrier synchronization?",
      "answer": "Point where all threads must wait",
      "alternatives": [
        "Synchronization barrier",
        "Barrier"
      ],
      "explanation": "A barrier forces all threads to reach a specific point before any can proceed further.",
      "id": "par_pro_par_pro_a_finals_008"
    },
    {
      "question": "What is thread-level parallelism?",
      "answer": "Parallel execution of multiple threads",
      "alternatives": [
        "TLP",
        "Thread parallelism"
      ],
      "explanation": "TLP exploits parallelism by running multiple independent threads simultaneously.",
      "id": "par_pro_par_pro_a_finals_009"
    }
  ]
}