{
  "subtopic_id": "computer_history",
  "subtopic_name": "Computer History",
  "mode": "finals",
  "difficulty": "difficult",
  "questions": [
    {
      "question": "What mathematical principle underpins computation?",
      "answer": "Turing completeness",
      "alternatives": [
        "Computability"
      ],
      "explanation": "Turing complete: can compute anything computable. Definition: universal computational power. Examples: most programming languages, lambda calculus. Non-Turing: regular expressions, FSM. Significance: defines computational boundary. Practical: theoretical limit matters. Implications: undecidable problems exist."
    },
    {
      "question": "What architecture dominated computing eras?",
      "answer": "von Neumann architecture",
      "alternatives": [
        "Stored program model"
      ],
      "explanation": "von Neumann: computer architecture (1945). Model: program as data in memory. Components: CPU, memory, I/O, control unit. Significance: unified memory innovation. Modern computers: still based on this model. Alternatives: Harvard (separate instruction/data memory). Impact: foundational computing concept."
    },
    {
      "question": "What theoretical boundary exists in computation?",
      "answer": "Halting problem",
      "alternatives": [
        "Undecidability"
      ],
      "explanation": "Halting problem: determine if program halts (Turing, 1936). Result: undecidable (no general algorithm). Proof: contradiction argument. Implications: some problems fundamentally unsolvable. Church-Turing Thesis: connects to computability. Philosophical: suggests limits to automation."
    },
    {
      "question": "What computing principle explains algorithm correctness?",
      "answer": "Big-O notation",
      "alternatives": [
        "Computational complexity"
      ],
      "explanation": "Big-O: measures algorithm efficiency (Bachmann, Landau). Notation: O(n), O(n), O(log n), etc. Significance: predicts scalability. Analysis: ignores constants, focuses growth. Classes: P, NP, NP-complete. Research: P vs NP million-dollar problem. Practical: guides algorithm selection."
    },
    {
      "question": "What principle governs parallel computing?",
      "answer": "Amdahl's law",
      "alternatives": [
        "Speedup limitation"
      ],
      "explanation": "Amdahl's law: limits parallelism speedup. Formula: speedup based on serial fraction. Implication: diminishing returns (not linear scaling). Example: 90% parallelizable = max 10x speedup. Practical: identifies bottlenecks. Modern: multi-core ubiquitous, bottlenecks shifted. Challenge: extracting parallelism."
    },
    {
      "question": "What engineering discipline shaped computing?",
      "answer": "Information theory",
      "alternatives": [
        "Claude Shannon"
      ],
      "explanation": "Information theory: Shannon (1948). Concepts: entropy, bits, channels, redundancy. Significance: quantifies information. Applications: coding theory, compression, cryptography. Impact: fundamental to digital communication. Implications: channel capacity limits. Foundation: digital systems theory."
    },
    {
      "question": "What mathematical structure models computation?",
      "answer": "Lambda calculus",
      "alternatives": [
        "Functional calculus"
      ],
      "explanation": "Lambda calculus: Church (1930s). Model: computation via function application. Significance: equivalent to Turing machines. Applications: functional programming (Lisp, Haskell). Concepts: closures, higher-order functions. Implications: functions as first-class citizens. Impact: shaped programming language theory."
    },
    {
      "question": "What principle optimizes resource allocation?",
      "answer": "Load balancing",
      "alternatives": [
        "Resource distribution"
      ],
      "explanation": "Load balancing: distributes work across resources. Techniques: round-robin, least-loaded, dynamic. Goals: maximize utilization, minimize latency. Challenges: prediction, overhead. Scalability: critical for distributed systems. Real-world: servers, databases, network routers. Impact: enables large-scale systems."
    },
    {
      "question": "What design pattern unified computer interfaces?",
      "answer": "Abstraction layers",
      "alternatives": [
        "Layered architecture"
      ],
      "explanation": "Abstraction layers: hide complexity (each level). Examples: OSI model (networking), hardware layers, software layers. Benefit: isolates concerns, enables modularity. Maintenance: change implementation, not interface. Tradeoff: performance overhead. Modern: microservices extension. Impact: enables large systems."
    },
    {
      "question": "What computing metric defines system performance?",
      "answer": "Throughput and latency",
      "alternatives": [
        "Performance metrics"
      ],
      "explanation": "Throughput: operations per time unit. Latency: time for single operation. Tradeoff: often inversely related. Measurement: different for different systems. Optimization: workload dependent. Monitoring: production systems continuously. Tuning: balance requirements. Impact: user experience quality."
    }
  ]
}